# analyze_38com.py
"""
38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ HTML êµ¬ì¡° ë¶„ì„ ë„êµ¬

ì‹¤ì œ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ í¬ë¡¤ëŸ¬ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ì‚¬ìš©
"""

import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import json

class SiteAnalyzer:
    """ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def analyze_list_page(self, url: str):
        """ëª©ë¡ í˜ì´ì§€ ë¶„ì„"""
        
        print("="*60)
        print("ğŸ“Š 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëª©ë¡ í˜ì´ì§€ ë¶„ì„")
        print("="*60)
        print()
        
        # 1. HTML ê°€ì ¸ì˜¤ê¸°
        print(f"ğŸ” URL ì¡°íšŒ: {url}")
        
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            html = response.text
            
            print(f"âœ… ì¡°íšŒ ì„±ê³µ ({len(html):,} bytes)\n")
        
        except Exception as e:
            print(f"âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return
        
        # HTML íŒŒì¼ë¡œ ì €ì¥
        with open('38com_list_page.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("ğŸ’¾ ì €ì¥: 38com_list_page.html\n")
        
        # 2. êµ¬ì¡° ë¶„ì„
        soup = BeautifulSoup(html, 'html.parser')
        
        # 2-1. ë§í¬ ë¶„ì„
        self._analyze_links(soup)
        
        # 2-2. í…Œì´ë¸” ë¶„ì„
        self._analyze_tables(soup)
        
        # 2-3. ë‚ ì§œ íŒ¨í„´
        self._analyze_dates(soup)
        
        # 2-4. í´ë˜ìŠ¤ ì‚¬ìš© í˜„í™©
        self._analyze_classes(soup)
    
    def analyze_detail_page(self, url: str):
        """ìƒì„¸ í˜ì´ì§€ ë¶„ì„"""
        
        print("\n" + "="*60)
        print("ğŸ“„ 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìƒì„¸ í˜ì´ì§€ ë¶„ì„")
        print("="*60)
        print()
        
        # 1. HTML ê°€ì ¸ì˜¤ê¸°
        print(f"ğŸ” URL ì¡°íšŒ: {url}")
        
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            html = response.text
            
            print(f"âœ… ì¡°íšŒ ì„±ê³µ ({len(html):,} bytes)\n")
        
        except Exception as e:
            print(f"âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return
        
        # HTML íŒŒì¼ë¡œ ì €ì¥
        with open('38com_detail_page.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("ğŸ’¾ ì €ì¥: 38com_detail_page.html\n")
        
        # 2. êµ¬ì¡° ë¶„ì„
        soup = BeautifulSoup(html, 'html.parser')
        
        # 2-1. ì œëª© í›„ë³´
        self._find_title_candidates(soup)
        
        # 2-2. ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´
        self._find_analyst_candidates(soup)
        
        # 2-3. ì¢…ëª© ì •ë³´
        self._find_stock_candidates(soup)
        
        # 2-4. íˆ¬ìì˜ê²¬
        self._find_opinion_candidates(soup)
        
        # 2-5. ëª©í‘œê°€
        self._find_target_price_candidates(soup)
        
        # 2-6. ë‚ ì§œ
        self._find_date_candidates(soup)
    
    def _analyze_links(self, soup: BeautifulSoup):
        """ë§í¬ ë¶„ì„"""
        
        print("ğŸ”— ë§í¬ ë¶„ì„")
        print("-" * 40)
        
        links = soup.find_all('a', href=True)
        
        print(f"ì´ {len(links)}ê°œ ë§í¬ ë°œê²¬\n")
        
        # ë§í¬ íŒ¨í„´ ë¶„ë¥˜
        patterns = Counter()
        
        for link in links:
            href = link['href']
            
            # íŒ¨í„´ ì¶”ì¶œ
            if 'research' in href.lower():
                patterns['research'] += 1
            elif 'report' in href.lower():
                patterns['report'] += 1
            elif 'view' in href.lower():
                patterns['view'] += 1
        
        print("íŒ¨í„´ë³„ ê°œìˆ˜:")
        for pattern, count in patterns.most_common():
            print(f"  {pattern}: {count}ê°œ")
        
        # ìƒ˜í”Œ ë§í¬ ì¶œë ¥
        print("\nìƒ˜í”Œ ë§í¬ (ìµœëŒ€ 10ê°œ):")
        
        relevant_links = [
            link for link in links
            if any(kw in link['href'].lower() for kw in ['research', 'report', 'view'])
        ]
        
        for i, link in enumerate(relevant_links[:10], 1):
            href = link['href']
            text = link.get_text(strip=True)[:50]
            print(f"  {i}. {href}")
            if text:
                print(f"     í…ìŠ¤íŠ¸: {text}")
        
        print()
    
    def _analyze_tables(self, soup: BeautifulSoup):
        """í…Œì´ë¸” ë¶„ì„"""
        
        print("ğŸ“Š í…Œì´ë¸” ë¶„ì„")
        print("-" * 40)
        
        tables = soup.find_all('table')
        
        print(f"ì´ {len(tables)}ê°œ í…Œì´ë¸” ë°œê²¬\n")
        
        for i, table in enumerate(tables, 1):
            rows = table.find_all('tr')
            cols = table.find_all('td')
            
            print(f"í…Œì´ë¸” {i}:")
            print(f"  í–‰: {len(rows)}ê°œ")
            print(f"  ì…€: {len(cols)}ê°œ")
            
            # class í™•ì¸
            if table.get('class'):
                print(f"  í´ë˜ìŠ¤: {table['class']}")
            
            # ì²« í–‰ ìƒ˜í”Œ
            if rows:
                first_row = rows[0]
                cells = first_row.find_all(['td', 'th'])
                
                if cells:
                    print(f"  ì²« í–‰ ìƒ˜í”Œ:")
                    for cell in cells[:5]:
                        text = cell.get_text(strip=True)[:30]
                        if text:
                            print(f"    - {text}")
            
            print()
    
    def _analyze_dates(self, soup: BeautifulSoup):
        """ë‚ ì§œ íŒ¨í„´ ë¶„ì„"""
        
        print("ğŸ“… ë‚ ì§œ íŒ¨í„´ ë¶„ì„")
        print("-" * 40)
        
        text = soup.get_text()
        
        # ë‚ ì§œ íŒ¨í„´
        patterns = {
            'YYYY.MM.DD': r'20\d{2}\.\d{1,2}\.\d{1,2}',
            'YYYY-MM-DD': r'20\d{2}-\d{1,2}-\d{1,2}',
            'YYYY/MM/DD': r'20\d{2}/\d{1,2}/\d{1,2}',
        }
        
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text)
            
            if matches:
                print(f"\n{pattern_name} íŒ¨í„´:")
                print(f"  ë°œê²¬: {len(matches)}ê°œ")
                print(f"  ìƒ˜í”Œ: {matches[:5]}")
        
        print()
    
    def _analyze_classes(self, soup: BeautifulSoup):
        """CSS í´ë˜ìŠ¤ ë¶„ì„"""
        
        print("ğŸ¨ CSS í´ë˜ìŠ¤ ë¶„ì„")
        print("-" * 40)
        
        # ëª¨ë“  í´ë˜ìŠ¤ ìˆ˜ì§‘
        classes = []
        
        for element in soup.find_all(class_=True):
            classes.extend(element['class'])
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        class_counter = Counter(classes)
        
        print(f"ì´ {len(set(classes))}ê°œ ê³ ìœ  í´ë˜ìŠ¤\n")
        print("ìì£¼ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ (Top 20):")
        
        for cls, count in class_counter.most_common(20):
            print(f"  {cls}: {count}íšŒ")
        
        print()
    
    def _find_title_candidates(self, soup: BeautifulSoup):
        """ì œëª© í›„ë³´ ì°¾ê¸°"""
        
        print("ğŸ“Œ ì œëª© í›„ë³´")
        print("-" * 40)
        
        candidates = []
        
        # h1, h2, h3, title íƒœê·¸
        for tag in ['h1', 'h2', 'h3', 'title']:
            elements = soup.find_all(tag)
            
            for el in elements:
                text = el.get_text(strip=True)
                
                if text and 5 < len(text) < 200:
                    candidates.append({
                        'tag': tag,
                        'text': text,
                        'class': el.get('class', [])
                    })
        
        print(f"ë°œê²¬: {len(candidates)}ê°œ\n")
        
        for i, cand in enumerate(candidates[:10], 1):
            print(f"{i}. <{cand['tag']}> {cand['text'][:80]}")
            if cand['class']:
                print(f"   í´ë˜ìŠ¤: {cand['class']}")
        
        print()
    
    def _find_analyst_candidates(self, soup: BeautifulSoup):
        """ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ í›„ë³´"""
        
        print("ğŸ‘¤ ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ í›„ë³´")
        print("-" * 40)
        
        candidates = []
        
        # 'ì¦ê¶Œ' í‚¤ì›Œë“œê°€ ìˆëŠ” ìš”ì†Œ
        for element in soup.find_all(['div', 'span', 'p', 'td']):
            text = element.get_text(strip=True)
            
            if 'ì¦ê¶Œ' in text and '/' in text and len(text) < 150:
                candidates.append({
                    'tag': element.name,
                    'text': text,
                    'class': element.get('class', [])
                })
        
        print(f"ë°œê²¬: {len(candidates)}ê°œ\n")
        
        # ì¤‘ë³µ ì œê±° (ë™ì¼ í…ìŠ¤íŠ¸)
        seen = set()
        unique = []
        
        for cand in candidates:
            if cand['text'] not in seen:
                seen.add(cand['text'])
                unique.append(cand)
        
        for i, cand in enumerate(unique[:10], 1):
            print(f"{i}. {cand['text']}")
            if cand['class']:
                print(f"   í´ë˜ìŠ¤: {cand['class']}")
        
        print()
    
    def _find_stock_candidates(self, soup: BeautifulSoup):
        """ì¢…ëª© ì •ë³´ í›„ë³´"""
        
        print("ğŸ“ˆ ì¢…ëª© ì •ë³´ í›„ë³´")
        print("-" * 40)
        
        # 6ìë¦¬ ìˆ«ì (ì¢…ëª© ì½”ë“œ)
        text = soup.get_text()
        codes = re.findall(r'\b\d{6}\b', text)
        
        print(f"6ìë¦¬ ì½”ë“œ ë°œê²¬: {len(codes)}ê°œ")
        print(f"ê³ ìœ  ì½”ë“œ: {len(set(codes))}ê°œ\n")
        
        if codes:
            print("ìƒ˜í”Œ:")
            for code in list(set(codes))[:10]:
                print(f"  {code}")
        
        print()
    
    def _find_opinion_candidates(self, soup: BeautifulSoup):
        """íˆ¬ìì˜ê²¬ í›„ë³´"""
        
        print("ğŸ’¡ íˆ¬ìì˜ê²¬ í›„ë³´")
        print("-" * 40)
        
        keywords = ['ë§¤ìˆ˜', 'ë§¤ë„', 'ì¤‘ë¦½', 'Buy', 'Sell', 'Hold', 
                   'ìƒí–¥', 'í•˜í–¥', 'ìœ ì§€']
        
        candidates = []
        
        for element in soup.find_all(['div', 'span', 'td', 'strong']):
            text = element.get_text(strip=True)
            
            if any(kw in text for kw in keywords) and len(text) < 100:
                candidates.append({
                    'tag': element.name,
                    'text': text,
                    'class': element.get('class', [])
                })
        
        print(f"ë°œê²¬: {len(candidates)}ê°œ\n")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        
        for cand in candidates:
            if cand['text'] not in seen:
                seen.add(cand['text'])
                unique.append(cand)
        
        for i, cand in enumerate(unique[:10], 1):
            print(f"{i}. {cand['text']}")
            if cand['class']:
                print(f"   í´ë˜ìŠ¤: {cand['class']}")
        
        print()
    
    def _find_target_price_candidates(self, soup: BeautifulSoup):
        """ëª©í‘œê°€ í›„ë³´"""
        
        print("ğŸ’° ëª©í‘œê°€ í›„ë³´")
        print("-" * 40)
        
        candidates = []
        
        for element in soup.find_all(['div', 'span', 'td', 'strong']):
            text = element.get_text(strip=True)
            
            # "ëª©í‘œê°€" í‚¤ì›Œë“œ ë˜ëŠ” ìˆ«ì+ì› íŒ¨í„´
            if ('ëª©í‘œ' in text or 'target' in text.lower()) and \
               re.search(r'\d{1,3}[,\d]*ì›?', text):
                candidates.append({
                    'tag': element.name,
                    'text': text,
                    'class': element.get('class', [])
                })
        
        print(f"ë°œê²¬: {len(candidates)}ê°œ\n")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        
        for cand in candidates:
            if cand['text'] not in seen:
                seen.add(cand['text'])
                unique.append(cand)
        
        for i, cand in enumerate(unique[:10], 1):
            print(f"{i}. {cand['text']}")
            if cand['class']:
                print(f"   í´ë˜ìŠ¤: {cand['class']}")
        
        print()
    
    def _find_date_candidates(self, soup: BeautifulSoup):
        """ë‚ ì§œ í›„ë³´"""
        
        print("ğŸ“… ë‚ ì§œ í›„ë³´")
        print("-" * 40)
        
        candidates = []
        
        # ë‚ ì§œ íŒ¨í„´
        date_pattern = r'20\d{2}[./-]\d{1,2}[./-]\d{1,2}'
        
        for element in soup.find_all(['div', 'span', 'td', 'time']):
            text = element.get_text(strip=True)
            
            if re.search(date_pattern, text):
                candidates.append({
                    'tag': element.name,
                    'text': text,
                    'class': element.get('class', [])
                })
        
        print(f"ë°œê²¬: {len(candidates)}ê°œ\n")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        
        for cand in candidates:
            if cand['text'] not in seen:
                seen.add(cand['text'])
                unique.append(cand)
        
        for i, cand in enumerate(unique[:10], 1):
            print(f"{i}. {cand['text']}")
            if cand['class']:
                print(f"   í´ë˜ìŠ¤: {cand['class']}")
        
        print()
    
    def generate_extraction_code(self):
        """ì¶”ì¶œ ì½”ë“œ ìƒì„±"""
        
        print("\n" + "="*60)
        print("ğŸ”§ í¬ë¡¤ëŸ¬ ìˆ˜ì • ì½”ë“œ ìƒì„±")
        print("="*60)
        print()
        
        print("ìœ„ì—ì„œ í™•ì¸í•œ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ crawler_38com.pyë¥¼ ìˆ˜ì •í•˜ì„¸ìš”:")
        print()
        
        print("""
# ì˜ˆì‹œ: ì œëª© ì¶”ì¶œ
def _extract_title(self, soup):
    # ë°©ë²• 1: íŠ¹ì • í´ë˜ìŠ¤
    title_div = soup.find('div', {'class': 'report-title'})
    if title_div:
        return title_div.get_text(strip=True)
    
    # ë°©ë²• 2: h1 íƒœê·¸
    h1 = soup.find('h1')
    if h1:
        return h1.get_text(strip=True)
    
    return None

# ì˜ˆì‹œ: ì• ë„ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
def _extract_analyst(self, soup):
    # ë°œê²¬í•œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ìˆ˜ì •
    analyst_div = soup.find('div', {'class': 'analyst-info'})
    if analyst_div:
        text = analyst_div.get_text(strip=True)
        parts = text.split('/')
        return {
            'name': parts[0].strip(),
            'firm': parts[1].strip() if len(parts) > 1 else None
        }
    
    return {'name': 'UNKNOWN', 'firm': 'UNKNOWN'}
        """)

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    analyzer = SiteAnalyzer()
    
    # 1. ëª©ë¡ í˜ì´ì§€ ë¶„ì„
    list_url = "https://www.38.co.kr/html/fund/research_sec.html"
    
    print("38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ HTML êµ¬ì¡° ë¶„ì„ ë„êµ¬\n")
    print("ì´ ë„êµ¬ëŠ” ì‹¤ì œ ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬")
    print("í¬ë¡¤ëŸ¬ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n")
    
    choice = input("ë¶„ì„í•  í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. ëª©ë¡ í˜ì´ì§€\n2. ìƒì„¸ í˜ì´ì§€\n3. ë‘˜ ë‹¤\nì„ íƒ (1-3): ")
    
    if choice in ['1', '3']:
        analyzer.analyze_list_page(list_url)
    
    if choice in ['2', '3']:
        # ìƒì„¸ í˜ì´ì§€ URL ì…ë ¥
        detail_url = input("\nìƒì„¸ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        
        if detail_url:
            analyzer.analyze_detail_page(detail_url)
        else:
            print("âš ï¸  URLì´ ì…ë ¥ë˜ì§€ ì•Šì•„ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # ì½”ë“œ ìƒì„± ê°€ì´ë“œ
    analyzer.generate_extraction_code()
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ìƒì„±ëœ HTML íŒŒì¼ í™•ì¸ (38com_list_page.html, 38com_detail_page.html)")
    print("2. ìœ„ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ crawler_38com.py ìˆ˜ì •")
    print("3. test_crawler.pyë¡œ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()


