# crawler_naver_finance_research.py
"""
ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ í¬ë¡¤ëŸ¬

ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
https://finance.naver.com/research/

ê³„ì•½:
- ì…ë ¥: stock_name (str, ì¢…ëª©ëª…), stock_code (Optional[str], ì¢…ëª©ì½”ë“œ), days (int, ìµœê·¼ Nì¼)
- ì¶œë ¥: List[ReportMetadata] (ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
- ì˜ˆì™¸: requests.RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜), ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), OSError (íŒŒì¼ ì €ì¥ ì˜¤ë¥˜)
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import json
import hashlib
from urllib.parse import urljoin, urlparse, urlencode
import urllib3
import re
import os

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
try:
    from adaptive_crawler import AdaptiveCrawler, SiteProfile
    ADAPTIVE_CRAWLER_AVAILABLE = True
except ImportError:
    ADAPTIVE_CRAWLER_AVAILABLE = False

@dataclass
class ReportMetadata:
    """ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° (ë„¤ì´ë²„ ê¸ˆìœµìš©)"""
    report_id: str
    title: str
    stock_code: str
    stock_name: str
    analyst_name: str
    firm: str
    published_date: datetime
    source_url: str
    pdf_url: Optional[str] = None
    
    # íˆ¬ì ì •ë³´
    investment_opinion: Optional[str] = None  # BUY, HOLD, SELL
    target_price: Optional[int] = None  # ìˆ«ìë¡œ ì €ì¥
    current_price: Optional[int] = None
    
    # ì†ŒìŠ¤ ì •ë³´
    source: str = "NaverFinance"  # NaverFinance, HankyungConsensus
    
    # íŒŒì¼ ê²½ë¡œ
    pdf_path: Optional[str] = None
    meta_path: Optional[str] = None
    
    def to_dict(self) -> dict:
        data = asdict(self)
        data['published_date'] = self.published_date.isoformat()
        return data
    
    def to_meta_json(self) -> dict:
        """ë©”íƒ€ë°ì´í„° JSON í˜•ì‹ (íŒŒì¼ ì €ì¥ìš©)"""
        return {
            "symbol": self.stock_code,
            "company": self.stock_name,
            "date": self.published_date.strftime("%Y-%m-%d"),
            "securities": self.firm,
            "analyst": self.analyst_name,
            "rating": self.investment_opinion or "N/A",
            "target_price": self.target_price,
            "current_price": self.current_price,
            "source": self.source,
            "url": self.source_url,
            "pdf_url": self.pdf_url,
            "title": self.title
        }

class NaverFinanceResearchCrawler:
    """
    ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ í¬ë¡¤ëŸ¬
    
    ì‚¬ìš©ë²•:
        crawler = NaverFinanceResearchCrawler()
        reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", days=7)
        
        # PDF ë‹¤ìš´ë¡œë“œ í¬í•¨
        reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", days=7, download_pdf=True)
    """
    
    BASE_URL = "https://finance.naver.com"
    RESEARCH_URL = "https://finance.naver.com/research"
    
    def __init__(self, delay: float = 2.0, max_retries: int = 3, retry_delay: float = 5.0,
                 use_adaptive: bool = True, site_domain: str = "finance.naver.com",
                 download_dir: str = "AnalystReports"):
        """
        ì´ˆê¸°í™”
        
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            use_adaptive: ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš© ì—¬ë¶€
            site_domain: ì‚¬ì´íŠ¸ ë„ë©”ì¸
            download_dir: ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        """
        self.delay = delay
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.use_adaptive = use_adaptive and ADAPTIVE_CRAWLER_AVAILABLE
        self.site_domain = site_domain
        self.download_dir = download_dir
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        if self.use_adaptive:
            profile = SiteProfile(
                domain=site_domain,
                base_delay=delay,
                max_retries=max_retries
            )
            self.adaptive_crawler = AdaptiveCrawler(profile)
            self.session = self.adaptive_crawler.session
            self.logger.info("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ í™œì„±í™”")
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Referer': 'https://finance.naver.com/',
            })
            self.adaptive_crawler = None
    
    def search_by_stock(
        self,
        stock_name: str,
        stock_code: Optional[str] = None,
        days: int = 7,
        max_reports: int = 50,
        download_pdf: bool = False
    ) -> List[ReportMetadata]:
        """
        íŠ¹ì • ì¢…ëª©ìœ¼ë¡œ ë¦¬í¬íŠ¸ ê²€ìƒ‰
        
        Args:
            stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            stock_code: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930", Noneì´ë©´ ìë™ ê²€ìƒ‰, ê¸°ë³¸ê°’: None)
            days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’: 7)
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 50)
            download_pdf: PDF ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            List[ReportMetadata]: ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì˜ëª»ëœ stock_name ë˜ëŠ” days < 0
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨
            OSError: PDF ë‹¤ìš´ë¡œë“œ ë˜ëŠ” íŒŒì¼ ì €ì¥ ì‹¤íŒ¨
            
        ê³„ì•½:
        - ì…ë ¥: stock_nameì€ ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ìì—´, daysëŠ” ì–‘ìˆ˜
        - ì¶œë ¥: ReportMetadata ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        - ì˜ˆì™¸: ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜), OSError (íŒŒì¼ ì˜¤ë¥˜)
        """
        
        # ì…ë ¥ ê²€ì¦
        if not stock_name or not isinstance(stock_name, str):
            raise ValueError(f"stock_name must be a non-empty string, got {stock_name}")
        if days < 0:
            raise ValueError(f"days must be non-negative, got {days}")
        if max_reports < 0:
            raise ValueError(f"max_reports must be non-negative, got {max_reports}")
        
        self.logger.info(f"ğŸ” ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ ê²€ìƒ‰: {stock_name} (ìµœê·¼ {days}ì¼)")
        
        # ì¢…ëª© ì½”ë“œê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰
        if not stock_code:
            stock_code = self._search_stock_code(stock_name)
            if not stock_code:
                self.logger.error(f"ì¢…ëª© ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stock_name}")
                return []
        
        # ë¦¬ì„œì¹˜ í˜ì´ì§€ ì ‘ê·¼
        research_url = f"{self.RESEARCH_URL}/company_list.naver?code={stock_code}"
        
        html = self._fetch(research_url)
        
        if not html:
            self.logger.error(f"ë¦¬ì„œì¹˜ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {stock_name}")
            return []
        
        # ë¦¬í¬íŠ¸ ëª©ë¡ ì¶”ì¶œ
        report_links = self._extract_report_links(html, stock_code)
        
        self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë¦¬í¬íŠ¸: {len(report_links)}ê°œ")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for i, link_info in enumerate(report_links[:max_reports], 1):
            try:
                report = self._crawl_report_detail(link_info, stock_code, stock_name)
                
                if report:
                    # ë‚ ì§œ í•„í„°ë§
                    if report.published_date >= cutoff_date:
                        # PDF ë‹¤ìš´ë¡œë“œ
                        if download_pdf and report.pdf_url:
                            pdf_path = self._download_pdf(
                                report.pdf_url,
                                report.stock_name,
                                report.stock_code,
                                report.published_date,
                                report.firm,
                                report.investment_opinion,
                                report.target_price
                            )
                            report.pdf_path = pdf_path
                            
                            # ë©”íƒ€ë°ì´í„° ì €ì¥
                            meta_path = self._save_metadata(report)
                            report.meta_path = meta_path
                        
                        reports.append(report)
                        self.logger.info(
                            f"[{i}] âœ… {report.stock_name} - {report.analyst_name} ({report.firm}) "
                            f"- {report.investment_opinion} - ëª©í‘œê°€: {report.target_price}"
                        )
                
                if i < len(report_links):
                    time.sleep(self.delay)
                    
            except Exception as e:
                self.logger.error(f"ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {e}")
                continue
        
        self.logger.info(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ: {len(reports)}ê°œ")
        return reports
    
    def _search_stock_code(self, stock_name: str) -> Optional[str]:
        """ì¢…ëª©ëª…ìœ¼ë¡œ ì¢…ëª© ì½”ë“œ ê²€ìƒ‰"""
        
        search_url = f"{self.BASE_URL}/item/search.naver?query={stock_name}"
        
        html = self._fetch(search_url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ì¢…ëª© ì½”ë“œ íŒ¨í„´ ì°¾ê¸° (6ìë¦¬ ìˆ«ì)
        code_match = re.search(r'code=(\d{6})', html)
        if code_match:
            return code_match.group(1)
        
        return None
    
    def _fetch(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ ì¡°íšŒ"""
        
        if self.use_adaptive and self.adaptive_crawler:
            response = self.adaptive_crawler.fetch(url)
            if response:
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                return response.text
            return None
        
        for attempt in range(1, self.max_retries + 1):
            try:
                response = self.session.get(url, timeout=10, verify=True)
                response.raise_for_status()
                
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                return response.text
            
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    self.logger.warning(f"í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨ (ì‹œë„ {attempt}/{self.max_retries}): {e}")
                    time.sleep(self.retry_delay)
                else:
                    self.logger.error(f"í˜ì´ì§€ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨: {url} - {e}")
                    return None
        
        return None
    
    def _extract_report_links(self, html: str, stock_code: str) -> List[Dict]:
        """
        ë¦¬í¬íŠ¸ ëª©ë¡ì—ì„œ ë§í¬ ì¶”ì¶œ
        
        Returns:
            [{'url': '...', 'title': '...', 'date': '...', ...}, ...]
        """
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ í…Œì´ë¸” êµ¬ì¡° íŒŒì‹±
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                
                if len(cells) < 4:  # ìµœì†Œ ì»¬ëŸ¼ ìˆ˜ í™•ì¸
                    continue
                
                # ë¦¬í¬íŠ¸ ë§í¬ ì°¾ê¸°
                link_elem = row.find('a', href=True)
                if not link_elem:
                    continue
                
                href = link_elem['href']
                if not href.startswith('http'):
                    href = urljoin(self.BASE_URL, href)
                
                # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
                link_info = {
                    'url': href,
                    'title': link_elem.get_text(strip=True),
                    'date': None,
                    'firm': None,
                    'analyst': None,
                    'opinion': None,
                    'target_price': None
                }
                
                # ê° ì…€ì—ì„œ ì •ë³´ ì¶”ì¶œ
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)
                    
                    # ë‚ ì§œ (YYYY.MM.DD í˜•ì‹)
                    if re.match(r'\d{4}\.\d{2}\.\d{2}', text):
                        link_info['date'] = text
                    
                    # ì¦ê¶Œì‚¬ëª…
                    if 'ì¦ê¶Œ' in text or 'íˆ¬ì' in text or 'ìì‚°' in text:
                        link_info['firm'] = text
                    
                    # íˆ¬ìì˜ê²¬ (BUY, HOLD, SELL ë“±)
                    if any(word in text.upper() for word in ['BUY', 'HOLD', 'SELL', 'ë§¤ìˆ˜', 'ë³´ìœ ', 'ë§¤ë„']):
                        link_info['opinion'] = text
                    
                    # ëª©í‘œê°€ (ìˆ«ì + ì›)
                    price_match = re.search(r'([\d,]+)\s*ì›?', text)
                    if price_match:
                        price_str = price_match.group(1).replace(',', '')
                        try:
                            link_info['target_price'] = int(price_str)
                        except:
                            pass
                
                if link_info['url']:
                    links.append(link_info)
        
        return links
    
    def _crawl_report_detail(
        self,
        link_info: Dict,
        stock_code: str,
        stock_name: str
    ) -> Optional[ReportMetadata]:
        """ë¦¬í¬íŠ¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        
        url = link_info['url']
        html = self._fetch(url)
        
        if not html:
            return None
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª©
            title = link_info.get('title') or self._extract_title(soup)
            
            if not title:
                return None
            
            # ë‚ ì§œ íŒŒì‹±
            date_str = link_info.get('date')
            if date_str:
                published_date = self._parse_date(date_str)
            else:
                published_date = self._extract_date(soup)
            
            # ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´
            analyst_info = self._extract_analyst(soup, link_info)
            
            # íˆ¬ìì˜ê²¬
            opinion = link_info.get('opinion') or self._extract_opinion(soup)
            
            # ëª©í‘œê°€
            target_price = link_info.get('target_price') or self._extract_target_price(soup)
            
            # PDF ë§í¬ ì°¾ê¸°
            pdf_url = self._extract_pdf_link(soup, url)
            
            # ë¦¬í¬íŠ¸ ID ìƒì„±
            report_id = self._generate_report_id(url, title)
            
            return ReportMetadata(
                report_id=report_id,
                title=title,
                stock_code=stock_code,
                stock_name=stock_name,
                analyst_name=analyst_info.get('name', 'UNKNOWN'),
                firm=link_info.get('firm') or analyst_info.get('firm', 'UNKNOWN'),
                published_date=published_date,
                source_url=url,
                pdf_url=pdf_url,
                investment_opinion=self._normalize_opinion(opinion),
                target_price=target_price,
                source="NaverFinance"
            )
        
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ"""
        for tag in ['h1', 'h2', 'h3', 'title']:
            elem = soup.find(tag)
            if elem:
                text = elem.get_text(strip=True)
                if text and 5 < len(text) < 500:
                    return text
        return None
    
    def _extract_analyst(self, soup: BeautifulSoup, link_info: Dict) -> dict:
        """ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        analyst = link_info.get('analyst', 'UNKNOWN')
        firm = link_info.get('firm', 'UNKNOWN')
        
        # í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ì‹œë„
        text = soup.get_text()
        match = re.search(r'([ê°€-í£]{2,4})\s*[/Â·]\s*([ê°€-í£\w]+ì¦ê¶Œ)', text)
        if match:
            analyst = match.group(1)
            firm = match.group(2)
        
        return {'name': analyst, 'firm': firm}
    
    def _extract_opinion(self, soup: BeautifulSoup) -> Optional[str]:
        """íˆ¬ìì˜ê²¬ ì¶”ì¶œ"""
        text = soup.get_text()
        
        if 'ë§¤ìˆ˜' in text or 'BUY' in text.upper():
            return 'BUY'
        elif 'ë§¤ë„' in text or 'SELL' in text.upper():
            return 'SELL'
        elif 'ë³´ìœ ' in text or 'HOLD' in text.upper() or 'ì¤‘ë¦½' in text:
            return 'HOLD'
        
        return None
    
    def _extract_target_price(self, soup: BeautifulSoup) -> Optional[int]:
        """ëª©í‘œê°€ ì¶”ì¶œ"""
        text = soup.get_text()
        
        # "ëª©í‘œê°€: 98,000ì›" íŒ¨í„´
        match = re.search(r'ëª©í‘œê°€[:\s]*([\d,]+)', text)
        if match:
            try:
                return int(match.group(1).replace(',', ''))
            except:
                pass
        
        return None
    
    def _extract_pdf_link(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """PDF ë§í¬ ì¶”ì¶œ"""
        
        # PDF ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link['href']
            link_text = link.get_text(strip=True).lower()
            
            if 'pdf' in href.lower() or 'pdf' in link_text:
                if href.startswith('http'):
                    return href
                else:
                    return urljoin(base_url, href)
        
        return None
    
    def _parse_date(self, date_str: str) -> datetime:
        """ë‚ ì§œ íŒŒì‹± (YYYY.MM.DD í˜•ì‹)"""
        try:
            return datetime.strptime(date_str, '%Y.%m.%d')
        except:
            return datetime.now()
    
    def _extract_date(self, soup: BeautifulSoup) -> datetime:
        """ë‚ ì§œ ì¶”ì¶œ"""
        text = soup.get_text()
        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        if match:
            year, month, day = match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except:
                pass
        return datetime.now()
    
    def _normalize_opinion(self, opinion: Optional[str]) -> Optional[str]:
        """íˆ¬ìì˜ê²¬ ì •ê·œí™”"""
        if not opinion:
            return None
        
        opinion_upper = opinion.upper()
        
        if 'BUY' in opinion_upper or 'ë§¤ìˆ˜' in opinion:
            return 'BUY'
        elif 'SELL' in opinion_upper or 'ë§¤ë„' in opinion:
            return 'SELL'
        elif 'HOLD' in opinion_upper or 'ë³´ìœ ' in opinion or 'ì¤‘ë¦½' in opinion:
            return 'HOLD'
        
        return opinion
    
    def _generate_report_id(self, url: str, title: str) -> str:
        """ë³´ê³ ì„œ ID ìƒì„±"""
        content = f"{url}:{title}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def _download_pdf(
        self,
        pdf_url: str,
        stock_name: str,
        stock_code: str,
        date: datetime,
        firm: str,
        opinion: Optional[str],
        target_price: Optional[int]
    ) -> Optional[str]:
        """PDF ë‹¤ìš´ë¡œë“œ"""
        
        try:
            # í´ë” êµ¬ì¡° ìƒì„±: AnalystReports/ì¢…ëª©ëª…_ì¢…ëª©ì½”ë“œ/
            folder_name = f"{stock_name}_{stock_code}"
            folder_path = os.path.join(self.download_dir, folder_name)
            os.makedirs(folder_path, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±: YYYY-MM-DD_ì¦ê¶Œì‚¬_ì˜ê²¬_ëª©í‘œê°€.pdf
            date_str = date.strftime("%Y-%m-%d")
            firm_clean = firm.replace('/', '_').replace('\\', '_')
            opinion_str = opinion or "N/A"
            price_str = f"{target_price}" if target_price else "N/A"
            
            filename = f"{date_str}_{firm_clean}_{opinion_str}_{price_str}.pdf"
            filepath = os.path.join(folder_path, filename)
            
            # PDF ë‹¤ìš´ë¡œë“œ
            response = self.session.get(pdf_url, timeout=30, verify=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.logger.info(f"ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return filepath
        
        except Exception as e:
            self.logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {pdf_url} - {e}")
            return None
    
    def _save_metadata(self, report: ReportMetadata) -> Optional[str]:
        """ë©”íƒ€ë°ì´í„° JSON ì €ì¥"""
        
        try:
            if not report.pdf_path:
                return None
            
            # PDF íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ë©”íƒ€ë°ì´í„° ì €ì¥
            folder_path = os.path.dirname(report.pdf_path)
            meta_filename = os.path.basename(report.pdf_path).replace('.pdf', '.meta.json')
            meta_path = os.path.join(folder_path, meta_filename)
            
            meta_data = report.to_meta_json()
            
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {meta_path}")
            return meta_path
        
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    crawler = NaverFinanceResearchCrawler(delay=2.0)
    
    print("ğŸš€ ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ í¬ë¡¤ëŸ¬ ì‹œì‘\n")
    
    # ì‚¼ì„±ì „ì ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (PDF ë‹¤ìš´ë¡œë“œ í¬í•¨)
    reports = crawler.search_by_stock(
        stock_name="ì‚¼ì„±ì „ì",
        stock_code="005930",
        days=7,
        max_reports=20,
        download_pdf=True
    )
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(reports)}ê°œ\n")
    
    for i, report in enumerate(reports, 1):
        print(f"{i}. {report.stock_name} ({report.stock_code})")
        print(f"   ì œëª©: {report.title}")
        print(f"   ì• ë„ë¦¬ìŠ¤íŠ¸: {report.analyst_name} ({report.firm})")
        print(f"   ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}")
        print(f"   ì˜ê²¬: {report.investment_opinion}")
        print(f"   ëª©í‘œê°€: {report.target_price}")
        if report.pdf_path:
            print(f"   PDF: {report.pdf_path}")
        print()
    
    print("âœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()

