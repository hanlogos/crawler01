# crawler_with_analysis.py
"""
í¬ë¡¤ëŸ¬ + ë¶„ì„ ì‹œìŠ¤í…œ í†µí•©

í¬ë¡¤ë§ê³¼ ë™ì‹œì— ë³´ê³ ì„œ ë¶„ì„ ìˆ˜í–‰
"""

import sys
import io
import logging
from typing import List, Optional
from datetime import datetime

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from crawler_38com import ThirtyEightComCrawler, ReportMetadata
from report_knowledge_system import (
    ReportAnalysisOrchestrator,
    TradingAvatar,
    RiskAvatar,
    FinancialAvatar,
    MockLLM
)

# Ollama LLM ì„í¬íŠ¸ (ì„ íƒì )
try:
    from ollama_llm import OllamaLLM
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    OllamaLLM = None

class IntegratedCrawler:
    """í†µí•© í¬ë¡¤ëŸ¬ (í¬ë¡¤ë§ + ë¶„ì„)"""
    
    def __init__(
        self,
        use_analysis: bool = True,
        llm_processor = None,
        crawler_delay: float = 3.0,
        use_adaptive: bool = True,
        use_ollama: bool = False,
        ollama_model: str = "llama3"
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            use_analysis: ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€
            llm_processor: LLM í”„ë¡œì„¸ì„œ (Noneì´ë©´ MockLLM ë˜ëŠ” OllamaLLM ì‚¬ìš©)
            crawler_delay: í¬ë¡¤ëŸ¬ ì§€ì—° ì‹œê°„
            use_adaptive: ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš© ì—¬ë¶€
            use_ollama: Ollama LLM ì‚¬ìš© ì—¬ë¶€ (llm_processorê°€ Noneì¼ ë•Œ)
            ollama_model: Ollama ëª¨ë¸ ì´ë¦„ (llama3, mistral ë“±)
        """
        self.use_analysis = use_analysis
        self.logger = logging.getLogger(__name__)
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        self.crawler = ThirtyEightComCrawler(
            delay=crawler_delay,
            use_adaptive=use_adaptive
        )
        
        # ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if self.use_analysis:
            if llm_processor is None:
                # Ollama ì‚¬ìš© ì˜µì…˜
                if use_ollama and OLLAMA_AVAILABLE:
                    try:
                        llm_processor = OllamaLLM(model=ollama_model)
                        self.logger.info(f"âœ… Ollama LLM ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {ollama_model})")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸  Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                        self.logger.info("   MockLLMìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                        llm_processor = MockLLM()
                else:
                    llm_processor = MockLLM()
                    if use_ollama and not OLLAMA_AVAILABLE:
                        self.logger.warning("âš ï¸  Ollamaë¥¼ ì‚¬ìš©í•˜ë ¤ê³  í–ˆì§€ë§Œ ollama_llm.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            self.orchestrator = ReportAnalysisOrchestrator(llm_processor)
            self._setup_avatars()
            self.logger.info("âœ… ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            self.orchestrator = None
    
    def _setup_avatars(self):
        """ê¸°ë³¸ ì•„ë°”íƒ€ ì„¤ì •"""
        
        # Trading Avatars
        self.orchestrator.register_avatar(TradingAvatar("trader_short", "short"))
        self.orchestrator.register_avatar(TradingAvatar("trader_medium", "medium"))
        self.orchestrator.register_avatar(TradingAvatar("trader_long", "long"))
        
        # Risk Avatars
        self.orchestrator.register_avatar(RiskAvatar("risk_downside", "downside"))
        self.orchestrator.register_avatar(RiskAvatar("risk_upside", "upside"))
        
        # Financial Avatar
        self.orchestrator.register_avatar(FinancialAvatar("finance_1"))
        
        self.logger.info(f"âœ… {len(self.orchestrator.avatars)}ê°œ ì•„ë°”íƒ€ ë“±ë¡")
    
    def crawl_and_analyze(
        self,
        days: int = 1,
        max_reports: int = 10,
        extract_content: bool = True
    ) -> dict:
        """
        í¬ë¡¤ë§ + ë¶„ì„ ìˆ˜í–‰
        
        Args:
            days: ìµœê·¼ Nì¼
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            extract_content: ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì—¬ë¶€ (ë¶„ì„ì— í•„ìš”)
        
        Returns:
            {
                'reports': [...],  # ReportMetadata ë¦¬ìŠ¤íŠ¸
                'analysis_results': [...],  # ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                'summary': {...}  # ìš”ì•½ í†µê³„
            }
        """
        
        self.logger.info("="*60)
        self.logger.info("ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œì‘")
        self.logger.info("="*60)
        
        # 1. í¬ë¡¤ë§
        self.logger.info(f"\nğŸ“Š 1ë‹¨ê³„: ë³´ê³ ì„œ í¬ë¡¤ë§ (ìµœê·¼ {days}ì¼)")
        reports = self.crawler.crawl_recent_reports(
            days=days,
            max_reports=max_reports
        )
        
        self.logger.info(f"âœ… {len(reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        
        if not reports:
            return {
                'reports': [],
                'analysis_results': [],
                'summary': {
                    'total_reports': 0,
                    'analyzed': 0,
                    'failed': 0
                }
            }
        
        # 2. ë¶„ì„ (ì˜µì…˜)
        analysis_results = []
        
        if self.use_analysis and extract_content:
            self.logger.info(f"\nğŸ¤– 2ë‹¨ê³„: ë³´ê³ ì„œ ë¶„ì„ ({len(reports)}ê°œ)")
            
            for i, report in enumerate(reports, 1):
                self.logger.info(f"\n[{i}/{len(reports)}] {report.stock_name} - {report.title[:50]}...")
                
                try:
                    # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                    report_content = self._extract_report_content(report)
                    
                    if not report_content:
                        self.logger.warning(f"âš ï¸  ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {report.report_id}")
                        analysis_results.append({
                            'report_id': report.report_id,
                            'status': 'failed',
                            'error': 'ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨'
                        })
                        continue
                    
                    # ë¶„ì„ ìˆ˜í–‰
                    result = self.orchestrator.process_report(
                        report_id=report.report_id,
                        report_content=report_content
                    )
                    
                    analysis_results.append({
                        'report_id': report.report_id,
                        'status': 'success',
                        'result': result
                    })
                    
                    self.logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {result['total_time']:.2f}ì´ˆ")
                    
                except Exception as e:
                    self.logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    analysis_results.append({
                        'report_id': report.report_id,
                        'status': 'error',
                        'error': str(e)
                    })
        
        # 3. ìš”ì•½
        summary = {
            'total_reports': len(reports),
            'analyzed': sum(1 for r in analysis_results if r['status'] == 'success'),
            'failed': sum(1 for r in analysis_results if r['status'] != 'success'),
            'analysis_enabled': self.use_analysis
        }
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š ìµœì¢… ìš”ì•½")
        self.logger.info("="*60)
        self.logger.info(f"  ìˆ˜ì§‘: {summary['total_reports']}ê°œ")
        self.logger.info(f"  ë¶„ì„: {summary['analyzed']}ê°œ")
        self.logger.info(f"  ì‹¤íŒ¨: {summary['failed']}ê°œ")
        
        return {
            'reports': reports,
            'analysis_results': analysis_results,
            'summary': summary
        }
    
    def _extract_report_content(self, report: ReportMetadata) -> Optional[str]:
        """
        ë³´ê³ ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
        
        Args:
            report: ReportMetadata ê°ì²´
        
        Returns:
            ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©
        """
        
        try:
            # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            detail = self.crawler._crawl_report_detail(report.source_url)
            
            if not detail:
                return None
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª© + ë³¸ë¬¸)
            from bs4 import BeautifulSoup
            
            html = self.crawler._fetch(report.source_url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_parts = []
            
            # ì œëª©
            title = detail.get('title', report.title)
            if title:
                content_parts.append(f"ì œëª©: {title}")
            
            # ë³¸ë¬¸ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            body = soup.find('div', {'class': 'content'}) or \
                   soup.find('div', {'class': 'article'}) or \
                   soup.find('div', {'id': 'content'}) or \
                   soup.find('div', {'class': 'body'})
            
            if body:
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (íƒœê·¸ ì œê±°)
                text = body.get_text(separator='\n', strip=True)
                content_parts.append(text)
            else:
                # ì „ì²´ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                body = soup.find('body')
                if body:
                    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
                    for script in body(['script', 'style', 'nav', 'header', 'footer']):
                        script.decompose()
                    
                    text = body.get_text(separator='\n', strip=True)
                    content_parts.append(text)
            
            # ì¶”ê°€ ì •ë³´
            if detail.get('investment_opinion'):
                content_parts.append(f"íˆ¬ìì˜ê²¬: {detail['investment_opinion']}")
            
            if detail.get('target_price'):
                content_parts.append(f"ëª©í‘œê°€: {detail['target_price']}")
            
            return '\n\n'.join(content_parts)
            
        except Exception as e:
            self.logger.error(f"ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def save_results(
        self,
        results: dict,
        json_file: str = 'crawled_reports.json',
        analysis_file: str = 'analysis_results.json'
    ):
        """ê²°ê³¼ ì €ì¥"""
        
        import json
        
        # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
        if results['reports']:
            self.crawler.save_to_json(results['reports'], json_file)
            self.logger.info(f"ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {json_file}")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        if results['analysis_results']:
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            self.logger.info(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_file}")

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("\n" + "="*60)
    print("ğŸš€ í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*60)
    print()
    
    # í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    integrated = IntegratedCrawler(
        use_analysis=True,
        crawler_delay=3.0,
        use_adaptive=True
    )
    
    # í¬ë¡¤ë§ + ë¶„ì„
    results = integrated.crawl_and_analyze(
        days=1,
        max_reports=5,  # í…ŒìŠ¤íŠ¸ìš© 5ê°œ
        extract_content=True
    )
    
    # ê²°ê³¼ ì €ì¥
    integrated.save_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*60)
    print(f"ìˆ˜ì§‘: {results['summary']['total_reports']}ê°œ")
    print(f"ë¶„ì„: {results['summary']['analyzed']}ê°œ")
    print(f"ì‹¤íŒ¨: {results['summary']['failed']}ê°œ")
    
    # ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
    if results['analysis_results']:
        print("\nğŸ“‹ ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ:")
        for res in results['analysis_results'][:3]:
            if res['status'] == 'success':
                result = res['result']
                print(f"\n  ë³´ê³ ì„œ ID: {result['report_id']}")
                print(f"  ì¶”ì¶œ ì‹œê°„: {result['extract_time']:.2f}ì´ˆ")
                print(f"  ì•„ë°”íƒ€ ì‹œê°„: {result['avatar_time']:.2f}ì´ˆ")
                print(f"  ì•„ë°”íƒ€ ê²°ê³¼: {len(result['avatar_results'])}ê°œ")
                
                # ì²« ë²ˆì§¸ ì•„ë°”íƒ€ ê²°ê³¼
                if result['avatar_results']:
                    first = result['avatar_results'][0]
                    print(f"    - {first['avatar_id']}: {first['result']}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

