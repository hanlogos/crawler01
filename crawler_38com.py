# crawler_38com.py
"""
38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ëŸ¬

ì¦ê¶Œ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import json
import hashlib
from urllib.parse import urljoin, urlparse
import urllib3

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
try:
    from adaptive_crawler import AdaptiveCrawler, SiteProfile
    ADAPTIVE_CRAWLER_AVAILABLE = True
except ImportError:
    ADAPTIVE_CRAWLER_AVAILABLE = False

@dataclass
class ReportMetadata:
    """ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„°"""
    report_id: str
    title: str
    stock_code: str
    stock_name: str
    analyst_name: str
    firm: str
    published_date: datetime
    source_url: str
    
    # ì¶”ê°€ ì •ë³´ (ìˆìœ¼ë©´)
    investment_opinion: Optional[str] = None
    target_price: Optional[str] = None
    
    def to_dict(self) -> dict:
        data = asdict(self)
        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        data['published_date'] = self.published_date.isoformat()
        return data

class ThirtyEightComCrawler:
    """
    38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ëŸ¬
    
    ì‚¬ìš©ë²•:
        crawler = ThirtyEightComCrawler()
        reports = crawler.crawl_recent_reports(days=1)
    """
    
    BASE_URL = "http://www.38.co.kr"  # HTTPS SSL ë¬¸ì œë¡œ HTTP ì‚¬ìš©
    REPORT_LIST_URL = "http://www.38.co.kr/html/fund/"
    # ëŒ€ì•ˆ URLë“¤
    ALTERNATIVE_URLS = [
        "http://www.38.co.kr/html/fund/",
        "http://www.38.co.kr/html/news/?m=kosdaq&nkey=report",
        "http://www.38.co.kr/html/news/?m=kospi&nkey=report",
    ]
    
    def __init__(self, delay: float = 3.0, max_retries: int = 3, retry_delay: float = 5.0,
                 use_adaptive: bool = True, site_domain: str = "www.38.co.kr"):
        """
        ì´ˆê¸°í™”
        
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            use_adaptive: ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš© ì—¬ë¶€
            site_domain: ì‚¬ì´íŠ¸ ë„ë©”ì¸
        """
        self.delay = delay
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.use_adaptive = use_adaptive and ADAPTIVE_CRAWLER_AVAILABLE
        self.site_domain = site_domain
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        if self.use_adaptive:
            profile = SiteProfile(
                domain=site_domain,
                base_delay=delay,
                max_retries=max_retries
            )
            self.adaptive_crawler = AdaptiveCrawler(profile)
            self.session = self.adaptive_crawler.session
            self.logger.info("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ í™œì„±í™”")
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            })
            self.adaptive_crawler = None
    
    def crawl_recent_reports(
        self, 
        days: int = 1,
        max_reports: int = 100
    ) -> List[ReportMetadata]:
        """
        ìµœê·¼ ë³´ê³ ì„œ í¬ë¡¤ë§
        
        Args:
            days: ìµœê·¼ Nì¼
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        
        self.logger.info(f"ğŸ“Š í¬ë¡¤ë§ ì‹œì‘: ìµœê·¼ {days}ì¼")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        try:
            # 1. ë³´ê³ ì„œ ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ (ì—¬ëŸ¬ URL ì‹œë„)
            list_urls = [
                f"{self.REPORT_LIST_URL}research_sec.html",
                f"{self.REPORT_LIST_URL}",
            ] + self.ALTERNATIVE_URLS
            
            html = None
            list_url = None
            
            for url in list_urls:
                self.logger.info(f"ğŸ” ëª©ë¡ ì¡°íšŒ ì‹œë„: {url}")
                html = self._fetch(url)
                
                if html and len(html) > 1000:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                    list_url = url
                    self.logger.info(f"âœ… ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ ì„±ê³µ: {url}")
                    break
                else:
                    self.logger.warning(f"âš ï¸  ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ: {url}")
            
            if not html:
                self.logger.error("ëª¨ë“  ëª©ë¡ í˜ì´ì§€ URL ì¡°íšŒ ì‹¤íŒ¨")
                return []
            
            # 2. ë³´ê³ ì„œ ë§í¬ ì¶”ì¶œ
            report_links = self._extract_report_links(html)
            
            self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë³´ê³ ì„œ: {len(report_links)}ê°œ")
            
            # 3. ê° ë³´ê³ ì„œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            total_links = min(len(report_links), max_reports)
            
            for i, link in enumerate(report_links[:max_reports], 1):
                progress = f"[{i}/{total_links}]"
                self.logger.info(f"{progress} ì²˜ë¦¬ ì¤‘: {link[:80]}...")
                
                report = self._crawl_report_detail(link)
                
                if report:
                    # ë‚ ì§œ í•„í„°ë§
                    if report.published_date >= cutoff_date:
                        reports.append(report)
                        self.logger.info(
                            f"{progress} âœ… ìˆ˜ì§‘: {report.stock_name} - {report.analyst_name}"
                        )
                    else:
                        self.logger.info(f"{progress} â­ï¸  ì˜¤ë˜ëœ ë³´ê³ ì„œ (ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}), ì¤‘ë‹¨")
                        break
                else:
                    self.logger.warning(f"{progress} âŒ ì¶”ì¶œ ì‹¤íŒ¨")
                
                # ì˜ˆì˜ë°”ë¥¸ ëŒ€ê¸°
                if i < total_links:  # ë§ˆì§€ë§‰ í•­ëª©ì€ ëŒ€ê¸° ë¶ˆí•„ìš”
                    time.sleep(self.delay)
            
            self.logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(reports)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}", exc_info=True)
        
        return reports
    
    def _fetch(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨, ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì§€ì›)"""
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš©
        if self.use_adaptive and self.adaptive_crawler:
            response = self.adaptive_crawler.fetch(url)
            if response:
                # ì¸ì½”ë”© ì²˜ë¦¬
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    content_type = response.headers.get('Content-Type', '')
                    if 'euc-kr' in content_type.lower() or 'euckr' in content_type.lower():
                        response.encoding = 'euc-kr'
                    else:
                        response.encoding = 'utf-8'
                return response.text
            return None
        
        # ê¸°ë³¸ í¬ë¡¤ëŸ¬ (ê¸°ì¡´ ë¡œì§)
        for attempt in range(1, self.max_retries + 1):
            try:
                response = self.session.get(url, timeout=10, verify=False)
                response.raise_for_status()
                
                # ì¸ì½”ë”© ì²˜ë¦¬ (í•œê¸€)
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    content_type = response.headers.get('Content-Type', '')
                    if 'euc-kr' in content_type.lower() or 'euckr' in content_type.lower():
                        response.encoding = 'euc-kr'
                    else:
                        response.encoding = 'utf-8'
                
                return response.text
            
            except requests.exceptions.SSLError as e:
                if attempt < self.max_retries:
                    self.logger.warning(
                        f"SSL ì˜¤ë¥˜ (ì‹œë„ {attempt}/{self.max_retries}): {url} - {e}. "
                        f"{self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„..."
                    )
                    time.sleep(self.retry_delay)
                else:
                    self.logger.error(f"í˜ì´ì§€ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨ (SSL ì˜¤ë¥˜): {url} - {e}")
                    return None
            
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    self.logger.warning(
                        f"í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨ (ì‹œë„ {attempt}/{self.max_retries}): {url} - {e}. "
                        f"{self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„..."
                    )
                    time.sleep(self.retry_delay)
                else:
                    self.logger.error(f"í˜ì´ì§€ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨: {url} - {e}")
                    return None
        
        return None
    
    def pre_test_connection(self, url: Optional[str] = None) -> Tuple[bool, str]:
        """
        ì‚¬ì „ ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Args:
            url: í…ŒìŠ¤íŠ¸í•  URL (Noneì´ë©´ ê¸°ë³¸ URL ì‚¬ìš©)
            
        Returns:
            (success, message)
        """
        if not self.use_adaptive or not self.adaptive_crawler:
            self.logger.warning("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ì‚¬ì „ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True, "ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ë¹„í™œì„±í™”"
        
        test_url = url or f"{self.REPORT_LIST_URL}"
        return self.adaptive_crawler.pre_test(test_url)
    
    def get_crawler_status(self) -> Optional[Dict]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        if self.use_adaptive and self.adaptive_crawler:
            return self.adaptive_crawler.get_status()
        return None
    
    def _extract_report_links(self, html: str) -> List[str]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³´ê³ ì„œ ë§í¬ ì¶”ì¶œ
        
        38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì‹¤ì œ êµ¬ì¡°:
        - ë¦¬í¬íŠ¸ ëª©ë¡: /html/news/?m=kosdaq&nkey=report
        - ìƒì„¸ í˜ì´ì§€: /html/news/?o=v&m=kosdaq&key=report&no=1879932&page=1
        """
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # íŒ¨í„´ 1: ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ (o=v&no= íŒ¨í„´)
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´
            if ('o=v' in href or 'no=' in href) and ('report' in href.lower() or 'key=report' in href):
                # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
                if href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.BASE_URL, href)
                links.append(full_url)
        
        # íŒ¨í„´ 2: ê¸°ì¡´ íŒ¨í„´ (í•˜ìœ„ í˜¸í™˜ì„±)
        if not links:
            for link in soup.find_all('a', href=True):
                href = link['href']
                
                if 'research_view' in href or 'report_view' in href:
                    full_url = urljoin(self.REPORT_LIST_URL, href)
                    links.append(full_url)
        
        # ì¤‘ë³µ ì œê±°
        links = list(dict.fromkeys(links))
        
        return links
    
    def _crawl_report_detail(self, url: str) -> Optional[ReportMetadata]:
        """
        ë³´ê³ ì„œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        
        38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ êµ¬ì¡° (ì˜ˆìƒ):
        <div class="report-info">
          <h2>ì‚¼ì„±ì „ì - 4Q24 Preview</h2>
          <div class="analyst">í™ê¸¸ë™ / ì‚¼ì„±ì¦ê¶Œ / IT</div>
          <div class="date">2024.12.30</div>
          <div class="opinion">ë§¤ìˆ˜</div>
          <div class="target">ëª©í‘œê°€: 75,000ì›</div>
        </div>
        """
        
        html = self._fetch(url)
        
        if not html:
            return None
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            
            if not title:
                self.logger.warning(f"ì œëª© ì—†ìŒ: {url}")
                return None
            
            # ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´
            analyst_info = self._extract_analyst(soup)
            
            # ì¢…ëª© ì •ë³´
            stock_info = self._extract_stock(soup)
            
            # ë‚ ì§œ
            published_date = self._extract_date(soup)
            
            # íˆ¬ìì˜ê²¬
            opinion = self._extract_opinion(soup)
            
            # ëª©í‘œê°€
            target_price = self._extract_target_price(soup)
            
            # ë³´ê³ ì„œ ID ìƒì„±
            report_id = self._generate_report_id(url, title)
            
            return ReportMetadata(
                report_id=report_id,
                title=title,
                stock_code=stock_info.get('code', 'UNKNOWN'),
                stock_name=stock_info.get('name', 'UNKNOWN'),
                analyst_name=analyst_info.get('name', 'UNKNOWN'),
                firm=analyst_info.get('firm', 'UNKNOWN'),
                published_date=published_date,
                source_url=url,
                investment_opinion=opinion,
                target_price=target_price
            )
        
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ"""
        
        # íŒ¨í„´ ì‹œë„ ìˆœì„œ (ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ë°˜ì˜)
        patterns = [
            ('span', {'id': 'subject'}),  # 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì‹¤ì œ êµ¬ì¡°
            ('b', {}),  # <b>íƒœê·¸ ì•ˆì˜ ì œëª©
            ('h1', {}),
            ('h2', {'class': 'report-title'}),
            ('h2', {}),
            ('div', {'class': 'title'}),
            ('title', {}),
        ]
        
        for tag, attrs in patterns:
            element = soup.find(tag, attrs)
            if element:
                text = element.get_text(strip=True)
                # ì˜ë¯¸ìˆëŠ” ì œëª©ì¸ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì œì™¸)
                if text and 5 < len(text) < 500 and not text.startswith('ë¹„ìƒì¥ì£¼ì‹ê±°ë˜'):
                    return text
        
        return None
    
    def _extract_analyst(self, soup: BeautifulSoup) -> dict:
        """
        ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
        
        í˜•ì‹: "í™ê¸¸ë™ / ì‚¼ì„±ì¦ê¶Œ / ITíŒ€"
        """
        
        # íŒ¨í„´ 1: analyst í´ë˜ìŠ¤
        analyst_div = soup.find('div', {'class': 'analyst'})
        
        if not analyst_div:
            # íŒ¨í„´ 2: í…ìŠ¤íŠ¸ ê²€ìƒ‰
            for div in soup.find_all('div'):
                text = div.get_text(strip=True)
                if 'ì¦ê¶Œ' in text and '/' in text:
                    analyst_div = div
                    break
        
        if analyst_div:
            text = analyst_div.get_text(strip=True)
            parts = [p.strip() for p in text.split('/')]
            
            return {
                'name': parts[0] if len(parts) > 0 else 'UNKNOWN',
                'firm': parts[1] if len(parts) > 1 else 'UNKNOWN',
                'department': parts[2] if len(parts) > 2 else None
            }
        
        return {'name': 'UNKNOWN', 'firm': 'UNKNOWN'}
    
    def _extract_stock(self, soup: BeautifulSoup) -> dict:
        """ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        
        # ì œëª©ì—ì„œ ì¶”ì¶œ ì‹œë„
        title = self._extract_title(soup)
        
        if title:
            # "ì‚¼ì„±ì „ì - 4Q24 Preview" â†’ "ì‚¼ì„±ì „ì"
            stock_name = title.split('-')[0].strip()
            
            # ì¢…ëª© ì½”ë“œëŠ” ë³„ë„ ì¡°íšŒ í•„ìš”
            # (38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì— ìˆì„ ìˆ˜ë„)
            stock_code = self._find_stock_code(soup) or 'UNKNOWN'
            
            return {
                'name': stock_name,
                'code': stock_code
            }
        
        return {'name': 'UNKNOWN', 'code': 'UNKNOWN'}
    
    def _find_stock_code(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¢…ëª© ì½”ë“œ ì°¾ê¸°"""
        
        # íŒ¨í„´ 1: ì§ì ‘ í‘œì‹œ
        code_span = soup.find('span', {'class': 'stock-code'})
        if code_span:
            return code_span.get_text(strip=True)
        
        # íŒ¨í„´ 2: í…ìŠ¤íŠ¸ì—ì„œ 6ìë¦¬ ìˆ«ì ì°¾ê¸°
        import re
        text = soup.get_text()
        
        # í•œêµ­ ì£¼ì‹ì€ 6ìë¦¬
        codes = re.findall(r'\b\d{6}\b', text)
        
        if codes:
            return codes[0]
        
        return None
    
    def _extract_date(self, soup: BeautifulSoup) -> datetime:
        """ë‚ ì§œ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: "2025ë…„ 12ì›” 30ì¼" í˜•ì‹ (38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì‹¤ì œ êµ¬ì¡°)
        import re
        text = soup.get_text()
        
        # "2025ë…„ 12ì›” 30ì¼" íŒ¨í„´
        date_match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text)
        if date_match:
            year, month, day = date_match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except ValueError:
                pass
        
        # íŒ¨í„´ 2: date í´ë˜ìŠ¤
        date_div = soup.find('div', {'class': 'date'})
        
        if not date_div:
            # íŒ¨í„´ 3: ë‚ ì§œ í˜•ì‹ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            for div in soup.find_all('div', 'td'):
                text = div.get_text(strip=True)
                if self._looks_like_date(text):
                    date_div = div
                    break
        
        if date_div:
            text = date_div.get_text(strip=True)
            return self._parse_date(text)
        
        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜
        return datetime.now()
    
    def _looks_like_date(self, text: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸"""
        import re
        
        # "2024.12.30", "2024-12-30", "2024/12/30"
        pattern = r'20\d{2}[./-]\d{1,2}[./-]\d{1,2}'
        return bool(re.search(pattern, text))
    
    def _parse_date(self, text: str) -> datetime:
        """ë‚ ì§œ íŒŒì‹±"""
        import re
        
        # "2024.12.30 14:30" â†’ "2024.12.30"
        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        
        if match:
            year, month, day = match.groups()
            return datetime(int(year), int(month), int(day))
        
        return datetime.now()
    
    def _extract_opinion(self, soup: BeautifulSoup) -> Optional[str]:
        """íˆ¬ìì˜ê²¬ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: opinion í´ë˜ìŠ¤
        opinion_div = soup.find('div', {'class': 'opinion'})
        
        if not opinion_div:
            # íŒ¨í„´ 2: í‚¤ì›Œë“œ ê²€ìƒ‰
            for div in soup.find_all('div'):
                text = div.get_text(strip=True)
                if any(word in text for word in ['ë§¤ìˆ˜', 'ì¤‘ë¦½', 'ë§¤ë„', 'Buy', 'Hold', 'Sell']):
                    opinion_div = div
                    break
        
        if opinion_div:
            text = opinion_div.get_text(strip=True)
            
            # ì •ê·œí™”
            if 'ë§¤ìˆ˜' in text or 'Buy' in text:
                return 'buy'
            elif 'ë§¤ë„' in text or 'Sell' in text:
                return 'sell'
            elif 'ì¤‘ë¦½' in text or 'Hold' in text:
                return 'hold'
        
        return None
    
    def _extract_target_price(self, soup: BeautifulSoup) -> Optional[str]:
        """ëª©í‘œê°€ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: target í´ë˜ìŠ¤
        target_div = soup.find('div', {'class': 'target'})
        
        if not target_div:
            # íŒ¨í„´ 2: "ëª©í‘œê°€" í…ìŠ¤íŠ¸ ê²€ìƒ‰
            for div in soup.find_all('div'):
                text = div.get_text(strip=True)
                if 'ëª©í‘œê°€' in text:
                    target_div = div
                    break
        
        if target_div:
            text = target_div.get_text(strip=True)
            
            # "ëª©í‘œê°€: 75,000ì›" â†’ "75,000ì›"
            import re
            match = re.search(r'[\d,]+ì›?', text)
            
            if match:
                return match.group()
        
        return None
    
    def _generate_report_id(self, url: str, title: str) -> str:
        """ë³´ê³ ì„œ ID ìƒì„±"""
        
        # URL + ì œëª©ì˜ í•´ì‹œ
        content = f"{url}:{title}"
        
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def save_to_json(self, reports: List[ReportMetadata], filename: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        data = [report.to_dict() for report in reports]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def save_to_csv(self, reports: List[ReportMetadata], filename: str):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        import csv
        
        if not reports:
            return
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=reports[0].to_dict().keys())
            writer.writeheader()
            
            for report in reports:
                writer.writerow(report.to_dict())
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ThirtyEightComCrawler(delay=3.0)
    
    # ìµœê·¼ 1ì¼ ë³´ê³ ì„œ ìˆ˜ì§‘
    print("ğŸš€ 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ëŸ¬ ì‹œì‘\n")
    
    reports = crawler.crawl_recent_reports(
        days=1,
        max_reports=20  # í…ŒìŠ¤íŠ¸ìš© 20ê°œë§Œ
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(reports)}ê°œ\n")
    
    for i, report in enumerate(reports, 1):
        print(f"{i}. {report.stock_name} ({report.stock_code})")
        print(f"   ì œëª©: {report.title}")
        print(f"   ì• ë„ë¦¬ìŠ¤íŠ¸: {report.analyst_name} ({report.firm})")
        print(f"   ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}")
        
        if report.investment_opinion:
            print(f"   ì˜ê²¬: {report.investment_opinion}")
        
        if report.target_price:
            print(f"   ëª©í‘œê°€: {report.target_price}")
        
        print()
    
    # ì €ì¥
    if reports:
        crawler.save_to_json(reports, '38com_reports.json')
        crawler.save_to_csv(reports, '38com_reports.csv')
    
    print("âœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()

