# ollama_llm.py
"""
Ollama LLM í”„ë¡œì„¸ì„œ

ë¡œì»¬ Ollama ì„œë²„ì™€ ì—°ë™í•˜ì—¬ LLM ì‚¬ìš©
"""

import requests
import json
import time
import logging
from typing import Optional

class OllamaLLM:
    """Ollama LLM í”„ë¡œì„¸ì„œ"""
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3",
        timeout: int = 120
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            base_url: Ollama ì„œë²„ URL
            model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (llama3, mistral, codellama ë“±)
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_connection()
    
    def _test_connection(self):
        """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in models]
                
                self.logger.info(f"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
                self.logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(model_names[:5])}")
                
                # ì§€ì •ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                if not any(self.model in name for name in model_names):
                    self.logger.warning(
                        f"âš ï¸  ëª¨ë¸ '{self.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                        f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ 'ollama pull {self.model}'ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
                    )
                    if model_names:
                        self.logger.info(f"   ëŒ€ì‹  '{model_names[0]}' ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        self.model = model_names[0].split(':')[0]  # íƒœê·¸ ì œê±°
            else:
                self.logger.error(f"âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                
        except requests.exceptions.ConnectionError:
            self.logger.error(
                f"âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                f"   ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: 'ollama serve' ë˜ëŠ” Ollama ì•± ì‹¤í–‰"
            )
        except Exception as e:
            self.logger.error(f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def process(self, prompt: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        
        self.logger.info(f"ğŸ¤– Ollama LLM ì²˜ë¦¬ ì‹œì‘ (ëª¨ë¸: {self.model})...")
        start_time = time.time()
        
        try:
            # Ollama API í˜¸ì¶œ
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ì „ì²´ ì‘ë‹µ í•œë²ˆì— ë°›ê¸°)
                    "options": {
                        "temperature": 0.3,  # ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µì„ ìœ„í•´ ë‚®ì€ temperature
                        "top_p": 0.9,
                    }
                },
                timeout=self.timeout
            )
            
            if response.status_code != 200:
                error_msg = f"Ollama API ì˜¤ë¥˜: {response.status_code}"
                self.logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            result = response.json()
            
            # ì‘ë‹µ ì¶”ì¶œ
            if 'response' in result:
                output = result['response']
            else:
                output = str(result)
            
            elapsed = time.time() - start_time
            self.logger.info(f"âœ… LLM ì²˜ë¦¬ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
            
            return output
            
        except requests.exceptions.Timeout:
            error_msg = f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ ì´ˆê³¼)"
            self.logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
            
        except requests.exceptions.ConnectionError:
            error_msg = "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            self.logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)
            
        except Exception as e:
            self.logger.error(f"âŒ LLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def list_models(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                return [m.get('name', '') for m in models]
            else:
                return []
                
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

if __name__ == "__main__":
    import sys
    import io
    
    # Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')
        except:
            pass
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("\n" + "="*60)
    print("Ollama LLM í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    # Ollama LLM ì´ˆê¸°í™”
    try:
        llm = OllamaLLM(model="llama3")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        models = llm.list_models()
        if models:
            print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models)}ê°œ")
            for model in models[:5]:
                print(f"   - {model}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ¤– í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...")
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ìê¸°ì†Œê°œ í•´ì£¼ì„¸ìš”."
        response = llm.process(test_prompt)
        
        print(f"\nâœ… ì‘ë‹µ:")
        print(f"{response}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸: 'ollama serve'")
        print("   2. ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸: 'ollama list'")
        print("   3. ëª¨ë¸ì´ ì—†ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ: 'ollama pull llama3'")

