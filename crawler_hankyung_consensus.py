# crawler_hankyung_consensus.py
"""
í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬

í•œê²½ì½”ë¦¬ì•„ë§ˆì¼“ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
https://markets.hankyung.com/consensus

ê³„ì•½:
- ì…ë ¥: days (int, ìµœê·¼ Nì¼), max_reports (int, ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜)
- ì¶œë ¥: List[ReportMetadata] (ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
- ì˜ˆì™¸: requests.RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜), ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°)
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import json
import hashlib
from urllib.parse import urljoin, urlparse, urlencode
import urllib3
import re

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
try:
    from adaptive_crawler import AdaptiveCrawler, SiteProfile
    ADAPTIVE_CRAWLER_AVAILABLE = True
except ImportError:
    ADAPTIVE_CRAWLER_AVAILABLE = False

@dataclass
class ReportMetadata:
    """ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„°"""
    report_id: str
    title: str
    stock_code: str
    stock_name: str
    analyst_name: str
    firm: str
    published_date: datetime
    source_url: str
    
    # ì¶”ê°€ ì •ë³´ (ìˆìœ¼ë©´)
    investment_opinion: Optional[str] = None
    target_price: Optional[str] = None
    current_price: Optional[str] = None
    consensus_rating: Optional[str] = None
    
    def to_dict(self) -> dict:
        data = asdict(self)
        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        data['published_date'] = self.published_date.isoformat()
        return data

class HankyungConsensusCrawler:
    """
    í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬
    
    ì‚¬ìš©ë²•:
        crawler = HankyungConsensusCrawler()
        reports = crawler.crawl_recent_reports(days=1)
        
        # íŠ¹ì • ì¢…ëª© ê²€ìƒ‰
        reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", days=7)
        
        # í•„í„°ë§ ì˜µì…˜
        reports = crawler.crawl_recent_reports(
            days=7,
            report_type="stock",  # stock, industry, market, analyst
            firm_filter=None  # íŠ¹ì • ì¦ê¶Œì‚¬ í•„í„°
        )
    """
    
    BASE_URL = "https://markets.hankyung.com"
    CONSENSUS_URL = "https://markets.hankyung.com/consensus"
    
    # ë¦¬í¬íŠ¸ ìœ í˜•
    REPORT_TYPE_STOCK = "stock"  # ì¢…ëª© ë¦¬í¬íŠ¸
    REPORT_TYPE_INDUSTRY = "industry"  # ì‚°ì—… ë¦¬í¬íŠ¸
    REPORT_TYPE_MARKET = "market"  # ì‹œí™©/ì „ëµ ë¦¬í¬íŠ¸
    REPORT_TYPE_ANALYST = "analyst"  # ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸
    
    def __init__(self, delay: float = 3.0, max_retries: int = 3, retry_delay: float = 5.0,
                 use_adaptive: bool = True, site_domain: str = "markets.hankyung.com"):
        """
        ì´ˆê¸°í™”
        
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            use_adaptive: ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš© ì—¬ë¶€
            site_domain: ì‚¬ì´íŠ¸ ë„ë©”ì¸
        """
        self.delay = delay
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.use_adaptive = use_adaptive and ADAPTIVE_CRAWLER_AVAILABLE
        self.site_domain = site_domain
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        if self.use_adaptive:
            profile = SiteProfile(
                domain=site_domain,
                base_delay=delay,
                max_retries=max_retries
            )
            self.adaptive_crawler = AdaptiveCrawler(profile)
            self.session = self.adaptive_crawler.session
            self.logger.info("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ í™œì„±í™”")
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://markets.hankyung.com/',
            })
            self.adaptive_crawler = None
    
    def crawl_recent_reports(
        self, 
        days: int = 1,
        max_reports: int = 100,
        report_type: str = "stock",
        firm_filter: Optional[str] = None
    ) -> List[ReportMetadata]:
        """
        ìµœê·¼ ë³´ê³ ì„œ í¬ë¡¤ë§
        
        Args:
            days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’: 1)
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
            report_type: ë¦¬í¬íŠ¸ ìœ í˜• (ê¸°ë³¸ê°’: "stock")
                - "stock": ì¢…ëª© ë¦¬í¬íŠ¸
                - "industry": ì‚°ì—… ë¦¬í¬íŠ¸
                - "market": ì‹œí™©/ì „ëµ ë¦¬í¬íŠ¸
                - "analyst": ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸
            firm_filter: ì¦ê¶Œì‚¬ í•„í„° (Noneì´ë©´ ì „ì²´, ê¸°ë³¸ê°’: None)
            
        Returns:
            List[ReportMetadata]: ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì˜ëª»ëœ report_type ë˜ëŠ” days < 0
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨
            
        ê³„ì•½:
        - ì…ë ¥: daysëŠ” ì–‘ìˆ˜, report_typeì€ ìœ íš¨í•œ ê°’
        - ì¶œë ¥: ReportMetadata ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        - ì˜ˆì™¸: ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)
        """
        
        # ì…ë ¥ ê²€ì¦
        if days < 0:
            raise ValueError(f"days must be non-negative, got {days}")
        if max_reports < 0:
            raise ValueError(f"max_reports must be non-negative, got {max_reports}")
        if report_type not in [self.REPORT_TYPE_STOCK, self.REPORT_TYPE_INDUSTRY, 
                               self.REPORT_TYPE_MARKET, self.REPORT_TYPE_ANALYST]:
            raise ValueError(f"Invalid report_type: {report_type}")
        
        self.logger.info(f"ğŸ“Š í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ë§ ì‹œì‘: ìµœê·¼ {days}ì¼, ìœ í˜•={report_type}")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        try:
            # 1. ì¢…ëª© ë¦¬í¬íŠ¸ íƒ­ìœ¼ë¡œ ì´ë™ (ê¸°ë³¸ê°’)
            if report_type == "stock":
                # ì¢…ëª© ë¦¬í¬íŠ¸ í˜ì´ì§€ URL êµ¬ì„±
                # ì‹¤ì œ APIë‚˜ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ì¡°ì • í•„ìš”
                list_url = f"{self.CONSENSUS_URL}?type=stock"
            else:
                list_url = f"{self.CONSENSUS_URL}?type={report_type}"
            
            self.logger.info(f"ğŸ” ëª©ë¡ ì¡°íšŒ: {list_url}")
            html = self._fetch(list_url)
            
            if not html:
                # ê¸°ë³¸ URLë¡œ ì¬ì‹œë„
                self.logger.warning("í•„í„° URL ì‹¤íŒ¨, ê¸°ë³¸ URLë¡œ ì¬ì‹œë„")
                html = self._fetch(self.CONSENSUS_URL)
            
            if not html:
                self.logger.error("ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨")
                return []
            
            # 2. ë³´ê³ ì„œ ë§í¬ ì¶”ì¶œ (ì¢…ëª© ë¦¬í¬íŠ¸ ëª©ë¡)
            report_links = self._extract_report_links(html, report_type=report_type)
            
            self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë³´ê³ ì„œ: {len(report_links)}ê°œ")
            
            # 3. ê° ë³´ê³ ì„œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            total_links = min(len(report_links), max_reports)
            
            for i, link in enumerate(report_links[:max_reports], 1):
                progress = f"[{i}/{total_links}]"
                self.logger.info(f"{progress} ì²˜ë¦¬ ì¤‘: {link[:80]}...")
                
                report = self._crawl_report_detail(link)
                
                if report:
                    # ë‚ ì§œ í•„í„°ë§
                    if report.published_date >= cutoff_date:
                        # ì¦ê¶Œì‚¬ í•„í„°ë§
                        if firm_filter and firm_filter not in report.firm:
                            self.logger.info(f"{progress} â­ï¸  ì¦ê¶Œì‚¬ í•„í„° ë¶ˆì¼ì¹˜: {report.firm}")
                            continue
                        
                        reports.append(report)
                        self.logger.info(
                            f"{progress} âœ… ìˆ˜ì§‘: {report.stock_name} - {report.analyst_name} ({report.firm})"
                        )
                    else:
                        self.logger.info(f"{progress} â­ï¸  ì˜¤ë˜ëœ ë³´ê³ ì„œ (ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')})")
                        # ë‚ ì§œê°€ ì˜¤ë˜ëœ ê²½ìš° ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ (ìµœì‹ ìˆœ ì •ë ¬ ê°€ì •)
                        if i > 10:  # ìµœì†Œ 10ê°œëŠ” í™•ì¸
                            break
                else:
                    self.logger.warning(f"{progress} âŒ ì¶”ì¶œ ì‹¤íŒ¨")
                
                # ì˜ˆì˜ë°”ë¥¸ ëŒ€ê¸°
                if i < total_links:
                    time.sleep(self.delay)
            
            self.logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(reports)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}", exc_info=True)
        
        return reports
    
    def search_by_stock(
        self,
        stock_name: str,
        days: int = 7,
        max_reports: int = 50
    ) -> List[ReportMetadata]:
        """
        íŠ¹ì • ì¢…ëª©ìœ¼ë¡œ ë¦¬í¬íŠ¸ ê²€ìƒ‰
        
        Args:
            stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’: 7)
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 50)
            
        Returns:
            List[ReportMetadata]: ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì˜ëª»ëœ stock_name ë˜ëŠ” days < 0
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨
            
        ê³„ì•½:
        - ì…ë ¥: stock_nameì€ ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ìì—´, daysëŠ” ì–‘ìˆ˜
        - ì¶œë ¥: ReportMetadata ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        - ì˜ˆì™¸: ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)
        """
        
        # ì…ë ¥ ê²€ì¦
        if not stock_name or not isinstance(stock_name, str):
            raise ValueError(f"stock_name must be a non-empty string, got {stock_name}")
        if days < 0:
            raise ValueError(f"days must be non-negative, got {days}")
        if max_reports < 0:
            raise ValueError(f"max_reports must be non-negative, got {max_reports}")
        
        self.logger.info(f"ğŸ” ì¢…ëª© ê²€ìƒ‰: {stock_name} (ìµœê·¼ {days}ì¼)")
        
        # ì¢…ëª© ë¦¬í¬íŠ¸ íƒ­ìœ¼ë¡œ ì´ë™ í›„ ê²€ìƒ‰
        # ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ë‚˜ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ì— ë§ì¶° ì¡°ì • í•„ìš”
        search_url = f"{self.CONSENSUS_URL}?type=stock&search={stock_name}"
        
        html = self._fetch(search_url)
        
        if not html:
            self.logger.error(f"ì¢…ëª© ê²€ìƒ‰ ì‹¤íŒ¨: {stock_name}")
            return []
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¦¬í¬íŠ¸ ë§í¬ ì¶”ì¶œ
        report_links = self._extract_report_links(html, report_type="stock")
        
        self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë¦¬í¬íŠ¸: {len(report_links)}ê°œ")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for i, link in enumerate(report_links[:max_reports], 1):
            report = self._crawl_report_detail(link)
            
            if report:
                # ì¢…ëª©ëª… í™•ì¸
                if stock_name not in report.stock_name:
                    continue
                
                # ë‚ ì§œ í•„í„°ë§
                if report.published_date >= cutoff_date:
                    reports.append(report)
                    self.logger.info(
                        f"[{i}] âœ… {report.stock_name} - {report.analyst_name} ({report.firm})"
                    )
            
            if i < len(report_links):
                time.sleep(self.delay)
        
        return reports
    
    def _fetch(self, url: str) -> Optional[str]:
        """
        í˜ì´ì§€ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨, ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì§€ì›)
        
        Args:
            url: ì¡°íšŒí•  URL
            
        Returns:
            Optional[str]: HTML ë‚´ìš© (ì‹¤íŒ¨ ì‹œ None)
            
        Raises:
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ ì‹œ)
        """
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš©
        if self.use_adaptive and self.adaptive_crawler:
            response = self.adaptive_crawler.fetch(url)
            if response:
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                return response.text
            return None
        
        # ê¸°ë³¸ í¬ë¡¤ëŸ¬ (ê¸°ì¡´ ë¡œì§)
        for attempt in range(1, self.max_retries + 1):
            try:
                response = self.session.get(url, timeout=10, verify=True)
                response.raise_for_status()
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                return response.text
            
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    self.logger.warning(
                        f"í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨ (ì‹œë„ {attempt}/{self.max_retries}): {url} - {e}. "
                        f"{self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„..."
                    )
                    time.sleep(self.retry_delay)
                else:
                    self.logger.error(f"í˜ì´ì§€ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨: {url} - {e}")
                    return None
        
        return None
    
    def pre_test_connection(self, url: Optional[str] = None) -> Tuple[bool, str]:
        """
        ì‚¬ì „ ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Args:
            url: í…ŒìŠ¤íŠ¸í•  URL (Noneì´ë©´ ê¸°ë³¸ URL ì‚¬ìš©, ê¸°ë³¸ê°’: None)
            
        Returns:
            Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
                - (True, "ì—°ê²° ì„±ê³µ") ë˜ëŠ” (False, "ì—°ê²° ì‹¤íŒ¨: ì´ìœ ")
            
        Raises:
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
        """
        if not self.use_adaptive or not self.adaptive_crawler:
            self.logger.warning("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ì‚¬ì „ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True, "ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ë¹„í™œì„±í™”"
        
        test_url = url or self.CONSENSUS_URL
        return self.adaptive_crawler.pre_test(test_url)
    
    def get_crawler_status(self) -> Optional[Dict]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        if self.use_adaptive and self.adaptive_crawler:
            return self.adaptive_crawler.get_status()
        return None
    
    def _extract_report_links(self, html: str, report_type: str = "stock") -> List[str]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³´ê³ ì„œ ë§í¬ ì¶”ì¶œ
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ëª©ë¡ êµ¬ì¡°:
        - ë¦¬í¬íŠ¸ ëª©ë¡ì€ í…Œì´ë¸” ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        - ê° ë¦¬í¬íŠ¸ í–‰ì— ë§í¬ê°€ ìˆìŒ
        - ë¦¬í¬íŠ¸ ë³´ê¸° / PDF ë²„íŠ¼ì´ ìˆìŒ
        """
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # íŒ¨í„´ 1: ë¦¬í¬íŠ¸ ëª©ë¡ í…Œì´ë¸”ì—ì„œ ë§í¬ ì¶”ì¶œ
        # í•œê²½ ì»¨ì„¼ì„œìŠ¤ëŠ” ë³´í†µ <table> ë˜ëŠ” <ul>/<li> êµ¬ì¡° ì‚¬ìš©
        tables = soup.find_all('table')
        for table in tables:
            for row in table.find_all('tr'):
                for cell in row.find_all(['td', 'th']):
                    for link in cell.find_all('a', href=True):
                        href = link['href']
                        link_text = link.get_text(strip=True)
                        
                        # ë¦¬í¬íŠ¸ ê´€ë ¨ ë§í¬ í™•ì¸
                        if any(keyword in link_text.lower() for keyword in ['ë¦¬í¬íŠ¸', 'ë³´ê¸°', 'pdf', 'report', 'view']):
                            if href.startswith('http'):
                                full_url = href
                            else:
                                full_url = urljoin(self.BASE_URL, href)
                            
                            if full_url not in links:
                                links.append(full_url)
        
        # íŒ¨í„´ 2: ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ì—ì„œ ë§í¬ ì¶”ì¶œ
        lists = soup.find_all(['ul', 'ol', 'div'], class_=re.compile(r'list|report|item', re.I))
        for list_elem in lists:
            for item in list_elem.find_all(['li', 'div'], recursive=False):
                for link in item.find_all('a', href=True):
                    href = link['href']
                    
                    # ë¦¬í¬íŠ¸ ë§í¬ íŒ¨í„´
                    if '/consensus' in href or 'report' in href.lower() or 'analyst' in href.lower():
                        if href.startswith('http'):
                            full_url = href
                        else:
                            full_url = urljoin(self.BASE_URL, href)
                        
                        if full_url not in links:
                            links.append(full_url)
        
        # íŒ¨í„´ 3: ëª¨ë“  ë§í¬ì—ì„œ ë¦¬í¬íŠ¸ ê´€ë ¨ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link['href']
            link_text = link.get_text(strip=True)
            
            # ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ íŒ¨í„´
            if any(pattern in href.lower() for pattern in ['/consensus/', '/report/', '/analyst/', 'detail', 'view']):
                if href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.BASE_URL, href)
                
                # ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± í™•ì¸
                if full_url not in links and self.BASE_URL in full_url:
                    links.append(full_url)
        
        # íŒ¨í„´ 4: ë°ì´í„° ì†ì„±ì—ì„œ ë§í¬ ì¶”ì¶œ (ë™ì  ë¡œë”©)
        for element in soup.find_all(attrs={'data-url': True}):
            href = element.get('data-url')
            if href:
                if href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.BASE_URL, href)
                if full_url not in links:
                    links.append(full_url)
        
        # íŒ¨í„´ 5: onclick ì´ë²¤íŠ¸ì—ì„œ URL ì¶”ì¶œ
        for element in soup.find_all(attrs={'onclick': True}):
            onclick = element.get('onclick', '')
            # onclick="location.href='/consensus/...'" íŒ¨í„´
            match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick)
            if match:
                href = match.group(1)
                if href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.BASE_URL, href)
                if full_url not in links:
                    links.append(full_url)
        
        # ì¤‘ë³µ ì œê±°
        links = list(dict.fromkeys(links))
        
        # ë¦¬í¬íŠ¸ ë§í¬ë§Œ í•„í„°ë§ (ë¶ˆí•„ìš”í•œ ë§í¬ ì œê±°)
        filtered_links = []
        for link in links:
            # ë©”ì¸ í˜ì´ì§€, ë¡œê·¸ì¸, ê´‘ê³  ë“± ì œì™¸
            if any(exclude in link.lower() for exclude in ['login', 'signup', 'ad', 'banner', 'main', 'index']):
                continue
            filtered_links.append(link)
        
        self.logger.info(f"ì¶”ì¶œëœ ë§í¬: {len(filtered_links)}ê°œ (ì „ì²´ {len(links)}ê°œ ì¤‘)")
        
        return filtered_links
    
    def _crawl_report_detail(self, url: str) -> Optional[ReportMetadata]:
        """
        ë³´ê³ ì„œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ êµ¬ì¡° ë¶„ì„ í•„ìš”
        """
        
        html = self._fetch(url)
        
        if not html:
            return None
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            
            if not title:
                self.logger.warning(f"ì œëª© ì—†ìŒ: {url}")
                return None
            
            # ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´
            analyst_info = self._extract_analyst(soup)
            
            # ì¢…ëª© ì •ë³´
            stock_info = self._extract_stock(soup)
            
            # ë‚ ì§œ
            published_date = self._extract_date(soup)
            
            # íˆ¬ìì˜ê²¬
            opinion = self._extract_opinion(soup)
            
            # ëª©í‘œê°€
            target_price = self._extract_target_price(soup)
            
            # í˜„ì¬ê°€
            current_price = self._extract_current_price(soup)
            
            # ì»¨ì„¼ì„œìŠ¤ ë“±ê¸‰
            consensus_rating = self._extract_consensus_rating(soup)
            
            # ë³´ê³ ì„œ ID ìƒì„±
            report_id = self._generate_report_id(url, title)
            
            return ReportMetadata(
                report_id=report_id,
                title=title,
                stock_code=stock_info.get('code', 'UNKNOWN'),
                stock_name=stock_info.get('name', 'UNKNOWN'),
                analyst_name=analyst_info.get('name', 'UNKNOWN'),
                firm=analyst_info.get('firm', 'UNKNOWN'),
                published_date=published_date,
                source_url=url,
                investment_opinion=opinion,
                target_price=target_price,
                current_price=current_price,
                consensus_rating=consensus_rating
            )
        
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ"""
        
        # íŒ¨í„´ ì‹œë„ ìˆœì„œ
        patterns = [
            ('h1', {}),
            ('h2', {}),
            ('h3', {}),
            ('div', {'class': re.compile(r'title', re.I)}),
            ('span', {'class': re.compile(r'title', re.I)}),
            ('title', {}),
        ]
        
        for tag, attrs in patterns:
            element = soup.find(tag, attrs)
            if element:
                text = element.get_text(strip=True)
                if text and 5 < len(text) < 500:
                    return text
        
        return None
    
    def _extract_analyst(self, soup: BeautifulSoup) -> dict:
        """
        ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ êµ¬ì¡°:
        - ì¦ê¶Œì‚¬ëª…: ë³´í†µ í…Œì´ë¸”ì´ë‚˜ íŠ¹ì • ì˜ì—­ì— í‘œì‹œ
        - ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„: ì¦ê¶Œì‚¬ëª…ê³¼ í•¨ê»˜ í‘œì‹œ
        - í˜•ì‹: "ì• ë„ë¦¬ìŠ¤íŠ¸ëª… / ì¦ê¶Œì‚¬ëª…" ë˜ëŠ” ë³„ë„ í•„ë“œ
        """
        
        # íŒ¨í„´ 1: analyst, firm, company í´ë˜ìŠ¤
        analyst_elem = soup.find(['div', 'span', 'td'], {'class': re.compile(r'analyst|writer|author', re.I)})
        firm_elem = soup.find(['div', 'span', 'td'], {'class': re.compile(r'firm|company|sec|ì¦ê¶Œ', re.I)})
        
        analyst_name = 'UNKNOWN'
        firm_name = 'UNKNOWN'
        
        if analyst_elem:
            analyst_name = analyst_elem.get_text(strip=True)
        
        if firm_elem:
            firm_name = firm_elem.get_text(strip=True)
        
        # íŒ¨í„´ 2: í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ (í•œê²½ ì»¨ì„¼ì„œìŠ¤ëŠ” ë³´í†µ í…Œì´ë¸” êµ¬ì¡°)
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)
                    
                    # ì¦ê¶Œì‚¬ëª… ì°¾ê¸°
                    if 'ì¦ê¶Œ' in text or 'íˆ¬ì' in text or 'ìì‚°' in text:
                        firm_name = text
                        # ë‹¤ìŒ ì…€ì— ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„ì´ ìˆì„ ìˆ˜ ìˆìŒ
                        if i + 1 < len(cells):
                            next_text = cells[i + 1].get_text(strip=True)
                            if next_text and len(next_text) < 20:  # ì´ë¦„ ê¸¸ì´ ê°€ì •
                                analyst_name = next_text
                    
                    # ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„ ì°¾ê¸° (í•œê¸€ ì´ë¦„ íŒ¨í„´)
                    if re.match(r'^[ê°€-í£]{2,4}$', text) and 'ì¦ê¶Œ' not in text:
                        analyst_name = text
        
        # íŒ¨í„´ 3: í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if analyst_name == 'UNKNOWN' or firm_name == 'UNKNOWN':
            full_text = soup.get_text()
            
            # "í™ê¸¸ë™ / NHíˆ¬ìì¦ê¶Œ" íŒ¨í„´
            match = re.search(r'([ê°€-í£]{2,4})\s*[/Â·]\s*([ê°€-í£\w]+ì¦ê¶Œ)', full_text)
            if match:
                analyst_name = match.group(1)
                firm_name = match.group(2)
            else:
                # "NHíˆ¬ìì¦ê¶Œ / í™ê¸¸ë™" íŒ¨í„´
                match = re.search(r'([ê°€-í£\w]+ì¦ê¶Œ)\s*[/Â·]\s*([ê°€-í£]{2,4})', full_text)
                if match:
                    firm_name = match.group(1)
                    analyst_name = match.group(2)
        
        return {
            'name': analyst_name,
            'firm': firm_name,
            'department': None
        }
    
    def _extract_stock(self, soup: BeautifulSoup) -> dict:
        """ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        
        # ì œëª©ì—ì„œ ì¶”ì¶œ ì‹œë„
        title = self._extract_title(soup)
        
        if title:
            # "ì‚¼ì„±ì „ì - 4Q24 Preview" â†’ "ì‚¼ì„±ì „ì"
            stock_name = title.split('-')[0].split('(')[0].strip()
            
            # ì¢…ëª© ì½”ë“œ ì°¾ê¸°
            stock_code = self._find_stock_code(soup) or 'UNKNOWN'
            
            return {
                'name': stock_name,
                'code': stock_code
            }
        
        return {'name': 'UNKNOWN', 'code': 'UNKNOWN'}
    
    def _find_stock_code(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¢…ëª© ì½”ë“œ ì°¾ê¸°"""
        
        # íŒ¨í„´ 1: ì§ì ‘ í‘œì‹œ
        code_elements = soup.find_all(['span', 'div', 'td'], {'class': re.compile(r'code|stock', re.I)})
        for elem in code_elements:
            text = elem.get_text(strip=True)
            if re.match(r'^\d{6}$', text):
                return text
        
        # íŒ¨í„´ 2: í…ìŠ¤íŠ¸ì—ì„œ 6ìë¦¬ ìˆ«ì ì°¾ê¸°
        text = soup.get_text()
        codes = re.findall(r'\b\d{6}\b', text)
        
        if codes:
            return codes[0]
        
        return None
    
    def _extract_date(self, soup: BeautifulSoup) -> datetime:
        """ë‚ ì§œ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: date í´ë˜ìŠ¤
        date_elements = soup.find_all(['div', 'span', 'td'], {'class': re.compile(r'date|time', re.I)})
        
        for date_elem in date_elements:
            text = date_elem.get_text(strip=True)
            parsed = self._parse_date(text)
            if parsed:
                return parsed
        
        # íŒ¨í„´ 2: ë‚ ì§œ í˜•ì‹ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        date_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        if date_match:
            year, month, day = date_match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except ValueError:
                pass
        
        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜
        return datetime.now()
    
    def _parse_date(self, text: str) -> Optional[datetime]:
        """ë‚ ì§œ íŒŒì‹±"""
        
        # "2024.12.30 14:30" â†’ "2024.12.30"
        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        
        if match:
            year, month, day = match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except ValueError:
                pass
        
        return None
    
    def _extract_opinion(self, soup: BeautifulSoup) -> Optional[str]:
        """íˆ¬ìì˜ê²¬ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: opinion í´ë˜ìŠ¤
        opinion_elements = soup.find_all(['div', 'span'], {'class': re.compile(r'opinion|rating|recommend', re.I)})
        
        for elem in opinion_elements:
            text = elem.get_text(strip=True)
            if any(word in text for word in ['ë§¤ìˆ˜', 'ì¤‘ë¦½', 'ë§¤ë„', 'Buy', 'Hold', 'Sell', 'ê°•ë ¥ë§¤ìˆ˜', 'ë³´ìœ ']):
                # ì •ê·œí™”
                if 'ë§¤ìˆ˜' in text or 'Buy' in text:
                    return 'buy'
                elif 'ë§¤ë„' in text or 'Sell' in text:
                    return 'sell'
                elif 'ì¤‘ë¦½' in text or 'Hold' in text or 'ë³´ìœ ' in text:
                    return 'hold'
        
        # íŒ¨í„´ 2: í‚¤ì›Œë“œ ê²€ìƒ‰
        text = soup.get_text()
        if 'ë§¤ìˆ˜' in text or 'Buy' in text:
            return 'buy'
        elif 'ë§¤ë„' in text or 'Sell' in text:
            return 'sell'
        elif 'ì¤‘ë¦½' in text or 'Hold' in text or 'ë³´ìœ ' in text:
            return 'hold'
        
        return None
    
    def _extract_target_price(self, soup: BeautifulSoup) -> Optional[str]:
        """ëª©í‘œê°€ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: target í´ë˜ìŠ¤
        target_elements = soup.find_all(['div', 'span', 'td'], {'class': re.compile(r'target|price', re.I)})
        
        for elem in target_elements:
            text = elem.get_text(strip=True)
            if 'ëª©í‘œê°€' in text or 'target' in text.lower():
                match = re.search(r'[\d,]+ì›?', text)
                if match:
                    return match.group()
        
        # íŒ¨í„´ 2: "ëª©í‘œê°€" í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        if 'ëª©í‘œê°€' in text:
            match = re.search(r'ëª©í‘œê°€[:\s]*([\d,]+ì›?)', text)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_current_price(self, soup: BeautifulSoup) -> Optional[str]:
        """í˜„ì¬ê°€ ì¶”ì¶œ"""
        
        # íŒ¨í„´: í˜„ì¬ê°€ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        
        # "í˜„ì¬ê°€: 75,000ì›" íŒ¨í„´
        match = re.search(r'í˜„ì¬ê°€[:\s]*([\d,]+ì›?)', text)
        if match:
            return match.group(1)
        
        return None
    
    def _extract_consensus_rating(self, soup: BeautifulSoup) -> Optional[str]:
        """ì»¨ì„¼ì„œìŠ¤ ë“±ê¸‰ ì¶”ì¶œ"""
        
        # íŒ¨í„´: ì»¨ì„¼ì„œìŠ¤ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        consensus_elements = soup.find_all(['div', 'span'], {'class': re.compile(r'consensus|rating', re.I)})
        
        for elem in consensus_elements:
            text = elem.get_text(strip=True)
            if text and len(text) < 50:
                return text
        
        return None
    
    def _generate_report_id(self, url: str, title: str) -> str:
        """ë³´ê³ ì„œ ID ìƒì„±"""
        
        # URL + ì œëª©ì˜ í•´ì‹œ
        content = f"{url}:{title}"
        
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def save_to_json(self, reports: List[ReportMetadata], filename: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        data = [report.to_dict() for report in reports]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def save_to_csv(self, reports: List[ReportMetadata], filename: str):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        import csv
        
        if not reports:
            return
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=reports[0].to_dict().keys())
            writer.writeheader()
            
            for report in reports:
                writer.writerow(report.to_dict())
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HankyungConsensusCrawler(delay=3.0)
    
    # ìµœê·¼ 1ì¼ ë³´ê³ ì„œ ìˆ˜ì§‘
    print("ğŸš€ í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘\n")
    
    reports = crawler.crawl_recent_reports(
        days=1,
        max_reports=20  # í…ŒìŠ¤íŠ¸ìš© 20ê°œë§Œ
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(reports)}ê°œ\n")
    
    for i, report in enumerate(reports, 1):
        print(f"{i}. {report.stock_name} ({report.stock_code})")
        print(f"   ì œëª©: {report.title}")
        print(f"   ì• ë„ë¦¬ìŠ¤íŠ¸: {report.analyst_name} ({report.firm})")
        print(f"   ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}")
        
        if report.investment_opinion:
            print(f"   ì˜ê²¬: {report.investment_opinion}")
        
        if report.target_price:
            print(f"   ëª©í‘œê°€: {report.target_price}")
        
        print()
    
    # ì €ì¥
    if reports:
        crawler.save_to_json(reports, 'hankyung_consensus_reports.json')
        crawler.save_to_csv(reports, 'hankyung_consensus_reports.csv')
    
    print("âœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()

