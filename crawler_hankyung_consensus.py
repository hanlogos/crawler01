# crawler_hankyung_consensus.py
"""
í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬

í•œê²½ì½”ë¦¬ì•„ë§ˆì¼“ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
https://markets.hankyung.com/consensus

ê³„ì•½:
- ì…ë ¥: days (int, ìµœê·¼ Nì¼), max_reports (int, ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜)
- ì¶œë ¥: List[ReportMetadata] (ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
- ì˜ˆì™¸: requests.RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜), ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°)
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import json
import hashlib
from urllib.parse import urljoin, urlparse, urlencode
import urllib3
import re

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
try:
    from adaptive_crawler import AdaptiveCrawler, SiteProfile
    ADAPTIVE_CRAWLER_AVAILABLE = True
except ImportError:
    ADAPTIVE_CRAWLER_AVAILABLE = False

@dataclass
class ReportMetadata:
    """ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„°"""
    report_id: str
    title: str
    stock_code: str
    stock_name: str
    analyst_name: str
    firm: str
    published_date: datetime
    source_url: str
    
    # ì¶”ê°€ ì •ë³´ (ìˆìœ¼ë©´)
    investment_opinion: Optional[str] = None
    target_price: Optional[str] = None
    current_price: Optional[str] = None
    consensus_rating: Optional[str] = None
    
    def to_dict(self) -> dict:
        data = asdict(self)
        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        data['published_date'] = self.published_date.isoformat()
        return data

class HankyungConsensusCrawler:
    """
    í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬
    
    ì‚¬ìš©ë²•:
        crawler = HankyungConsensusCrawler()
        reports = crawler.crawl_recent_reports(days=1)
        
        # íŠ¹ì • ì¢…ëª© ê²€ìƒ‰
        reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", days=7)
        
        # í•„í„°ë§ ì˜µì…˜
        reports = crawler.crawl_recent_reports(
            days=7,
            report_type="stock",  # stock, industry, market, analyst
            firm_filter=None  # íŠ¹ì • ì¦ê¶Œì‚¬ í•„í„°
        )
    """
    
    BASE_URL = "https://markets.hankyung.com"
    CONSENSUS_URL = "https://markets.hankyung.com/consensus"
    
    # ë¦¬í¬íŠ¸ ìœ í˜•
    REPORT_TYPE_STOCK = "stock"  # ì¢…ëª© ë¦¬í¬íŠ¸
    REPORT_TYPE_INDUSTRY = "industry"  # ì‚°ì—… ë¦¬í¬íŠ¸
    REPORT_TYPE_MARKET = "market"  # ì‹œí™©/ì „ëµ ë¦¬í¬íŠ¸
    REPORT_TYPE_ANALYST = "analyst"  # ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸
    
    def __init__(self, delay: float = 3.0, max_retries: int = 3, retry_delay: float = 5.0,
                 use_adaptive: bool = True, site_domain: str = "markets.hankyung.com"):
        """
        ì´ˆê¸°í™”
        
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            use_adaptive: ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš© ì—¬ë¶€
            site_domain: ì‚¬ì´íŠ¸ ë„ë©”ì¸
        """
        self.delay = delay
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.use_adaptive = use_adaptive and ADAPTIVE_CRAWLER_AVAILABLE
        self.site_domain = site_domain
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        if self.use_adaptive:
            profile = SiteProfile(
                domain=site_domain,
                base_delay=delay,
                max_retries=max_retries
            )
            self.adaptive_crawler = AdaptiveCrawler(profile)
            self.session = self.adaptive_crawler.session
            self.logger.info("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ í™œì„±í™”")
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://markets.hankyung.com/',
            })
            self.adaptive_crawler = None
    
    def crawl_recent_reports(
        self, 
        days: int = 1,
        max_reports: int = 100,
        report_type: str = "stock",
        firm_filter: Optional[str] = None
    ) -> List[ReportMetadata]:
        """
        ìµœê·¼ ë³´ê³ ì„œ í¬ë¡¤ë§
        
        Args:
            days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’: 1)
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
            report_type: ë¦¬í¬íŠ¸ ìœ í˜• (ê¸°ë³¸ê°’: "stock")
                - "stock": ì¢…ëª© ë¦¬í¬íŠ¸
                - "industry": ì‚°ì—… ë¦¬í¬íŠ¸
                - "market": ì‹œí™©/ì „ëµ ë¦¬í¬íŠ¸
                - "analyst": ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸
            firm_filter: ì¦ê¶Œì‚¬ í•„í„° (Noneì´ë©´ ì „ì²´, ê¸°ë³¸ê°’: None)
            
        Returns:
            List[ReportMetadata]: ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì˜ëª»ëœ report_type ë˜ëŠ” days < 0
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨
            
        ê³„ì•½:
        - ì…ë ¥: daysëŠ” ì–‘ìˆ˜, report_typeì€ ìœ íš¨í•œ ê°’
        - ì¶œë ¥: ReportMetadata ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        - ì˜ˆì™¸: ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)
        """
        
        # ì…ë ¥ ê²€ì¦
        if days < 0:
            raise ValueError(f"days must be non-negative, got {days}")
        if max_reports < 0:
            raise ValueError(f"max_reports must be non-negative, got {max_reports}")
        if report_type not in [self.REPORT_TYPE_STOCK, self.REPORT_TYPE_INDUSTRY, 
                               self.REPORT_TYPE_MARKET, self.REPORT_TYPE_ANALYST]:
            raise ValueError(f"Invalid report_type: {report_type}")
        
        self.logger.info(f"ğŸ“Š í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ë§ ì‹œì‘: ìµœê·¼ {days}ì¼, ìœ í˜•={report_type}")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        try:
            # 1. ì¢…ëª© ë¦¬í¬íŠ¸ íƒ­ìœ¼ë¡œ ì´ë™ (ê¸°ë³¸ê°’)
            if report_type == "stock":
                # ì¢…ëª© ë¦¬í¬íŠ¸ í˜ì´ì§€ URL êµ¬ì„±
                # ì‹¤ì œ APIë‚˜ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ì¡°ì • í•„ìš”
                list_url = f"{self.CONSENSUS_URL}?type=stock"
            else:
                list_url = f"{self.CONSENSUS_URL}?type={report_type}"
            
            self.logger.info(f"ğŸ” ëª©ë¡ ì¡°íšŒ: {list_url}")
            html = self._fetch(list_url)
            
            if not html:
                # ê¸°ë³¸ URLë¡œ ì¬ì‹œë„
                self.logger.warning("í•„í„° URL ì‹¤íŒ¨, ê¸°ë³¸ URLë¡œ ì¬ì‹œë„")
                html = self._fetch(self.CONSENSUS_URL)
            
            if not html:
                self.logger.error("ëª©ë¡ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨")
                return []
            
            # 2. ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
            report_list = self._extract_report_list(html, report_type=report_type)
            
            self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë³´ê³ ì„œ: {len(report_list)}ê°œ")
            
            # 3. ê° ë³´ê³ ì„œ ì²˜ë¦¬
            total_reports = min(len(report_list), max_reports)
            
            for i, report_data in enumerate(report_list[:max_reports], 1):
                progress = f"[{i}/{total_reports}]"
                url = report_data.get('url')
                if not url:
                    continue
                
                try:
                    # ë‚ ì§œ í™•ì¸
                    report_date = report_data.get('date')
                    if isinstance(report_date, str):
                        report_date = self._parse_date_from_text(report_date)
                    elif not isinstance(report_date, datetime):
                        report_date = datetime.now()
                    
                    if report_date < cutoff_date:
                        self.logger.info(f"{progress} â­ï¸  ì˜¤ë˜ëœ ë³´ê³ ì„œ (ë‚ ì§œ: {report_date.strftime('%Y-%m-%d')})")
                        if i > 10:  # ìµœì†Œ 10ê°œëŠ” í™•ì¸
                            break
                        continue
                    
                    # ì¦ê¶Œì‚¬ í•„í„°ë§
                    if firm_filter and report_data.get('firm'):
                        if firm_filter not in report_data.get('firm', ''):
                            self.logger.info(f"{progress} â­ï¸  ì¦ê¶Œì‚¬ í•„í„° ë¶ˆì¼ì¹˜: {report_data.get('firm')}")
                            continue
                    
                    # ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©
                    if report_data.get('firm') and report_data.get('firm') != 'UNKNOWN':
                        report = ReportMetadata(
                            report_id=self._generate_report_id(url, report_data.get('title', '')),
                            title=report_data.get('title', 'ë¦¬í¬íŠ¸'),
                            stock_code=report_data.get('stock_code', 'UNKNOWN'),
                            stock_name=report_data.get('stock_name', 'UNKNOWN'),
                            analyst_name=report_data.get('analyst_name', 'UNKNOWN'),
                            firm=report_data.get('firm', 'UNKNOWN'),
                            published_date=report_date,
                            source_url=url,
                            investment_opinion=report_data.get('opinion'),
                            target_price=report_data.get('target_price'),
                            current_price=None,
                            consensus_rating=None
                        )
                        reports.append(report)
                        self.logger.info(
                            f"{progress} âœ… ìˆ˜ì§‘: {report.stock_name} - {report.analyst_name} ({report.firm}) "
                            f"- {report.investment_opinion or 'N/A'} - ëª©í‘œê°€: {report.target_price or 'N/A'}"
                        )
                    else:
                        # ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ í•„ìš”
                        self.logger.info(f"{progress} ì²˜ë¦¬ ì¤‘: {url[:80]}...")
                        report = self._crawl_report_detail(url)
                        
                        if report:
                            # ì¦ê¶Œì‚¬ í•„í„°ë§
                            if firm_filter and firm_filter not in report.firm:
                                self.logger.info(f"{progress} â­ï¸  ì¦ê¶Œì‚¬ í•„í„° ë¶ˆì¼ì¹˜: {report.firm}")
                                continue
                            
                            reports.append(report)
                            self.logger.info(
                                f"{progress} âœ… ìˆ˜ì§‘: {report.stock_name} - {report.analyst_name} ({report.firm})"
                            )
                        else:
                            self.logger.warning(f"{progress} âŒ ì¶”ì¶œ ì‹¤íŒ¨")
                    
                    # ì˜ˆì˜ë°”ë¥¸ ëŒ€ê¸°
                    if i < total_reports:
                        time.sleep(self.delay)
                
                except Exception as e:
                    self.logger.error(f"{progress} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(reports)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            self.logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}", exc_info=True)
        
        return reports
    
    def search_by_stock(
        self,
        stock_name: str,
        days: int = 7,
        max_reports: int = 50
    ) -> List[ReportMetadata]:
        """
        íŠ¹ì • ì¢…ëª©ìœ¼ë¡œ ë¦¬í¬íŠ¸ ê²€ìƒ‰
        
        Args:
            stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’: 7)
            max_reports: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ê°’: 50)
            
        Returns:
            List[ReportMetadata]: ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì˜ëª»ëœ stock_name ë˜ëŠ” days < 0
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë˜ëŠ” í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨
            
        ê³„ì•½:
        - ì…ë ¥: stock_nameì€ ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ìì—´, daysëŠ” ì–‘ìˆ˜
        - ì¶œë ¥: ReportMetadata ë¦¬ìŠ¤íŠ¸ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        - ì˜ˆì™¸: ValueError (ì˜ëª»ëœ íŒŒë¼ë¯¸í„°), RequestException (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)
        """
        
        # ì…ë ¥ ê²€ì¦
        if not stock_name or not isinstance(stock_name, str):
            raise ValueError(f"stock_name must be a non-empty string, got {stock_name}")
        if days < 0:
            raise ValueError(f"days must be non-negative, got {days}")
        if max_reports < 0:
            raise ValueError(f"max_reports must be non-negative, got {max_reports}")
        
        self.logger.info(f"ğŸ” ì¢…ëª© ê²€ìƒ‰: {stock_name} (ìµœê·¼ {days}ì¼)")
        
        # ì¢…ëª© ë¦¬í¬íŠ¸ íƒ­ìœ¼ë¡œ ì´ë™ í›„ ê²€ìƒ‰
        # ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ë‚˜ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ì— ë§ì¶° ì¡°ì • í•„ìš”
        search_url = f"{self.CONSENSUS_URL}?type=stock&search={stock_name}"
        
        html = self._fetch(search_url)
        
        if not html:
            self.logger.error(f"ì¢…ëª© ê²€ìƒ‰ ì‹¤íŒ¨: {stock_name}")
            return []
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¦¬í¬íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
        report_list = self._extract_report_list(html, report_type="stock")
        
        self.logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë¦¬í¬íŠ¸: {len(report_list)}ê°œ")
        
        reports = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for i, report_data in enumerate(report_list[:max_reports], 1):
            try:
                # ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë©”íƒ€ë°ì´í„° í™œìš©
                url = report_data.get('url')
                if not url:
                    continue
                
                # ì¢…ëª©ëª… í™•ì¸
                extracted_stock_name = report_data.get('stock_name', '')
                if stock_name not in extracted_stock_name and extracted_stock_name:
                    continue
                
                # ë‚ ì§œ í™•ì¸
                report_date = report_data.get('date')
                if isinstance(report_date, str):
                    report_date = self._parse_date_from_text(report_date)
                elif not isinstance(report_date, datetime):
                    report_date = datetime.now()
                
                if report_date < cutoff_date:
                    continue
                
                # ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©
                if report_data.get('firm') and report_data.get('firm') != 'UNKNOWN':
                    # ReportMetadata ìƒì„±
                    report = ReportMetadata(
                        report_id=self._generate_report_id(url, report_data.get('title', '')),
                        title=report_data.get('title', 'ë¦¬í¬íŠ¸'),
                        stock_code=report_data.get('stock_code', 'UNKNOWN'),
                        stock_name=report_data.get('stock_name', stock_name),
                        analyst_name=report_data.get('analyst_name', 'UNKNOWN'),
                        firm=report_data.get('firm', 'UNKNOWN'),
                        published_date=report_date,
                        source_url=url,
                        investment_opinion=report_data.get('opinion'),
                        target_price=report_data.get('target_price'),
                        current_price=None,
                        consensus_rating=None
                    )
                    reports.append(report)
                    self.logger.info(
                        f"[{i}] âœ… {report.stock_name} - {report.analyst_name} ({report.firm}) "
                        f"- {report.investment_opinion or 'N/A'} - ëª©í‘œê°€: {report.target_price or 'N/A'}"
                    )
                else:
                    # ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ í•„ìš”
                    report = self._crawl_report_detail(url)
                    if report:
                        # ì¢…ëª©ëª… í™•ì¸
                        if stock_name not in report.stock_name:
                            continue
                        
                        # ë‚ ì§œ í•„í„°ë§
                        if report.published_date >= cutoff_date:
                            reports.append(report)
                            self.logger.info(
                                f"[{i}] âœ… {report.stock_name} - {report.analyst_name} ({report.firm})"
                            )
                
                if i < len(report_list):
                    time.sleep(self.delay)
            
            except Exception as e:
                self.logger.error(f"ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ [{i}]: {e}")
                continue
        
        return reports
    
    def _fetch(self, url: str) -> Optional[str]:
        """
        í˜ì´ì§€ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨, ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì§€ì›)
        
        Args:
            url: ì¡°íšŒí•  URL
            
        Returns:
            Optional[str]: HTML ë‚´ìš© (ì‹¤íŒ¨ ì‹œ None)
            
        Raises:
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ ì‹œ)
        """
        
        # ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ì‚¬ìš©
        if self.use_adaptive and self.adaptive_crawler:
            response = self.adaptive_crawler.fetch(url)
            if response:
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                return response.text
            return None
        
        # ê¸°ë³¸ í¬ë¡¤ëŸ¬ (ê¸°ì¡´ ë¡œì§)
        for attempt in range(1, self.max_retries + 1):
            try:
                response = self.session.get(url, timeout=10, verify=True)
                response.raise_for_status()
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                return response.text
            
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    self.logger.warning(
                        f"í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨ (ì‹œë„ {attempt}/{self.max_retries}): {url} - {e}. "
                        f"{self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„..."
                    )
                    time.sleep(self.retry_delay)
                else:
                    self.logger.error(f"í˜ì´ì§€ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨: {url} - {e}")
                    return None
        
        return None
    
    def pre_test_connection(self, url: Optional[str] = None) -> Tuple[bool, str]:
        """
        ì‚¬ì „ ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Args:
            url: í…ŒìŠ¤íŠ¸í•  URL (Noneì´ë©´ ê¸°ë³¸ URL ì‚¬ìš©, ê¸°ë³¸ê°’: None)
            
        Returns:
            Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
                - (True, "ì—°ê²° ì„±ê³µ") ë˜ëŠ” (False, "ì—°ê²° ì‹¤íŒ¨: ì´ìœ ")
            
        Raises:
            requests.RequestException: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
        """
        if not self.use_adaptive or not self.adaptive_crawler:
            self.logger.warning("ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ì‚¬ì „ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True, "ëŒ€ì‘í˜• í¬ë¡¤ëŸ¬ ë¹„í™œì„±í™”"
        
        test_url = url or self.CONSENSUS_URL
        return self.adaptive_crawler.pre_test(test_url)
    
    def get_crawler_status(self) -> Optional[Dict]:
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        if self.use_adaptive and self.adaptive_crawler:
            return self.adaptive_crawler.get_status()
        return None
    
    def _extract_report_links(self, html: str, report_type: str = "stock") -> List[str]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³´ê³ ì„œ ë§í¬ ì¶”ì¶œ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€ - í˜¸í™˜ì„±)
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ëª©ë¡ êµ¬ì¡°:
        - ë¦¬í¬íŠ¸ ëª©ë¡ì€ í…Œì´ë¸” ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        - ê° ë¦¬í¬íŠ¸ í–‰ì— ë§í¬ê°€ ìˆìŒ
        - ë¦¬í¬íŠ¸ ë³´ê¸° / PDF ë²„íŠ¼ì´ ìˆìŒ
        """
        report_list = self._extract_report_list(html, report_type)
        return [report['url'] for report in report_list if 'url' in report]
    
    def _extract_report_list(self, html: str, report_type: str = "stock") -> List[Dict[str, any]]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³´ê³ ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        
        PDF ì°¸ê³ : ë¦¬í¬íŠ¸ ëª©ë¡ í…Œì´ë¸”ì—ì„œ ë‹¤ìŒ ì •ë³´ ì¶”ì¶œ
        - ì¦ê¶Œì‚¬ëª… (ì˜ˆ: NHíˆ¬ìì¦ê¶Œ)
        - ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„
        - íˆ¬ìì˜ê²¬ (BUY / HOLD / SELL)
        - ëª©í‘œì£¼ê°€
        - ë¦¬í¬íŠ¸ ë‚ ì§œ
        - ë¦¬í¬íŠ¸ URL
        - PDF URL (ìˆëŠ” ê²½ìš°)
        
        Args:
            html: ëª©ë¡ í˜ì´ì§€ HTML
            report_type: ë¦¬í¬íŠ¸ ìœ í˜• (stock, industry, market, analyst)
        
        Returns:
            List[Dict]: ë¦¬í¬íŠ¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'url': 'https://...',
                        'pdf_url': 'https://...' (ì˜µì…˜),
                        'title': 'ì œëª©',
                        'firm': 'NHíˆ¬ìì¦ê¶Œ',
                        'analyst_name': 'í™ê¸¸ë™',
                        'opinion': 'BUY',
                        'target_price': '98000',
                        'date': '2025-01-05',
                        'stock_name': 'ì‚¼ì„±ì „ì',
                        'stock_code': '005930'
                    },
                    ...
                ]
        """
        soup = BeautifulSoup(html, 'html.parser')
        reports = []
        
        # íŒ¨í„´ 1: í…Œì´ë¸” êµ¬ì¡°ì—ì„œ ì¶”ì¶œ (í•œê²½ ì»¨ì„¼ì„œìŠ¤ ì£¼ìš” êµ¬ì¡°)
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            
            # í—¤ë” í–‰ ì°¾ê¸° (ì»¬ëŸ¼ ì¸ë±ìŠ¤ í™•ì¸)
            header_row = None
            header_indices = {}
            for i, row in enumerate(rows[:3]):  # ì²˜ìŒ 3í–‰ ì¤‘ í—¤ë” ì°¾ê¸°
                cells = row.find_all(['th', 'td'])
                cell_texts = [cell.get_text(strip=True).lower() for cell in cells]
                
                # í—¤ë” í‚¤ì›Œë“œ í™•ì¸
                if any(keyword in ' '.join(cell_texts) for keyword in ['ë‚ ì§œ', 'ì¦ê¶Œì‚¬', 'ì• ë„ë¦¬ìŠ¤íŠ¸', 'ì˜ê²¬', 'ëª©í‘œ', 'ë¦¬í¬íŠ¸']):
                    header_row = i
                    for j, text in enumerate(cell_texts):
                        if 'ë‚ ì§œ' in text or 'date' in text:
                            header_indices['date'] = j
                        if 'ì¦ê¶Œì‚¬' in text or 'firm' in text or 'company' in text:
                            header_indices['firm'] = j
                        if 'ì• ë„ë¦¬ìŠ¤íŠ¸' in text or 'analyst' in text or 'ì‘ì„±' in text:
                            header_indices['analyst'] = j
                        if 'ì˜ê²¬' in text or 'opinion' in text or 'rating' in text:
                            header_indices['opinion'] = j
                        if 'ëª©í‘œ' in text or 'target' in text:
                            header_indices['target_price'] = j
                        if 'ì¢…ëª©' in text or 'stock' in text:
                            header_indices['stock'] = j
                    break
            
            # ë°ì´í„° í–‰ ì²˜ë¦¬
            start_idx = header_row + 1 if header_row is not None else 0
            for row in rows[start_idx:]:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:  # ìµœì†Œ 3ê°œ ì…€ í•„ìš”
                    continue
                
                report_data = {}
                
                # ê° ì…€ì—ì„œ ì •ë³´ ì¶”ì¶œ
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    if i == header_indices.get('date', -1) or (i == 0 and re.match(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}', text)):
                        report_data['date'] = self._parse_date_from_text(text)
                    
                    # ì¦ê¶Œì‚¬ëª… ì¶”ì¶œ
                    if i == header_indices.get('firm', -1) or ('ì¦ê¶Œ' in text or 'íˆ¬ì' in text or 'ìì‚°' in text):
                        if not report_data.get('firm'):
                            report_data['firm'] = text
                    
                    # ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„ ì¶”ì¶œ
                    if i == header_indices.get('analyst', -1) or (re.match(r'^[ê°€-í£]{2,4}$', text) and 'ì¦ê¶Œ' not in text):
                        if not report_data.get('analyst_name'):
                            report_data['analyst_name'] = text
                    
                    # íˆ¬ìì˜ê²¬ ì¶”ì¶œ
                    if i == header_indices.get('opinion', -1):
                        report_data['opinion'] = self._normalize_opinion(text)
                    elif any(keyword in text.upper() for keyword in ['BUY', 'HOLD', 'SELL', 'ë§¤ìˆ˜', 'ì¤‘ë¦½', 'ë§¤ë„']):
                        if not report_data.get('opinion'):
                            report_data['opinion'] = self._normalize_opinion(text)
                    
                    # ëª©í‘œì£¼ê°€ ì¶”ì¶œ
                    if i == header_indices.get('target_price', -1):
                        report_data['target_price'] = self._extract_price_from_text(text)
                    elif re.search(r'\d{1,3}(?:,\d{3})*\s*ì›', text) or re.search(r'\d{4,6}', text):
                        price = self._extract_price_from_text(text)
                        if price and not report_data.get('target_price'):
                            report_data['target_price'] = price
                    
                    # ì¢…ëª© ì •ë³´ ì¶”ì¶œ
                    if i == header_indices.get('stock', -1):
                        # ì¢…ëª©ëª…ê³¼ ì½”ë“œ ë¶„ë¦¬
                        stock_match = re.search(r'([ê°€-í£\w]+)\s*\(?(\d{6})?\)?', text)
                        if stock_match:
                            report_data['stock_name'] = stock_match.group(1)
                            if stock_match.group(2):
                                report_data['stock_code'] = stock_match.group(2)
                    
                    # ë§í¬ ì¶”ì¶œ
                    link = cell.find('a', href=True)
                    if link:
                        href = link['href']
                        if href.startswith('http'):
                            url = href
                        else:
                            url = urljoin(self.BASE_URL, href)
                        
                        # ë¦¬í¬íŠ¸ ë§í¬ì¸ì§€ í™•ì¸
                        if any(pattern in url.lower() for pattern in ['/consensus/', '/report/', '/analyst/', 'detail', 'view']):
                            report_data['url'] = url
                            
                            # PDF ë§í¬ í™•ì¸
                            link_text = link.get_text(strip=True).lower()
                            if 'pdf' in link_text or 'ë‹¤ìš´ë¡œë“œ' in link_text:
                                report_data['pdf_url'] = url
                        
                        # ì œëª© ì¶”ì¶œ (ë§í¬ í…ìŠ¤íŠ¸)
                        if not report_data.get('title'):
                            title_text = link.get_text(strip=True)
                            if title_text and len(title_text) > 5:
                                report_data['title'] = title_text
                
                # ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if report_data.get('url') or report_data.get('firm'):
                    # ê¸°ë³¸ê°’ ì„¤ì •
                    if not report_data.get('firm'):
                        report_data['firm'] = 'UNKNOWN'
                    if not report_data.get('analyst_name'):
                        report_data['analyst_name'] = 'UNKNOWN'
                    if not report_data.get('date'):
                        report_data['date'] = datetime.now()
                    
                    reports.append(report_data)
        
        # íŒ¨í„´ 2: ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ì—ì„œ ì¶”ì¶œ (í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš°)
        if not reports:
            lists = soup.find_all(['ul', 'ol', 'div'], class_=re.compile(r'list|report|item', re.I))
            for list_elem in lists:
                items = list_elem.find_all(['li', 'div'], recursive=False)
                for item in items:
                    report_data = {}
                    text = item.get_text()
                    
                    # ë§í¬ ì¶”ì¶œ
                    link = item.find('a', href=True)
                    if link:
                        href = link['href']
                        if href.startswith('http'):
                            url = href
                        else:
                            url = urljoin(self.BASE_URL, href)
                        report_data['url'] = url
                        report_data['title'] = link.get_text(strip=True)
                    
                    # í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    if 'ì¦ê¶Œ' in text:
                        firm_match = re.search(r'([ê°€-í£\w]+ì¦ê¶Œ)', text)
                        if firm_match:
                            report_data['firm'] = firm_match.group(1)
                    
                    analyst_match = re.search(r'([ê°€-í£]{2,4})\s*[/Â·]', text)
                    if analyst_match:
                        report_data['analyst_name'] = analyst_match.group(1)
                    
                    date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', text)
                    if date_match:
                        report_data['date'] = self._parse_date_from_text(date_match.group(1))
                    
                    if report_data.get('url'):
                        reports.append(report_data)
        
        # íŒ¨í„´ 3: ê¸°ì¡´ ë°©ì‹ (ë§í¬ë§Œ ì¶”ì¶œ) - í˜¸í™˜ì„± ìœ ì§€
        if not reports:
            for link in soup.find_all('a', href=True):
                href = link['href']
                if any(pattern in href.lower() for pattern in ['/consensus/', '/report/', '/analyst/', 'detail', 'view']):
                    if href.startswith('http'):
                        url = href
                    else:
                        url = urljoin(self.BASE_URL, href)
                    
                    if self.BASE_URL in url:
                        reports.append({'url': url, 'title': link.get_text(strip=True)})
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_reports = []
        for report in reports:
            url = report.get('url')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_reports.append(report)
        
        self.logger.info(f"ì¶”ì¶œëœ ë¦¬í¬íŠ¸: {len(unique_reports)}ê°œ (ë©”íƒ€ë°ì´í„° í¬í•¨: {sum(1 for r in unique_reports if r.get('firm') != 'UNKNOWN')}ê°œ)")
        
        return unique_reports
    
    def _crawl_report_detail(self, url: str) -> Optional[ReportMetadata]:
        """
        ë³´ê³ ì„œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ êµ¬ì¡° ë¶„ì„ í•„ìš”
        """
        
        html = self._fetch(url)
        
        if not html:
            return None
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            
            if not title:
                self.logger.warning(f"ì œëª© ì—†ìŒ: {url}")
                return None
            
            # ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´
            analyst_info = self._extract_analyst(soup)
            
            # ì¢…ëª© ì •ë³´
            stock_info = self._extract_stock(soup)
            
            # ë‚ ì§œ
            published_date = self._extract_date(soup)
            
            # íˆ¬ìì˜ê²¬
            opinion = self._extract_opinion(soup)
            
            # ëª©í‘œê°€
            target_price = self._extract_target_price(soup)
            
            # í˜„ì¬ê°€
            current_price = self._extract_current_price(soup)
            
            # ì»¨ì„¼ì„œìŠ¤ ë“±ê¸‰
            consensus_rating = self._extract_consensus_rating(soup)
            
            # ë³´ê³ ì„œ ID ìƒì„±
            report_id = self._generate_report_id(url, title)
            
            return ReportMetadata(
                report_id=report_id,
                title=title,
                stock_code=stock_info.get('code', 'UNKNOWN'),
                stock_name=stock_info.get('name', 'UNKNOWN'),
                analyst_name=analyst_info.get('name', 'UNKNOWN'),
                firm=analyst_info.get('firm', 'UNKNOWN'),
                published_date=published_date,
                source_url=url,
                investment_opinion=opinion,
                target_price=target_price,
                current_price=current_price,
                consensus_rating=consensus_rating
            )
        
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ"""
        
        # íŒ¨í„´ ì‹œë„ ìˆœì„œ
        patterns = [
            ('h1', {}),
            ('h2', {}),
            ('h3', {}),
            ('div', {'class': re.compile(r'title', re.I)}),
            ('span', {'class': re.compile(r'title', re.I)}),
            ('title', {}),
        ]
        
        for tag, attrs in patterns:
            element = soup.find(tag, attrs)
            if element:
                text = element.get_text(strip=True)
                if text and 5 < len(text) < 500:
                    return text
        
        return None
    
    def _extract_analyst(self, soup: BeautifulSoup) -> dict:
        """
        ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
        
        í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ êµ¬ì¡°:
        - ì¦ê¶Œì‚¬ëª…: ë³´í†µ í…Œì´ë¸”ì´ë‚˜ íŠ¹ì • ì˜ì—­ì— í‘œì‹œ
        - ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„: ì¦ê¶Œì‚¬ëª…ê³¼ í•¨ê»˜ í‘œì‹œ
        - í˜•ì‹: "ì• ë„ë¦¬ìŠ¤íŠ¸ëª… / ì¦ê¶Œì‚¬ëª…" ë˜ëŠ” ë³„ë„ í•„ë“œ
        """
        
        # íŒ¨í„´ 1: analyst, firm, company í´ë˜ìŠ¤
        analyst_elem = soup.find(['div', 'span', 'td'], {'class': re.compile(r'analyst|writer|author', re.I)})
        firm_elem = soup.find(['div', 'span', 'td'], {'class': re.compile(r'firm|company|sec|ì¦ê¶Œ', re.I)})
        
        analyst_name = 'UNKNOWN'
        firm_name = 'UNKNOWN'
        
        if analyst_elem:
            analyst_name = analyst_elem.get_text(strip=True)
        
        if firm_elem:
            firm_name = firm_elem.get_text(strip=True)
        
        # íŒ¨í„´ 2: í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ (í•œê²½ ì»¨ì„¼ì„œìŠ¤ëŠ” ë³´í†µ í…Œì´ë¸” êµ¬ì¡°)
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)
                    
                    # ì¦ê¶Œì‚¬ëª… ì°¾ê¸°
                    if 'ì¦ê¶Œ' in text or 'íˆ¬ì' in text or 'ìì‚°' in text:
                        firm_name = text
                        # ë‹¤ìŒ ì…€ì— ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„ì´ ìˆì„ ìˆ˜ ìˆìŒ
                        if i + 1 < len(cells):
                            next_text = cells[i + 1].get_text(strip=True)
                            if next_text and len(next_text) < 20:  # ì´ë¦„ ê¸¸ì´ ê°€ì •
                                analyst_name = next_text
                    
                    # ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„ ì°¾ê¸° (í•œê¸€ ì´ë¦„ íŒ¨í„´)
                    if re.match(r'^[ê°€-í£]{2,4}$', text) and 'ì¦ê¶Œ' not in text:
                        analyst_name = text
        
        # íŒ¨í„´ 3: í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if analyst_name == 'UNKNOWN' or firm_name == 'UNKNOWN':
            full_text = soup.get_text()
            
            # "í™ê¸¸ë™ / NHíˆ¬ìì¦ê¶Œ" íŒ¨í„´
            match = re.search(r'([ê°€-í£]{2,4})\s*[/Â·]\s*([ê°€-í£\w]+ì¦ê¶Œ)', full_text)
            if match:
                analyst_name = match.group(1)
                firm_name = match.group(2)
            else:
                # "NHíˆ¬ìì¦ê¶Œ / í™ê¸¸ë™" íŒ¨í„´
                match = re.search(r'([ê°€-í£\w]+ì¦ê¶Œ)\s*[/Â·]\s*([ê°€-í£]{2,4})', full_text)
                if match:
                    firm_name = match.group(1)
                    analyst_name = match.group(2)
        
        return {
            'name': analyst_name,
            'firm': firm_name,
            'department': None
        }
    
    def _extract_stock(self, soup: BeautifulSoup) -> dict:
        """ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        
        # ì œëª©ì—ì„œ ì¶”ì¶œ ì‹œë„
        title = self._extract_title(soup)
        
        if title:
            # "ì‚¼ì„±ì „ì - 4Q24 Preview" â†’ "ì‚¼ì„±ì „ì"
            stock_name = title.split('-')[0].split('(')[0].strip()
            
            # ì¢…ëª© ì½”ë“œ ì°¾ê¸°
            stock_code = self._find_stock_code(soup) or 'UNKNOWN'
            
            return {
                'name': stock_name,
                'code': stock_code
            }
        
        return {'name': 'UNKNOWN', 'code': 'UNKNOWN'}
    
    def _find_stock_code(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¢…ëª© ì½”ë“œ ì°¾ê¸°"""
        
        # íŒ¨í„´ 1: ì§ì ‘ í‘œì‹œ
        code_elements = soup.find_all(['span', 'div', 'td'], {'class': re.compile(r'code|stock', re.I)})
        for elem in code_elements:
            text = elem.get_text(strip=True)
            if re.match(r'^\d{6}$', text):
                return text
        
        # íŒ¨í„´ 2: í…ìŠ¤íŠ¸ì—ì„œ 6ìë¦¬ ìˆ«ì ì°¾ê¸°
        text = soup.get_text()
        codes = re.findall(r'\b\d{6}\b', text)
        
        if codes:
            return codes[0]
        
        return None
    
    def _extract_date(self, soup: BeautifulSoup) -> datetime:
        """ë‚ ì§œ ì¶”ì¶œ"""
        
        # íŒ¨í„´ 1: date í´ë˜ìŠ¤
        date_elements = soup.find_all(['div', 'span', 'td'], {'class': re.compile(r'date|time', re.I)})
        
        for date_elem in date_elements:
            text = date_elem.get_text(strip=True)
            parsed = self._parse_date(text)
            if parsed:
                return parsed
        
        # íŒ¨í„´ 2: ë‚ ì§œ í˜•ì‹ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        date_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        if date_match:
            year, month, day = date_match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except ValueError:
                pass
        
        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜
        return datetime.now()
    
    def _parse_date(self, text: str) -> Optional[datetime]:
        """ë‚ ì§œ íŒŒì‹±"""
        
        # "2024.12.30 14:30" â†’ "2024.12.30"
        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
        
        if match:
            year, month, day = match.groups()
            try:
                return datetime(int(year), int(month), int(day))
            except ValueError:
                pass
        
        return None
    
    def _parse_date_from_text(self, text: str) -> datetime:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒŒì‹± (ëª©ë¡ í˜ì´ì§€ìš©)
        
        Args:
            text: ë‚ ì§œ í…ìŠ¤íŠ¸ (ì˜ˆ: "2025-01-05", "2025.01.05")
        
        Returns:
            datetime: íŒŒì‹±ëœ ë‚ ì§œ (ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë‚ ì§œ)
        """
        parsed = self._parse_date(text)
        return parsed if parsed else datetime.now()
    
    def _normalize_opinion(self, text: str) -> Optional[str]:
        """
        íˆ¬ìì˜ê²¬ í…ìŠ¤íŠ¸ ì •ê·œí™”
        
        PDF ì°¸ê³ : BUY / HOLD / SELLë¡œ ì •ê·œí™”
        
        Args:
            text: ì›ë³¸ ì˜ê²¬ í…ìŠ¤íŠ¸
        
        Returns:
            Optional[str]: ì •ê·œí™”ëœ ì˜ê²¬ ('BUY', 'HOLD', 'SELL', None)
        """
        if not text:
            return None
        
        text_upper = text.upper().strip()
        text_lower = text.lower().strip()
        
        # Strong Buy íŒ¨í„´
        if any(keyword in text_upper for keyword in ['STRONG BUY', 'STRONGBUY', 'ë§¤ìˆ˜(ê°•ë ¥)', 'ê°•ë ¥ë§¤ìˆ˜', 'ì ê·¹ë§¤ìˆ˜']):
            return 'STRONG_BUY'
        
        # Buy íŒ¨í„´
        if any(keyword in text_upper for keyword in ['BUY', 'ë§¤ìˆ˜', 'ë¹„ì¤‘í™•ëŒ€']):
            return 'BUY'
        
        # Strong Sell íŒ¨í„´
        if any(keyword in text_upper for keyword in ['STRONG SELL', 'STRONGSELL', 'ë§¤ë„(ê°•ë ¥)', 'ê°•ë ¥ë§¤ë„']):
            return 'STRONG_SELL'
        
        # Sell íŒ¨í„´
        if any(keyword in text_upper for keyword in ['SELL', 'ë§¤ë„', 'ë¹„ì¤‘ì¶•ì†Œ']):
            return 'SELL'
        
        # Hold íŒ¨í„´
        if any(keyword in text_upper for keyword in ['HOLD', 'ì¤‘ë¦½', 'ë³´ìœ ', 'ì‹œì¥ìˆ˜ìµë¥ ', 'NEUTRAL']):
            return 'HOLD'
        
        return None
    
    def _extract_price_from_text(self, text: str) -> Optional[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ì¶”ì¶œ
        
        Args:
            text: ê°€ê²©ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ (ì˜ˆ: "98,000ì›", "98000", "ëª©í‘œê°€: 98,000ì›")
        
        Returns:
            Optional[str]: ì¶”ì¶œëœ ê°€ê²© ë¬¸ìì—´ (ì˜ˆ: "98000") ë˜ëŠ” None
        """
        if not text:
            return None
        
        # íŒ¨í„´ 1: "98,000ì›" ë˜ëŠ” "98,000"
        match = re.search(r'([\d,]+)\s*ì›?', text)
        if match:
            price_str = match.group(1).replace(',', '')
            # ìœ íš¨í•œ ê°€ê²© ë²”ìœ„ í™•ì¸ (1,000ì› ~ 1,000,000,000ì›)
            try:
                price = int(price_str)
                if 1000 <= price <= 1000000000:
                    return price_str
            except ValueError:
                pass
        
        # íŒ¨í„´ 2: ìˆ«ìë§Œ (4ìë¦¬ ì´ìƒ)
        match = re.search(r'\b(\d{4,})\b', text)
        if match:
            price_str = match.group(1)
            try:
                price = int(price_str)
                if 1000 <= price <= 1000000000:
                    return price_str
            except ValueError:
                pass
        
        return None
    
    def _extract_opinion(self, soup: BeautifulSoup) -> Optional[str]:
        """íˆ¬ìì˜ê²¬ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ìš©)"""
        
        # íŒ¨í„´ 1: opinion í´ë˜ìŠ¤
        opinion_elements = soup.find_all(['div', 'span', 'td'], {'class': re.compile(r'opinion|rating|recommend', re.I)})
        
        for elem in opinion_elements:
            text = elem.get_text(strip=True)
            normalized = self._normalize_opinion(text)
            if normalized:
                return normalized
        
        # íŒ¨í„´ 2: í‚¤ì›Œë“œ ê²€ìƒ‰
        text = soup.get_text()
        normalized = self._normalize_opinion(text)
        if normalized:
            return normalized
        
        return None
    
    def _extract_target_price(self, soup: BeautifulSoup) -> Optional[str]:
        """ëª©í‘œê°€ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ìš©)"""
        
        # íŒ¨í„´ 1: target í´ë˜ìŠ¤
        target_elements = soup.find_all(['div', 'span', 'td'], {'class': re.compile(r'target|price', re.I)})
        
        for elem in target_elements:
            text = elem.get_text(strip=True)
            if 'ëª©í‘œê°€' in text or 'target' in text.lower():
                price = self._extract_price_from_text(text)
                if price:
                    return price
        
        # íŒ¨í„´ 2: "ëª©í‘œê°€" í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        if 'ëª©í‘œê°€' in text:
            price = self._extract_price_from_text(text)
            if price:
                return price
        
        return None
    
    def _extract_current_price(self, soup: BeautifulSoup) -> Optional[str]:
        """í˜„ì¬ê°€ ì¶”ì¶œ"""
        
        # íŒ¨í„´: í˜„ì¬ê°€ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text = soup.get_text()
        
        # "í˜„ì¬ê°€: 75,000ì›" íŒ¨í„´
        match = re.search(r'í˜„ì¬ê°€[:\s]*([\d,]+ì›?)', text)
        if match:
            return match.group(1)
        
        return None
    
    def _extract_consensus_rating(self, soup: BeautifulSoup) -> Optional[str]:
        """ì»¨ì„¼ì„œìŠ¤ ë“±ê¸‰ ì¶”ì¶œ"""
        
        # íŒ¨í„´: ì»¨ì„¼ì„œìŠ¤ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        consensus_elements = soup.find_all(['div', 'span'], {'class': re.compile(r'consensus|rating', re.I)})
        
        for elem in consensus_elements:
            text = elem.get_text(strip=True)
            if text and len(text) < 50:
                return text
        
        return None
    
    def _extract_pdf_url(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """
        PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ (ê°•í™”)
        
        PDF ì°¸ê³ : [PDF] ë²„íŠ¼ ë˜ëŠ” ë¦¬í¬íŠ¸ ë³´ê¸° ë§í¬ì—ì„œ PDF URL ì¶”ì¶œ
        
        Args:
            soup: BeautifulSoup ê°ì²´
            base_url: ê¸°ë³¸ URL (ìƒëŒ€ ê²½ë¡œ ë³€í™˜ìš©)
        
        Returns:
            Optional[str]: PDF URL ë˜ëŠ” None
        """
        # íŒ¨í„´ 1: PDF ë§í¬ ì§ì ‘ ì°¾ê¸°
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf|pdf|download', re.I))
        for link in pdf_links:
            href = link.get('href', '')
            link_text = link.get_text(strip=True).lower()
            if 'pdf' in link_text or 'ë‹¤ìš´ë¡œë“œ' in link_text or 'download' in link_text:
                if href.startswith('http'):
                    return href
                else:
                    return urljoin(base_url, href)
        
        # íŒ¨í„´ 2: [PDF] ë²„íŠ¼ í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            link_text = link.get_text(strip=True).lower()
            if 'pdf' in link_text or 'ë‹¤ìš´ë¡œë“œ' in link_text:
                href = link.get('href', '')
                if href.startswith('http'):
                    return href
                else:
                    return urljoin(base_url, href)
        
        # íŒ¨í„´ 3: iframe ë‚´ PDF ë§í¬
        iframes = soup.find_all('iframe', src=True)
        for iframe in iframes:
            src = iframe.get('src', '')
            if '.pdf' in src.lower() or 'pdf' in src.lower():
                if src.startswith('http'):
                    return src
                else:
                    return urljoin(base_url, src)
        
        # íŒ¨í„´ 4: data-url ì†ì„±
        for elem in soup.find_all(attrs={'data-pdf-url': True}):
            pdf_url = elem.get('data-pdf-url')
            if pdf_url:
                if pdf_url.startswith('http'):
                    return pdf_url
                else:
                    return urljoin(base_url, pdf_url)
        
        return None
    
    def _generate_report_id(self, url: str, title: str) -> str:
        """ë³´ê³ ì„œ ID ìƒì„±"""
        
        # URL + ì œëª©ì˜ í•´ì‹œ
        content = f"{url}:{title}"
        
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def save_to_json(self, reports: List[ReportMetadata], filename: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        data = [report.to_dict() for report in reports]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def save_to_csv(self, reports: List[ReportMetadata], filename: str):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        import csv
        
        if not reports:
            return
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=reports[0].to_dict().keys())
            writer.writeheader()
            
            for report in reports:
                writer.writerow(report.to_dict())
        
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HankyungConsensusCrawler(delay=3.0)
    
    # ìµœê·¼ 1ì¼ ë³´ê³ ì„œ ìˆ˜ì§‘
    print("ğŸš€ í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘\n")
    
    reports = crawler.crawl_recent_reports(
        days=1,
        max_reports=20  # í…ŒìŠ¤íŠ¸ìš© 20ê°œë§Œ
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(reports)}ê°œ\n")
    
    for i, report in enumerate(reports, 1):
        print(f"{i}. {report.stock_name} ({report.stock_code})")
        print(f"   ì œëª©: {report.title}")
        print(f"   ì• ë„ë¦¬ìŠ¤íŠ¸: {report.analyst_name} ({report.firm})")
        print(f"   ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}")
        
        if report.investment_opinion:
            print(f"   ì˜ê²¬: {report.investment_opinion}")
        
        if report.target_price:
            print(f"   ëª©í‘œê°€: {report.target_price}")
        
        print()
    
    # ì €ì¥
    if reports:
        crawler.save_to_json(reports, 'hankyung_consensus_reports.json')
        crawler.save_to_csv(reports, 'hankyung_consensus_reports.csv')
    
    print("âœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()

