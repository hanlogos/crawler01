"""
Phase A-1: ë‰´ìŠ¤ ìˆ˜ì§‘ í†µí•© ì„œë¹„ìŠ¤
í¬ë¡¤ë§ â†’ íŒ©íŠ¸ ì²´í¬ â†’ DB ì €ì¥ â†’ ì•Œë¦¼
"""

import sys
import logging
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import json

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

# PostgreSQL (ì„ íƒì )
try:
    import psycopg2
    from psycopg2.extras import execute_values, Json
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False

from news_crawler import NewsArticle, NewsCrawlerManager
from fact_check_engine import FactCheckEngine, FactCheckResult

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NewsDatabase:
    """ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    def __init__(self, conn_params: Dict):
        """
        ì´ˆê¸°í™”
        
        Args:
            conn_params: PostgreSQL ì—°ê²° íŒŒë¼ë¯¸í„°
                {
                    'host': 'localhost',
                    'port': 5432,
                    'database': 'abiseu',
                    'user': 'postgres',
                    'password': 'password'
                }
        """
        if not PSYCOPG2_AVAILABLE:
            raise ImportError("psycopg2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install psycopg2-binary'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        self.conn_params = conn_params
        self.conn = None
    
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.conn_params)
            logger.info("Database connected successfully")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def disconnect(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info("Database disconnected")
    
    def save_article(self, article: NewsArticle) -> Optional[int]:
        """
        ê¸°ì‚¬ ì €ì¥
        
        Returns:
            article_id (ì €ì¥ ì„±ê³µ) ë˜ëŠ” None (ì¤‘ë³µ/ì‹¤íŒ¨)
        """
        try:
            cursor = self.conn.cursor()
            
            query = """
                INSERT INTO news_articles (
                    title, content, summary, url,
                    source, source_tier, author, published_at,
                    category, urgency_level,
                    stock_codes, sectors, keywords,
                    sentiment, sentiment_score,
                    credibility_score,
                    content_hash, metadata
                ) VALUES (
                    %s, %s, %s, %s,
                    %s, %s, %s, %s,
                    %s, %s,
                    %s, %s, %s,
                    %s, %s,
                    %s,
                    %s, %s
                )
                ON CONFLICT (content_hash) DO NOTHING
                RETURNING article_id;
            """
            
            cursor.execute(query, (
                article.title,
                article.content,
                article.summary,
                article.url,
                article.source,
                article.source_tier,
                article.author,
                article.published_at,
                article.category,
                article.urgency_level,
                article.stock_codes,
                article.sectors,
                article.keywords,
                article.sentiment,
                article.sentiment_score,
                article.credibility_score,
                article.content_hash,
                Json(article.metadata)
            ))
            
            result = cursor.fetchone()
            self.conn.commit()
            
            if result:
                article_id = result[0]
                logger.debug(f"Article saved: {article_id} - {article.title[:50]}")
                return article_id
            else:
                logger.debug(f"Article duplicate: {article.title[:50]}")
                return None
                
        except Exception as e:
            self.conn.rollback()
            logger.error(f"Error saving article: {e}")
            return None
    
    def save_fact_check(self, fact_check: FactCheckResult) -> bool:
        """íŒ©íŠ¸ ì²´í¬ ê²°ê³¼ ì €ì¥"""
        try:
            cursor = self.conn.cursor()
            
            query = """
                INSERT INTO fact_checks (
                    article_id, verification_status, confidence_score,
                    supporting_sources, contradicting_sources,
                    llm_analysis, llm_reasoning,
                    cross_verified_count, total_sources_checked,
                    similar_past_events, past_accuracy_rate
                ) VALUES (
                    %s, %s, %s,
                    %s, %s,
                    %s, %s,
                    %s, %s,
                    %s, %s
                );
            """
            
            cursor.execute(query, (
                fact_check.article_id,
                fact_check.verification_status,
                fact_check.confidence_score,
                fact_check.supporting_sources,
                fact_check.contradicting_sources,
                fact_check.llm_analysis,
                fact_check.llm_reasoning,
                fact_check.cross_verified_count,
                fact_check.total_sources_checked,
                Json(fact_check.similar_past_events),
                fact_check.past_accuracy_rate
            ))
            
            self.conn.commit()
            logger.debug(f"Fact check saved for article: {fact_check.article_id}")
            return True
            
        except Exception as e:
            self.conn.rollback()
            logger.error(f"Error saving fact check: {e}")
            return False
    
    def get_urgent_news(self, hours: int = 24, min_urgency: int = 4) -> List[Dict]:
        """
        ê¸´ê¸‰ ë‰´ìŠ¤ ì¡°íšŒ
        
        Args:
            hours: ì¡°íšŒ ê¸°ê°„ (ì‹œê°„)
            min_urgency: ìµœì†Œ ê¸´ê¸‰ë„
        """
        try:
            cursor = self.conn.cursor()
            
            query = """
                SELECT * FROM v_urgent_news
                WHERE urgency_level >= %s
                  AND published_at > NOW() - INTERVAL '%s hours'
                ORDER BY published_at DESC
                LIMIT 50;
            """
            
            cursor.execute(query, (min_urgency, hours))
            
            columns = [desc[0] for desc in cursor.description]
            results = []
            
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            return results
            
        except Exception as e:
            logger.error(f"Error fetching urgent news: {e}")
            return []
    
    def get_stock_news(self, stock_code: str, days: int = 7) -> List[Dict]:
        """íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            cursor = self.conn.cursor()
            
            query = """
                SELECT * FROM v_stock_latest_news
                WHERE stock_code = %s
                  AND published_at > NOW() - INTERVAL '%s days'
                ORDER BY published_at DESC;
            """
            
            cursor.execute(query, (stock_code, days))
            
            columns = [desc[0] for desc in cursor.description]
            results = []
            
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            return results
            
        except Exception as e:
            logger.error(f"Error fetching stock news: {e}")
            return []
    
    def log_crawl_job(
        self, 
        source_name: str, 
        job_type: str,
        status: str,
        items_found: int = 0,
        items_new: int = 0,
        items_duplicate: int = 0
    ) -> int:
        """í¬ë¡¤ë§ ì‘ì—… ë¡œê·¸"""
        try:
            cursor = self.conn.cursor()
            
            query = """
                INSERT INTO crawl_jobs (
                    source_name, job_type, status,
                    items_found, items_new, items_duplicate,
                    completed_at
                ) VALUES (
                    %s, %s, %s,
                    %s, %s, %s,
                    NOW()
                )
                RETURNING job_id;
            """
            
            cursor.execute(query, (
                source_name, job_type, status,
                items_found, items_new, items_duplicate
            ))
            
            job_id = cursor.fetchone()[0]
            self.conn.commit()
            
            return job_id
            
        except Exception as e:
            self.conn.rollback()
            logger.error(f"Error logging crawl job: {e}")
            return 0


class AlertSystem:
    """ì•Œë¦¼ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.channels = []
    
    def add_channel(self, channel):
        """ì•Œë¦¼ ì±„ë„ ì¶”ê°€"""
        self.channels.append(channel)
    
    def send_urgent_alert(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ ì „ì†¡"""
        message = self._format_urgent_message(article)
        
        for channel in self.channels:
            try:
                channel.send(message)
            except Exception as e:
                logger.error(f"Alert failed on {channel.__class__.__name__}: {e}")
    
    def _format_urgent_message(self, article: Dict) -> str:
        """ê¸´ê¸‰ ì•Œë¦¼ ë©”ì‹œì§€ í¬ë§·"""
        return f"""
ğŸš¨ ê¸´ê¸‰ ë‰´ìŠ¤ ì•Œë¦¼

ì œëª©: {article['title']}
ì¶œì²˜: {article['source']}
ì‹œê°„: {article['published_at']}
ì‹ ë¢°ë„: {article.get('credibility_score', 'N/A')}

{article.get('summary', '')}

URL: {article.get('url', '')}
"""


class ConsoleChannel:
    """ì½˜ì†” ì•Œë¦¼ ì±„ë„"""
    
    def send(self, message: str):
        print("=" * 60)
        print(message)
        print("=" * 60)


class SlackChannel:
    """Slack ì•Œë¦¼ ì±„ë„ (êµ¬í˜„ ì˜ˆì‹œ)"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    def send(self, message: str):
        """Slack ì›¹í›…ìœ¼ë¡œ ì „ì†¡"""
        import requests
        
        payload = {
            "text": message
        }
        
        try:
            response = requests.post(
                self.webhook_url,
                json=payload,
                timeout=5
            )
            response.raise_for_status()
            logger.info("Slack alert sent")
        except Exception as e:
            logger.error(f"Slack alert failed: {e}")


# ================================================================
# í†µí•© ë‰´ìŠ¤ ì„œë¹„ìŠ¤
# ================================================================

class NewsIngestionService:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ í†µí•© ì„œë¹„ìŠ¤"""
    
    def __init__(
        self, 
        db_params: Optional[Dict] = None,
        openai_api_key: Optional[str] = None,
        use_ollama: bool = False,
        ollama_model: str = 'llama3',
        enable_fact_check: bool = True,
        enable_alerts: bool = True
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_params: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° íŒŒë¼ë¯¸í„° (Noneì´ë©´ DB ì‚¬ìš© ì•ˆí•¨)
            openai_api_key: OpenAI API í‚¤ (íŒ©íŠ¸ ì²´í¬ìš©)
            use_ollama: Ollama ì‚¬ìš© ì—¬ë¶€ (Trueë©´ OpenAI ëŒ€ì‹  Ollama ì‚¬ìš©)
            ollama_model: Ollama ëª¨ë¸ëª…
            enable_fact_check: íŒ©íŠ¸ ì²´í¬ í™œì„±í™”
            enable_alerts: ì•Œë¦¼ í™œì„±í™”
        """
        self.crawler_manager = NewsCrawlerManager()
        
        if enable_fact_check:
            self.fact_check_engine = FactCheckEngine(
                openai_api_key=openai_api_key,
                use_ollama=use_ollama,
                ollama_model=ollama_model
            )
        else:
            self.fact_check_engine = None
        
        if db_params:
            self.database = NewsDatabase(db_params)
        else:
            self.database = None
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. DB ì €ì¥ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        self.alert_system = AlertSystem() if enable_alerts else None
        if self.alert_system:
            # ê¸°ë³¸ ì½˜ì†” ì±„ë„ ì¶”ê°€
            self.alert_system.add_channel(ConsoleChannel())
    
    def run_ingestion_cycle(self, use_llm: bool = False):
        """
        1íšŒ ìˆ˜ì§‘ ì‚¬ì´í´ ì‹¤í–‰
        
        Args:
            use_llm: LLM íŒ©íŠ¸ ì²´í¬ ì‚¬ìš© ì—¬ë¶€
        """
        logger.info("=" * 60)
        logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‚¬ì´í´ ì‹œì‘")
        logger.info("=" * 60)
        
        # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (ìˆëŠ” ê²½ìš°)
        if self.database:
            self.database.connect()
        
        # 2. ë‰´ìŠ¤ í¬ë¡¤ë§
        logger.info("\n[1/5] ë‰´ìŠ¤ í¬ë¡¤ë§...")
        articles = self.crawler_manager.crawl_all()
        logger.info(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
        
        if not articles:
            logger.info("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
            if self.database:
                self.database.disconnect()
            return
        
        # 3. íŒ©íŠ¸ ì²´í¬ (ì„ íƒ)
        fact_check_results = {}
        if self.fact_check_engine:
            logger.info("\n[2/5] íŒ©íŠ¸ ì²´í¬...")
            results = self.fact_check_engine.batch_verify(articles, use_llm=use_llm)
            
            # article_id ë§¤í•‘ (ì €ì¥ ì „ì´ë¯€ë¡œ í•´ì‹œë¡œ)
            for i, result in enumerate(results):
                fact_check_results[articles[i].content_hash] = result
            
            logger.info(f"íŒ©íŠ¸ ì²´í¬ ì™„ë£Œ: {len(results)}ê°œ")
        
        # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        saved_count = 0
        duplicate_count = 0
        
        if self.database:
            logger.info("\n[3/5] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥...")
            for article in articles:
                # íŒ©íŠ¸ ì²´í¬ ê²°ê³¼ ë°˜ì˜
                if article.content_hash in fact_check_results:
                    fact_check = fact_check_results[article.content_hash]
                    article.credibility_score = fact_check.confidence_score
                
                # ê¸°ì‚¬ ì €ì¥
                article_id = self.database.save_article(article)
                
                if article_id:
                    saved_count += 1
                    
                    # íŒ©íŠ¸ ì²´í¬ ê²°ê³¼ ì €ì¥
                    if article.content_hash in fact_check_results:
                        fact_check = fact_check_results[article.content_hash]
                        fact_check.article_id = article_id
                        self.database.save_fact_check(fact_check)
                else:
                    duplicate_count += 1
            
            logger.info(f"ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ (ì¤‘ë³µ: {duplicate_count}ê°œ)")
        else:
            logger.info("\n[3/5] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê±´ë„ˆëœ€ (DB ë¯¸ì„¤ì •)")
        
        # 5. ê¸´ê¸‰ ì•Œë¦¼
        if self.alert_system:
            logger.info("\n[4/5] ê¸´ê¸‰ ì•Œë¦¼ í™•ì¸...")
            if self.database:
                urgent_news = self.database.get_urgent_news(hours=1, min_urgency=4)
            else:
                # DB ì—†ìœ¼ë©´ ê¸´ê¸‰ë„ 4 ì´ìƒì¸ ê¸°ì‚¬ë§Œ í•„í„°ë§
                urgent_news = [
                    {
                        'title': a.title,
                        'source': a.source,
                        'published_at': a.published_at,
                        'credibility_score': a.credibility_score,
                        'summary': a.summary,
                        'url': a.url
                    }
                    for a in articles if a.urgency_level >= 4
                ]
            
            for news in urgent_news:
                self.alert_system.send_urgent_alert(news)
            
            logger.info(f"ê¸´ê¸‰ ì•Œë¦¼ ì „ì†¡: {len(urgent_news)}ê°œ")
        
        # 6. í¬ë¡¤ë§ ë¡œê·¸ (DB ìˆëŠ” ê²½ìš°)
        if self.database:
            logger.info("\n[5/5] ì‘ì—… ë¡œê·¸...")
            for crawler in self.crawler_manager.crawlers:
                self.database.log_crawl_job(
                    source_name=crawler.source_name,
                    job_type='rss',
                    status='completed',
                    items_found=len([a for a in articles if a.source == crawler.source_name]),
                    items_new=saved_count,
                    items_duplicate=duplicate_count
                )
        
        # 7. ì—°ê²° ì¢…ë£Œ
        if self.database:
            self.database.disconnect()
        
        logger.info("\n" + "=" * 60)
        logger.info("ìˆ˜ì§‘ ì‚¬ì´í´ ì™„ë£Œ")
        logger.info("=" * 60)
    
    def run_continuous(self, interval_minutes: int = 5):
        """
        ì§€ì†ì  ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤)
        
        Args:
            interval_minutes: ì‹¤í–‰ ê°„ê²© (ë¶„)
        """
        import time
        
        logger.info(f"ì§€ì†ì  ì‹¤í–‰ ëª¨ë“œ ì‹œì‘ (ê°„ê²©: {interval_minutes}ë¶„)")
        
        while True:
            try:
                self.run_ingestion_cycle()
            except Exception as e:
                logger.error(f"ìˆ˜ì§‘ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
            
            logger.info(f"\n{interval_minutes}ë¶„ ëŒ€ê¸°...")
            time.sleep(interval_minutes * 60)


# ================================================================
# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰
# ================================================================

if __name__ == '__main__':
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (ì„ íƒ)
    DB_PARAMS = {
        'host': 'localhost',
        'port': 5432,
        'database': 'abiseu',
        'user': 'postgres',
        'password': 'your_password'  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ë³€ê²½
    }
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (DB ì—†ì´ í…ŒìŠ¤íŠ¸)
    service = NewsIngestionService(
        db_params=None,  # DB ì—†ì´ í…ŒìŠ¤íŠ¸
        openai_api_key=None,  # LLM ë¯¸ì‚¬ìš©
        use_ollama=True,  # Ollama ì‚¬ìš©
        ollama_model='llama3',
        enable_fact_check=True,
        enable_alerts=True
    )
    
    # 1íšŒ ì‹¤í–‰
    print("=== ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ì‹œì‘ ===\n")
    service.run_ingestion_cycle(use_llm=True)
    
    # ì§€ì†ì  ì‹¤í–‰ (ì£¼ì„ í•´ì œ ì‹œ)
    # service.run_continuous(interval_minutes=5)


