# find_correct_url.py
"""
ì˜¬ë°”ë¥¸ URL ì°¾ê¸°

38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
"""

import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import requests
from bs4 import BeautifulSoup
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def find_research_urls(base_url="http://www.38.co.kr"):
    """ë¦¬ì„œì¹˜ í˜ì´ì§€ URL ì°¾ê¸°"""
    
    print("="*60)
    print("38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„")
    print("="*60)
    print()
    
    # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼
    print(f"ğŸ” ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼: {base_url}")
    try:
        response = requests.get(base_url, timeout=10, allow_redirects=True)
        print(f"âœ… Status: {response.status_code}")
        print(f"ğŸ“„ í¬ê¸°: {len(response.text):,} bytes")
        print(f"ğŸ”— ìµœì¢… URL: {response.url}\n")
        
        # HTML ì €ì¥
        with open('38com_main.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print("ğŸ’¾ ì €ì¥: 38com_main.html\n")
        
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}\n")
        return
    
    # 2. ë§í¬ ë¶„ì„
    print("="*60)
    print("ë§í¬ ë¶„ì„")
    print("="*60)
    print()
    
    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print(f"ì´ {len(links)}ê°œ ë§í¬ ë°œê²¬\n")
    
    # ë¦¬ì„œì¹˜ ê´€ë ¨ ë§í¬ ì°¾ê¸°
    research_keywords = ['research', 'report', 'ë¦¬ì„œì¹˜', 'ë³´ê³ ì„œ', 'fund', 'ì¦ê¶Œ']
    research_links = []
    
    for link in links:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        
        if any(keyword.lower() in href.lower() or keyword in text for keyword in research_keywords):
            full_url = requests.compat.urljoin(base_url, href)
            research_links.append({
                'url': full_url,
                'text': text[:50],
                'href': href
            })
    
    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_links = []
    for link in research_links:
        if link['url'] not in seen:
            seen.add(link['url'])
            unique_links.append(link)
    
    print(f"ë¦¬ì„œì¹˜ ê´€ë ¨ ë§í¬: {len(unique_links)}ê°œ\n")
    
    for i, link in enumerate(unique_links[:20], 1):
        print(f"{i}. {link['text']}")
        print(f"   URL: {link['url']}")
        print(f"   ì›ë³¸: {link['href']}")
        print()
    
    # 3. ê°€ëŠ¥í•œ URL íŒ¨í„´ ì‹œë„
    print("="*60)
    print("ê°€ëŠ¥í•œ URL íŒ¨í„´ í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    test_urls = [
        "http://www.38.co.kr/html/fund/research_sec.html",
        "http://www.38.co.kr/html/fund/research.html",
        "http://www.38.co.kr/html/fund/",
        "http://www.38.co.kr/fund/research_sec.html",
        "http://www.38.co.kr/research/",
        "http://www.38.co.kr/html/fund/list.html",
    ]
    
    for url in test_urls:
        try:
            test_response = requests.get(url, timeout=5, allow_redirects=True)
            status = "âœ…" if test_response.status_code == 200 else "âš ï¸"
            print(f"{status} {url}")
            print(f"   Status: {test_response.status_code}")
            if test_response.status_code == 200:
                print(f"   í¬ê¸°: {len(test_response.text):,} bytes")
        except Exception as e:
            print(f"âŒ {url}")
            print(f"   ì˜¤ë¥˜: {e}")
        print()

if __name__ == "__main__":
    find_research_urls()


