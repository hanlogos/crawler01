# site_crawling_manager.py
"""
ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê´€ë¦¬ ì‹œìŠ¤í…œ

ê° ì‚¬ì´íŠ¸ë³„ë¡œ í¬ë¡¤ë§ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ê³  ì œì–´
"""

import sys
import io
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import json
import logging
import threading
import time

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

class CrawlingStatus(Enum):
    """í¬ë¡¤ë§ ìƒíƒœ"""
    IDLE = "idle"  # ëŒ€ê¸°
    RUNNING = "running"  # ì‹¤í–‰ ì¤‘
    PAUSED = "paused"  # ì¼ì‹œì •ì§€
    STOPPED = "stopped"  # ì •ì§€
    ERROR = "error"  # ì˜¤ë¥˜

class CrawlingMode(Enum):
    """í¬ë¡¤ë§ ëª¨ë“œ"""
    MANUAL = "manual"  # ìˆ˜ë™
    AUTO = "auto"  # ìë™

@dataclass
class SiteCrawlingState:
    """ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ìƒíƒœ"""
    site_id: str
    site_name: str
    site_url: str
    
    # ìƒíƒœ
    status: CrawlingStatus = CrawlingStatus.IDLE
    mode: CrawlingMode = CrawlingMode.MANUAL
    
    # í†µê³„
    total_collected: int = 0
    total_failed: int = 0
    last_collected: Optional[datetime] = None
    last_error: Optional[str] = None
    
    # ì§„í–‰ ìƒí™©
    current_progress: int = 0
    total_target: int = 0
    
    # ìŠ¤ì¼€ì¤„
    schedule: Dict = None  # {'interval': 'daily', 'time': '09:00'}
    next_run: Optional[datetime] = None
    
    # ì„¤ì •
    days: int = 1
    max_reports: int = 100
    fake_face_profile: str = 'casual'
    
    # ë©”íƒ€ë°ì´í„°
    created_at: datetime = None
    updated_at: datetime = None
    
    def __post_init__(self):
        if self.schedule is None:
            self.schedule = {}
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.updated_at is None:
            self.updated_at = datetime.now()
    
    def to_dict(self) -> dict:
        data = asdict(self)
        data['status'] = self.status.value
        data['mode'] = self.mode.value
        if self.last_collected:
            data['last_collected'] = self.last_collected.isoformat()
        if self.next_run:
            data['next_run'] = self.next_run.isoformat()
        if self.created_at:
            data['created_at'] = self.created_at.isoformat()
        if self.updated_at:
            data['updated_at'] = self.updated_at.isoformat()
        return data

class SiteCrawlingManager:
    """ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.sites: Dict[str, SiteCrawlingState] = {}
        self.logger = logging.getLogger(__name__)
        self._lock = threading.Lock()
        self._running_tasks: Dict[str, threading.Thread] = {}
        
        # ìƒíƒœ íŒŒì¼
        self.state_file = "crawling_states.json"
        self._load_states()
    
    def register_site(
        self,
        site_id: str,
        site_name: str,
        site_url: str,
        **kwargs
    ) -> SiteCrawlingState:
        """ì‚¬ì´íŠ¸ ë“±ë¡"""
        
        if site_id in self.sites:
            self.logger.warning(f"ì‚¬ì´íŠ¸ ì´ë¯¸ ë“±ë¡ë¨: {site_id}")
            return self.sites[site_id]
        
        state = SiteCrawlingState(
            site_id=site_id,
            site_name=site_name,
            site_url=site_url,
            **kwargs
        )
        
        self.sites[site_id] = state
        self._save_states()
        
        self.logger.info(f"ì‚¬ì´íŠ¸ ë“±ë¡: {site_id} ({site_name})")
        return state
    
    def get_site_state(self, site_id: str) -> Optional[SiteCrawlingState]:
        """ì‚¬ì´íŠ¸ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"""
        return self.sites.get(site_id)
    
    def start_crawling(
        self,
        site_id: str,
        mode: CrawlingMode = CrawlingMode.MANUAL
    ) -> bool:
        """í¬ë¡¤ë§ ì‹œì‘"""
        
        with self._lock:
            state = self.sites.get(site_id)
            if not state:
                self.logger.error(f"ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site_id}")
                return False
            
            if state.status == CrawlingStatus.RUNNING:
                self.logger.warning(f"ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤: {site_id}")
                return False
            
            state.status = CrawlingStatus.RUNNING
            state.mode = mode
            state.updated_at = datetime.now()
            
            # ìŠ¤ë ˆë“œ ì‹œì‘
            thread = threading.Thread(
                target=self._crawling_worker,
                args=(site_id,),
                daemon=True
            )
            self._running_tasks[site_id] = thread
            thread.start()
            
            self._save_states()
            self.logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {site_id} ({mode.value} ëª¨ë“œ)")
            return True
    
    def pause_crawling(self, site_id: str) -> bool:
        """í¬ë¡¤ë§ ì¼ì‹œì •ì§€"""
        
        with self._lock:
            state = self.sites.get(site_id)
            if not state:
                return False
            
            if state.status == CrawlingStatus.RUNNING:
                state.status = CrawlingStatus.PAUSED
                state.updated_at = datetime.now()
                self._save_states()
                self.logger.info(f"í¬ë¡¤ë§ ì¼ì‹œì •ì§€: {site_id}")
                return True
            
            return False
    
    def resume_crawling(self, site_id: str) -> bool:
        """í¬ë¡¤ë§ ì´ì–´ê°€ê¸°"""
        
        with self._lock:
            state = self.sites.get(site_id)
            if not state:
                return False
            
            if state.status == CrawlingStatus.PAUSED:
                state.status = CrawlingStatus.RUNNING
                state.updated_at = datetime.now()
                self._save_states()
                self.logger.info(f"í¬ë¡¤ë§ ì¬ê°œ: {site_id}")
                return True
            
            return False
    
    def stop_crawling(self, site_id: str) -> bool:
        """í¬ë¡¤ë§ ì •ì§€"""
        
        with self._lock:
            state = self.sites.get(site_id)
            if not state:
                return False
            
            if state.status in [CrawlingStatus.RUNNING, CrawlingStatus.PAUSED]:
                state.status = CrawlingStatus.STOPPED
                state.updated_at = datetime.now()
                self._save_states()
                self.logger.info(f"í¬ë¡¤ë§ ì •ì§€: {site_id}")
                return True
            
            return False
    
    def clear_site_data(self, site_id: str) -> bool:
        """ì‚¬ì´íŠ¸ ë°ì´í„° ì§€ìš°ê¸°"""
        
        with self._lock:
            state = self.sites.get(site_id)
            if not state:
                return False
            
            state.total_collected = 0
            state.total_failed = 0
            state.current_progress = 0
            state.last_collected = None
            state.last_error = None
            state.updated_at = datetime.now()
            
            self._save_states()
            self.logger.info(f"ì‚¬ì´íŠ¸ ë°ì´í„° ì´ˆê¸°í™”: {site_id}")
            return True
    
    def save_site_data(self, site_id: str, filename: str = None) -> bool:
        """ì‚¬ì´íŠ¸ ë°ì´í„° ì €ì¥"""
        
        state = self.sites.get(site_id)
        if not state:
            return False
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"crawling_data_{site_id}_{timestamp}.json"
        
        data = {
            'site_id': site_id,
            'state': state.to_dict(),
            'saved_at': datetime.now().isoformat()
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
            return True
        
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _crawling_worker(self, site_id: str):
        """í¬ë¡¤ë§ ì›Œì»¤ (ìŠ¤ë ˆë“œ)"""
        
        state = self.sites.get(site_id)
        if not state:
            return
        
        try:
            # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§
            from integrated_crawler_manager import IntegratedCrawlerManager
            
            manager = IntegratedCrawlerManager(
                use_fake_face=True,
                fake_face_profile=state.fake_face_profile
            )
            
            # total_targetì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
            if state.total_target == 0:
                state.total_target = state.max_reports
            
            self.logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {site_id} (ëª©í‘œ: {state.total_target}ê°œ)")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            while state.status == CrawlingStatus.RUNNING:
                # ì¼ì‹œì •ì§€ í™•ì¸
                if state.status == CrawlingStatus.PAUSED:
                    time.sleep(1)
                    continue
                
                # ì •ì§€ í™•ì¸
                if state.status == CrawlingStatus.STOPPED:
                    break
                
                try:
                    # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
                    # 38com ì‚¬ì´íŠ¸ì¸ ê²½ìš° ì‹¤ì œ í¬ë¡¤ëŸ¬ ì‚¬ìš©
                    if site_id == "38com":
                        from crawler_38com import ThirtyEightComCrawler
                        crawler = ThirtyEightComCrawler(
                            delay=3.0,
                            max_retries=3,
                            use_adaptive=True
                        )
                        
                        # ìµœê·¼ ë³´ê³ ì„œ í¬ë¡¤ë§
                        self.logger.info(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: {site_id} (days={state.days}, max={state.max_reports})")
                        reports = crawler.crawl_recent_reports(
                            days=state.days,
                            max_reports=state.max_reports
                        )
                        
                        if reports:
                            state.current_progress = len(reports)
                            state.total_collected += len(reports)
                            state.last_collected = datetime.now()
                            state.updated_at = datetime.now()
                            self.logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {site_id} - {len(reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘")
                        else:
                            self.logger.warning(f"âš ï¸  í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ: {site_id}")
                        
                        # í¬ë¡¤ë§ ì™„ë£Œ í›„ ì¢…ë£Œ
                        state.status = CrawlingStatus.IDLE
                        break
                    elif site_id == "hankyung_consensus":
                        # í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬ ì‚¬ìš©
                        from crawler_hankyung_consensus import HankyungConsensusCrawler
                        crawler = HankyungConsensusCrawler(
                            delay=3.0,
                            max_retries=3,
                            use_adaptive=True
                        )
                        
                        # ìµœê·¼ ë³´ê³ ì„œ í¬ë¡¤ë§
                        self.logger.info(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: {site_id} (days={state.days}, max={state.max_reports})")
                        reports = crawler.crawl_recent_reports(
                            days=state.days,
                            max_reports=state.max_reports,
                            report_type="stock"  # ê¸°ë³¸ì ìœ¼ë¡œ ì¢…ëª© ë¦¬í¬íŠ¸
                        )
                        
                        if reports:
                            state.current_progress = len(reports)
                            state.total_collected += len(reports)
                            state.last_collected = datetime.now()
                            state.updated_at = datetime.now()
                            self.logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {site_id} - {len(reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘")
                            
                            # ì •ê·œí™” ë° ì €ì¥ íŒŒì´í”„ë¼ì¸ í†µí•© (ì˜µì…˜)
                            try:
                                from analyst_report_pipeline import AnalystReportPipeline
                                import os
                                
                                db_params = {
                                    'host': os.getenv('DB_HOST', 'localhost'),
                                    'database': os.getenv('DB_NAME', 'crawler_db'),
                                    'user': os.getenv('DB_USER', 'postgres'),
                                    'password': os.getenv('DB_PASSWORD', '')
                                }
                                
                                # DB ì €ì¥ í™œì„±í™” ì—¬ë¶€ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì„¤ì •)
                                enable_db = os.getenv('ENABLE_DB_STORAGE', 'false').lower() == 'true'
                                
                                if enable_db and db_params.get('password'):
                                    pipeline = AnalystReportPipeline(db_params, enable_db=True)
                                    saved_count = pipeline.process_reports(reports, source='hankyung', skip_errors=True)
                                    self.logger.info(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ë¦¬í¬íŠ¸ ì €ì¥")
                                else:
                                    self.logger.debug("DB ì €ì¥ ë¹„í™œì„±í™” (ENABLE_DB_STORAGE=false ë˜ëŠ” DB_PASSWORD ì—†ìŒ)")
                                    
                            except ImportError:
                                self.logger.debug("ì •ê·œí™” íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                            except Exception as e:
                                self.logger.warning(f"ì •ê·œí™”/ì €ì¥ ì‹¤íŒ¨ (í¬ë¡¤ë§ì€ ì„±ê³µ): {e}")
                        else:
                            self.logger.warning(f"âš ï¸  í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ: {site_id}")
                        
                        # í¬ë¡¤ë§ ì™„ë£Œ í›„ ì¢…ë£Œ
                        state.status = CrawlingStatus.IDLE
                        break
                    else:
                        # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” í†µí•© í¬ë¡¤ëŸ¬ ì‚¬ìš©
                        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ (í•„ìš”ì‹œ í™•ì¥)
                        state.current_progress += 1
                        state.total_collected += 1
                        state.last_collected = datetime.now()
                        state.updated_at = datetime.now()
                        
                        self._save_states()
                        
                        # ëŒ€ê¸°
                        time.sleep(2)
                        
                        # ëª©í‘œ ë‹¬ì„± í™•ì¸
                        if state.current_progress >= state.total_target:
                            state.status = CrawlingStatus.IDLE
                            break
                
                except Exception as e:
                    self.logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ({site_id}): {e}")
                    state.total_failed += 1
                    state.last_error = str(e)
                    # ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ (ì¬ì‹œë„)
                    time.sleep(5)
                    continue
        
        except Exception as e:
            state.status = CrawlingStatus.ERROR
            state.last_error = str(e)
            state.updated_at = datetime.now()
            self.logger.error(f"í¬ë¡¤ë§ ì›Œì»¤ ì˜¤ë¥˜ ({site_id}): {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        
        finally:
            state.updated_at = datetime.now()
            self._save_states()
            if site_id in self._running_tasks:
                del self._running_tasks[site_id]
            self.logger.info(f"í¬ë¡¤ë§ ì›Œì»¤ ì¢…ë£Œ: {site_id}")
    
    def _load_states(self):
        """ìƒíƒœ ë¡œë“œ"""
        
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for site_data in data.get('sites', []):
                state = SiteCrawlingState(**site_data)
                # enum ë³€í™˜
                state.status = CrawlingStatus(state.status) if isinstance(state.status, str) else state.status
                state.mode = CrawlingMode(state.mode) if isinstance(state.mode, str) else state.mode
                # datetime ë³€í™˜
                if isinstance(state.last_collected, str):
                    state.last_collected = datetime.fromisoformat(state.last_collected)
                if isinstance(state.next_run, str):
                    state.next_run = datetime.fromisoformat(state.next_run)
                
                self.sites[state.site_id] = state
            
            self.logger.info(f"ìƒíƒœ ë¡œë“œ ì™„ë£Œ: {len(self.sites)}ê°œ ì‚¬ì´íŠ¸")
        
        except FileNotFoundError:
            self.logger.info("ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _save_states(self):
        """ìƒíƒœ ì €ì¥"""
        
        try:
            data = {
                'sites': [state.to_dict() for state in self.sites.values()],
                'saved_at': datetime.now().isoformat()
            }
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        except Exception as e:
            self.logger.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_all_states(self) -> List[SiteCrawlingState]:
        """ëª¨ë“  ì‚¬ì´íŠ¸ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"""
        return list(self.sites.values())
    
    def update_schedule(self, site_id: str, schedule: Dict):
        """ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸"""
        
        state = self.sites.get(site_id)
        if not state:
            return False
        
        state.schedule = schedule
        state.mode = CrawlingMode.AUTO if schedule else CrawlingMode.MANUAL
        
        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        if schedule:
            state.next_run = self._calculate_next_run(schedule)
        
        state.updated_at = datetime.now()
        self._save_states()
        
        return True
    
    def _calculate_next_run(self, schedule: Dict) -> Optional[datetime]:
        """ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°"""
        
        now = datetime.now()
        
        if schedule.get('interval') == 'daily':
            time_str = schedule.get('time', '09:00')
            hour, minute = map(int, time_str.split(':'))
            
            next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=1)
            
            return next_run
        
        elif schedule.get('interval') == 'weekly':
            day_map = {
                'monday': 0, 'tuesday': 1, 'wednesday': 2,
                'thursday': 3, 'friday': 4, 'saturday': 5, 'sunday': 6
            }
            
            target_day = day_map.get(schedule.get('day', 'monday').lower(), 0)
            time_str = schedule.get('time', '10:00')
            hour, minute = map(int, time_str.split(':'))
            
            days_ahead = target_day - now.weekday()
            if days_ahead <= 0:
                days_ahead += 7
            
            next_run = now + timedelta(days=days_ahead)
            next_run = next_run.replace(hour=hour, minute=minute, second=0, microsecond=0)
            
            return next_run
        
        return None

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("\n" + "="*60)
    print("ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    manager = SiteCrawlingManager()
    
    # ì‚¬ì´íŠ¸ ë“±ë¡
    site1 = manager.register_site(
        site_id="38com",
        site_name="38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
        site_url="http://www.38.co.kr",
        days=1,
        max_reports=50,
        fake_face_profile='casual'
    )
    
    print(f"ì‚¬ì´íŠ¸ ë“±ë¡: {site1.site_name}")
    print(f"  ìƒíƒœ: {site1.status.value}")
    print(f"  ëª¨ë“œ: {site1.mode.value}")
    
    # ìŠ¤ì¼€ì¤„ ì„¤ì •
    manager.update_schedule("38com", {
        'interval': 'daily',
        'time': '09:00'
    })
    
    state = manager.get_site_state("38com")
    if state and state.next_run:
        print(f"  ë‹¤ìŒ ì‹¤í–‰: {state.next_run.strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


