# í¬ë¡¤ëŸ¬-ì •ê·œí™”-ì €ì¥ íŒŒì´í”„ë¼ì¸ í†µí•© ì™„ë£Œ ë³´ê³ ì„œ

## âœ… ì™„ë£Œëœ í†µí•©

### site_crawling_manager.pyì— ì •ê·œí™”/ì €ì¥ íŒŒì´í”„ë¼ì¸ í†µí•©

**ëª©í‘œ**: í¬ë¡¤ë§ â†’ ì •ê·œí™” â†’ PostgreSQL ì €ì¥ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰

## ğŸ”§ êµ¬í˜„ ë‚´ìš©

### 1. í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬ì— íŒŒì´í”„ë¼ì¸ í†µí•©

**ìœ„ì¹˜**: `site_crawling_manager.py` - `_crawling_worker()` ë©”ì„œë“œ

**ê¸°ëŠ¥**:
- í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì •ê·œí™” ë° ì €ì¥
- í™˜ê²½ë³€ìˆ˜ë¡œ DB ì €ì¥ í™œì„±í™”/ë¹„í™œì„±í™” ì œì–´
- ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ìœ ì§€

**ë™ì‘ ì¡°ê±´**:
- `ENABLE_DB_STORAGE=true` í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- `DB_PASSWORD` í™˜ê²½ë³€ìˆ˜ ì„¤ì • (DB ì—°ê²° í•„ìˆ˜)
- `analyst_report_pipeline.py` ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥

## ğŸ“Š í†µí•© íë¦„

```
1. site_crawling_managerì—ì„œ í¬ë¡¤ë§ ì‹œì‘
   â†“
2. í•œê²½ ì»¨ì„¼ì„œìŠ¤ í¬ë¡¤ëŸ¬ ì‹¤í–‰
   â†“
3. ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ
   â†“
4. ì •ê·œí™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì˜µì…˜)
   - AnalystReportPipeline ì´ˆê¸°í™”
   - ë¦¬í¬íŠ¸ ì •ê·œí™” (korea_normalize.py)
   - PostgreSQL ì €ì¥ (analyst_snapshot_store.py)
   â†“
5. ê²°ê³¼ ë¡œê¹…
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ë³€ìˆ˜
ENABLE_DB_STORAGE=true
DB_HOST=localhost
DB_NAME=crawler_db
DB_USER=postgres
DB_PASSWORD=your_password
```

### DB ì €ì¥ í™œì„±í™”/ë¹„í™œì„±í™”

**í™œì„±í™”**:
```bash
export ENABLE_DB_STORAGE=true
export DB_PASSWORD=your_password
```

**ë¹„í™œì„±í™”**:
```bash
export ENABLE_DB_STORAGE=false
# ë˜ëŠ” DB_PASSWORDë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ
```

## ğŸ“ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (í¬ë¡¤ë§ë§Œ)

```python
from site_crawling_manager import SiteCrawlingManager

manager = SiteCrawlingManager()
manager.start_crawling("hankyung_consensus", days=7, max_reports=50)
```

### 2. DB ì €ì¥ í¬í•¨ (í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export ENABLE_DB_STORAGE=true
export DB_HOST=localhost
export DB_NAME=crawler_db
export DB_USER=postgres
export DB_PASSWORD=your_password
```

```python
from site_crawling_manager import SiteCrawlingManager

manager = SiteCrawlingManager()
manager.start_crawling("hankyung_consensus", days=7, max_reports=50)
# ìë™ìœ¼ë¡œ ì •ê·œí™” ë° DB ì €ì¥ ìˆ˜í–‰
```

### 3. ì§ì ‘ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©

```python
from crawler_hankyung_consensus import HankyungConsensusCrawler
from analyst_report_pipeline import AnalystReportPipeline
import os

# í¬ë¡¤ë§
crawler = HankyungConsensusCrawler()
reports = crawler.crawl_recent_reports(days=7, max_reports=50)

# ì •ê·œí™” ë° ì €ì¥
db_params = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'database': os.getenv('DB_NAME', 'crawler_db'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', '')
}

pipeline = AnalystReportPipeline(db_params, enable_db=True)
saved_count = pipeline.process_reports(reports, source='hankyung')
print(f"ì €ì¥ëœ ë¦¬í¬íŠ¸: {saved_count}ê°œ")
```

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

### 1. ìë™í™”
- í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì •ê·œí™” ë° ì €ì¥
- ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¶ˆí•„ìš”

### 2. ìœ ì—°ì„±
- í™˜ê²½ë³€ìˆ˜ë¡œ í™œì„±í™”/ë¹„í™œì„±í™” ì œì–´
- DB ì—°ê²° ì‹¤íŒ¨ ì‹œì—ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ìœ ì§€

### 3. ì˜¤ë¥˜ ì²˜ë¦¬
- ì •ê·œí™”/ì €ì¥ ì‹¤íŒ¨ ì‹œì—ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ìœ ì§€
- ìƒì„¸í•œ ë¡œê¹…ìœ¼ë¡œ ë¬¸ì œ ì¶”ì  ê°€ëŠ¥

## ğŸ“Š ë¡œê·¸ ì˜ˆì‹œ

### DB ì €ì¥ í™œì„±í™” ì‹œ
```
í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: hankyung_consensus (days=7, max=50)
âœ… í¬ë¡¤ë§ ì™„ë£Œ: hankyung_consensus - 15ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘
ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: 15ê°œ ë¦¬í¬íŠ¸ ì €ì¥
```

### DB ì €ì¥ ë¹„í™œì„±í™” ì‹œ
```
í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: hankyung_consensus (days=7, max=50)
âœ… í¬ë¡¤ë§ ì™„ë£Œ: hankyung_consensus - 15ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘
DB ì €ì¥ ë¹„í™œì„±í™” (ENABLE_DB_STORAGE=false ë˜ëŠ” DB_PASSWORD ì—†ìŒ)
```

### ëª¨ë“ˆ ì—†ìŒ ì‹œ
```
í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: hankyung_consensus (days=7, max=50)
âœ… í¬ë¡¤ë§ ì™„ë£Œ: hankyung_consensus - 15ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘
ì •ê·œí™” íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ**: `analyst_reports_schema.sql`ì„ ë¨¼ì € ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
2. **í™˜ê²½ë³€ìˆ˜**: DB ì €ì¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
3. **ì˜ì¡´ì„±**: `analyst_report_pipeline.py`, `korea_normalize.py`, `analyst_snapshot_store.py` ëª¨ë“ˆ í•„ìš”

## ğŸ” ë‹¤ìŒ ë‹¨ê³„ (ì„ íƒ)

1. **ë‹¤ë¥¸ í¬ë¡¤ëŸ¬ í†µí•©**: ë„¤ì´ë²„ ê¸ˆìœµ, 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë“±ì—ë„ íŒŒì´í”„ë¼ì¸ í†µí•©
2. **ëŒ€ì‹œë³´ë“œ ì—°ë™**: ëŒ€ì‹œë³´ë“œì—ì„œ DB ì €ì¥ ìƒíƒœ í‘œì‹œ
3. **ìŠ¤ì¼€ì¤„ë§**: ìë™ ìŠ¤ì¼€ì¤„ë§ ì‹œ DB ì €ì¥ í¬í•¨
4. **ëª¨ë‹ˆí„°ë§**: ì €ì¥ ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ìˆ˜ì§‘

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- `NORMALIZATION_INTEGRATION.md`: ì •ê·œí™” ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ ë³´ê³ ì„œ
- `analyst_report_pipeline.py`: íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ
- `site_crawling_manager.py`: í¬ë¡¤ë§ ê´€ë¦¬ì
- `analyst_reports_schema.sql`: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

