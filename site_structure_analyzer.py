# site_structure_analyzer.py
"""
ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°

ë©”ë‰´, ë§í¬ íŒ¨í„´, ë°ì´í„° êµ¬ì¡° ë“±ì„ ë¶„ì„í•˜ì—¬
êµ¬ì¡° ë³€ê²½ì„ ê°ì§€í•˜ê³  ëŒ€ì‘í•  ìˆ˜ ìˆë„ë¡ í•¨
"""

import sys
import io
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import hashlib
import logging
from collections import defaultdict

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

@dataclass
class MenuItem:
    """ë©”ë‰´ í•­ëª©"""
    text: str
    url: str
    level: int  # ë©”ë‰´ ê¹Šì´ (1, 2, 3...)
    parent: Optional[str] = None
    children: List[str] = None
    
    def __post_init__(self):
        if self.children is None:
            self.children = []

@dataclass
class LinkPattern:
    """ë§í¬ íŒ¨í„´"""
    pattern: str  # ì •ê·œì‹ íŒ¨í„´ ë˜ëŠ” í‚¤ì›Œë“œ
    url_type: str  # 'report_detail', 'report_list', 'category' ë“±
    confidence: float  # íŒ¨í„´ ì‹ ë¢°ë„ (0.0 ~ 1.0)
    examples: List[str] = None  # ì˜ˆì‹œ URLë“¤
    
    def __post_init__(self):
        if self.examples is None:
            self.examples = []

@dataclass
class DataStructure:
    """ë°ì´í„° êµ¬ì¡°"""
    page_type: str  # 'list', 'detail', 'category' ë“±
    title_selector: str  # ì œëª© ì„ íƒì
    date_selector: str  # ë‚ ì§œ ì„ íƒì
    content_selector: str  # ë³¸ë¬¸ ì„ íƒì
    metadata: Dict = None  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class SiteStructure:
    """ì‚¬ì´íŠ¸ êµ¬ì¡° ìŠ¤ëƒ…ìƒ·"""
    domain: str
    timestamp: datetime
    base_url: str
    menus: List[MenuItem]
    link_patterns: List[LinkPattern]
    data_structures: Dict[str, DataStructure]  # page_type -> DataStructure
    checksum: str  # êµ¬ì¡° ì²´í¬ì„¬ (ë³€ê²½ ê°ì§€ìš©)
    
    def to_dict(self) -> dict:
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return data
    
    def calculate_checksum(self) -> str:
        """êµ¬ì¡° ì²´í¬ì„¬ ê³„ì‚°"""
        content = json.dumps(self.to_dict(), sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content.encode()).hexdigest()

class SiteStructureAnalyzer:
    """ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.logger = logging.getLogger(__name__)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def analyze(self) -> SiteStructure:
        """ì „ì²´ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„"""
        
        self.logger.info(f"ğŸ” ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹œì‘: {self.base_url}")
        
        # 1. ë©”ë‰´ êµ¬ì¡° ë¶„ì„
        menus = self._analyze_menus()
        self.logger.info(f"âœ… ë©”ë‰´ {len(menus)}ê°œ ë°œê²¬")
        
        # 2. ë§í¬ íŒ¨í„´ ë¶„ì„
        link_patterns = self._analyze_link_patterns()
        self.logger.info(f"âœ… ë§í¬ íŒ¨í„´ {len(link_patterns)}ê°œ ë°œê²¬")
        
        # 3. ë°ì´í„° êµ¬ì¡° ë¶„ì„
        data_structures = self._analyze_data_structures()
        self.logger.info(f"âœ… ë°ì´í„° êµ¬ì¡° {len(data_structures)}ê°œ ë¶„ì„")
        
        # êµ¬ì¡° ìƒì„±
        structure = SiteStructure(
            domain=self.domain,
            timestamp=datetime.now(),
            base_url=self.base_url,
            menus=menus,
            link_patterns=link_patterns,
            data_structures=data_structures,
            checksum=""
        )
        
        # ì²´í¬ì„¬ ê³„ì‚°
        structure.checksum = structure.calculate_checksum()
        
        self.logger.info(f"âœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ (ì²´í¬ì„¬: {structure.checksum[:8]}...)")
        
        return structure
    
    def _analyze_menus(self) -> List[MenuItem]:
        """ë©”ë‰´ êµ¬ì¡° ë¶„ì„"""
        
        menus = []
        
        try:
            html = self._fetch(self.base_url)
            if not html:
                return menus
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë©”ë‰´ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            menu_selectors = [
                'nav', 'nav ul', 'nav li',
                '.menu', '.navigation', '.nav',
                '#menu', '#navigation', '#nav',
                'ul.menu', 'ul.nav', 'div.menu'
            ]
            
            menu_elements = []
            for selector in menu_selectors:
                elements = soup.select(selector)
                if elements:
                    menu_elements.extend(elements)
                    break
            
            # ë§í¬ ì¶”ì¶œ
            links = soup.find_all('a', href=True)
            
            menu_map = {}  # url -> MenuItem
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if not text or not href:
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(self.base_url, href)
                
                # ë©”ë‰´ í‚¤ì›Œë“œ í™•ì¸
                menu_keywords = ['ë¦¬í¬íŠ¸', 'report', 'research', 'ë¶„ì„', 'news', 'ë‰´ìŠ¤', 'fund', 'í€ë“œ']
                if any(keyword in href.lower() or keyword in text.lower() for keyword in menu_keywords):
                    level = self._determine_menu_level(link)
                    
                    menu_item = MenuItem(
                        text=text,
                        url=full_url,
                        level=level
                    )
                    
                    menus.append(menu_item)
                    menu_map[full_url] = menu_item
            
            # ë¶€ëª¨-ìì‹ ê´€ê³„ ì„¤ì •
            self._build_menu_hierarchy(menus, menu_map)
            
        except Exception as e:
            self.logger.error(f"ë©”ë‰´ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return menus
    
    def _determine_menu_level(self, element) -> int:
        """ë©”ë‰´ ë ˆë²¨ ê²°ì •"""
        
        level = 1
        parent = element.parent
        
        while parent:
            if parent.name in ['ul', 'nav', 'div']:
                if 'menu' in parent.get('class', []) or 'nav' in parent.get('class', []):
                    level += 1
            parent = parent.parent
            if level > 5:  # ìµœëŒ€ ê¹Šì´ ì œí•œ
                break
        
        return min(level, 5)
    
    def _build_menu_hierarchy(self, menus: List[MenuItem], menu_map: Dict):
        """ë©”ë‰´ ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•"""
        
        # URL ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë¶€ëª¨ ì°¾ê¸°
        for menu in menus:
            parsed = urlparse(menu.url)
            path_parts = [p for p in parsed.path.split('/') if p]
            
            if len(path_parts) > 1:
                # ë¶€ëª¨ ê²½ë¡œ ì°¾ê¸°
                parent_path = '/'.join(path_parts[:-1])
                parent_url = f"{parsed.scheme}://{parsed.netloc}/{parent_path}"
                
                # ë¶€ëª¨ ë©”ë‰´ ì°¾ê¸°
                for other_menu in menus:
                    if other_menu.url == parent_url or parent_url in other_menu.url:
                        menu.parent = other_menu.url
                        if menu.url not in other_menu.children:
                            other_menu.children.append(menu.url)
                        break
    
    def _analyze_link_patterns(self) -> List[LinkPattern]:
        """ë§í¬ íŒ¨í„´ ë¶„ì„"""
        
        patterns = []
        
        try:
            html = self._fetch(self.base_url)
            if not html:
                return patterns
            
            soup = BeautifulSoup(html, 'html.parser')
            links = soup.find_all('a', href=True)
            
            # URL ê·¸ë£¹í™”
            url_groups = defaultdict(list)
            
            for link in links:
                href = link.get('href', '')
                if not href:
                    continue
                
                full_url = urljoin(self.base_url, href)
                parsed = urlparse(full_url)
                
                # URL íŒ¨í„´ ë¶„ë¥˜
                url_type = self._classify_url_type(full_url, link)
                url_groups[url_type].append(full_url)
            
            # íŒ¨í„´ ìƒì„±
            for url_type, urls in url_groups.items():
                if not urls:
                    continue
                
                # ê³µí†µ íŒ¨í„´ ì¶”ì¶œ
                pattern = self._extract_common_pattern(urls)
                
                if pattern:
                    patterns.append(LinkPattern(
                        pattern=pattern,
                        url_type=url_type,
                        confidence=0.8,  # ê¸°ë³¸ ì‹ ë¢°ë„
                        examples=urls[:5]  # ìµœëŒ€ 5ê°œ ì˜ˆì‹œ
                    ))
            
        except Exception as e:
            self.logger.error(f"ë§í¬ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return patterns
    
    def _classify_url_type(self, url: str, element) -> str:
        """URL íƒ€ì… ë¶„ë¥˜"""
        
        url_lower = url.lower()
        text = element.get_text(strip=True).lower()
        
        # ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€
        if any(x in url_lower for x in ['o=v', 'view', 'detail', 'read']):
            if 'report' in url_lower or 'ë¦¬í¬íŠ¸' in text:
                return 'report_detail'
        
        # ë¦¬í¬íŠ¸ ëª©ë¡ í˜ì´ì§€
        if any(x in url_lower for x in ['report', 'research', 'ë¦¬í¬íŠ¸', 'ë¶„ì„']):
            if 'list' in url_lower or 'ëª©ë¡' in text:
                return 'report_list'
            return 'report_list'  # ê¸°ë³¸ê°’
        
        # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€
        if any(x in url_lower for x in ['category', 'cat', 'ì¹´í…Œê³ ë¦¬', 'ë¶„ë¥˜']):
            return 'category'
        
        # ë‰´ìŠ¤ í˜ì´ì§€
        if 'news' in url_lower or 'ë‰´ìŠ¤' in text:
            return 'news'
        
        # ê¸°íƒ€
        return 'other'
    
    def _extract_common_pattern(self, urls: List[str]) -> str:
        """ê³µí†µ íŒ¨í„´ ì¶”ì¶œ"""
        
        if not urls:
            return ""
        
        if len(urls) == 1:
            return urls[0]
        
        # URL íŒŒì‹±
        parsed_urls = [urlparse(url) for url in urls]
        
        # ê³µí†µ ê²½ë¡œ ì°¾ê¸°
        common_path = ""
        path_parts_list = [url.path.split('/') for url in parsed_urls]
        
        if path_parts_list:
            min_length = min(len(parts) for parts in path_parts_list)
            
            for i in range(min_length):
                parts = [parts[i] for parts in path_parts_list]
                if len(set(parts)) == 1:  # ëª¨ë‘ ê°™ìŒ
                    common_path += "/" + parts[0]
                else:
                    # íŒŒë¼ë¯¸í„° íŒ¨í„´ ì°¾ê¸°
                    if '=' in parts[0] or '?' in parts[0]:
                        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒ¨í„´
                        common_path += "/*"
                    break
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒ¨í„´
        if parsed_urls[0].query:
            common_path += "?" + parsed_urls[0].query.split('&')[0] + "=*"
        
        return common_path or urls[0]
    
    def _analyze_data_structures(self) -> Dict[str, DataStructure]:
        """ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        
        structures = {}
        
        try:
            # ëª©ë¡ í˜ì´ì§€ ë¶„ì„
            list_structure = self._analyze_list_page()
            if list_structure:
                structures['list'] = list_structure
            
            # ìƒì„¸ í˜ì´ì§€ ë¶„ì„
            detail_structure = self._analyze_detail_page()
            if detail_structure:
                structures['detail'] = detail_structure
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return structures
    
    def _analyze_list_page(self) -> Optional[DataStructure]:
        """ëª©ë¡ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
        
        # ë¦¬í¬íŠ¸ ëª©ë¡ URL ì‹œë„
        test_urls = [
            f"{self.base_url}/html/news/?m=kosdaq&nkey=report",
            f"{self.base_url}/html/fund/",
            f"{self.base_url}/html/news/",
        ]
        
        for url in test_urls:
            try:
                html = self._fetch(url)
                if not html or len(html) < 1000:
                    continue
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì„ íƒì ì°¾ê¸°
                title_selectors = self._find_selectors_for_text(soup, ['ì œëª©', 'title', 'subject'])
                
                # ë‚ ì§œ ì„ íƒì ì°¾ê¸°
                date_selectors = self._find_selectors_for_text(soup, ['ë‚ ì§œ', 'date', 'ì‘ì„±ì¼'])
                
                return DataStructure(
                    page_type='list',
                    title_selector=title_selectors[0] if title_selectors else 'a',
                    date_selector=date_selectors[0] if date_selectors else '.date',
                    content_selector='.list, .item, tr, li',
                    metadata={'test_url': url}
                )
                
            except Exception as e:
                continue
        
        return None
    
    def _analyze_detail_page(self) -> Optional[DataStructure]:
        """ìƒì„¸ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
        
        # ë¦¬í¬íŠ¸ ìƒì„¸ URL ì‹œë„ (ì˜ˆì‹œ)
        # ì‹¤ì œë¡œëŠ” ëª©ë¡ì—ì„œ ë§í¬ë¥¼ ì°¾ì•„ì„œ ë¶„ì„í•´ì•¼ í•¨
        
        return DataStructure(
            page_type='detail',
            title_selector='h1, .title, .subject',
            date_selector='.date, .published, time',
            content_selector='.content, .article, .body, #content',
            metadata={}
        )
    
    def _find_selectors_for_text(self, soup: BeautifulSoup, keywords: List[str]) -> List[str]:
        """íŠ¹ì • í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œì˜ ì„ íƒì ì°¾ê¸°"""
        
        selectors = []
        
        for keyword in keywords:
            # í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰
            elements = soup.find_all(string=lambda text: text and keyword in text)
            
            for element in elements[:3]:  # ìµœëŒ€ 3ê°œ
                parent = element.parent
                if parent:
                    # ì„ íƒì ìƒì„±
                    selector = self._generate_selector(parent)
                    if selector and selector not in selectors:
                        selectors.append(selector)
        
        return selectors
    
    def _generate_selector(self, element) -> str:
        """ìš”ì†Œì˜ CSS ì„ íƒì ìƒì„±"""
        
        if element.name:
            selector = element.name
            
            # IDê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if element.get('id'):
                return f"#{element.get('id')}"
            
            # í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            classes = element.get('class', [])
            if classes:
                class_str = '.'.join(classes)
                return f".{class_str}"
            
            return selector
        
        return ""
    
    def _fetch(self, url: str) -> Optional[str]:
        """URL ê°€ì ¸ì˜¤ê¸°"""
        
        try:
            response = self.session.get(url, timeout=10, verify=False)
            response.encoding = response.apparent_encoding
            return response.text
        except Exception as e:
            self.logger.error(f"URL ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url} - {e}")
            return None

# ============================================================
# êµ¬ì¡° ë³€ê²½ ê°ì§€ê¸°
# ============================================================

class StructureChangeDetector:
    """êµ¬ì¡° ë³€ê²½ ê°ì§€ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.structure_history: List[SiteStructure] = []
    
    def detect_changes(
        self,
        current: SiteStructure,
        previous: Optional[SiteStructure] = None
    ) -> Dict[str, any]:
        """êµ¬ì¡° ë³€ê²½ ê°ì§€"""
        
        if not previous:
            return {
                'has_changes': False,
                'message': 'ì´ì „ êµ¬ì¡°ê°€ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        changes = {
            'has_changes': False,
            'checksum_changed': current.checksum != previous.checksum,
            'menu_changes': [],
            'link_pattern_changes': [],
            'data_structure_changes': [],
            'timestamp': current.timestamp.isoformat()
        }
        
        # ë©”ë‰´ ë³€ê²½ ê°ì§€
        menu_changes = self._detect_menu_changes(current.menus, previous.menus)
        if menu_changes:
            changes['has_changes'] = True
            changes['menu_changes'] = menu_changes
        
        # ë§í¬ íŒ¨í„´ ë³€ê²½ ê°ì§€
        pattern_changes = self._detect_pattern_changes(
            current.link_patterns,
            previous.link_patterns
        )
        if pattern_changes:
            changes['has_changes'] = True
            changes['link_pattern_changes'] = pattern_changes
        
        # ë°ì´í„° êµ¬ì¡° ë³€ê²½ ê°ì§€
        structure_changes = self._detect_structure_changes(
            current.data_structures,
            previous.data_structures
        )
        if structure_changes:
            changes['has_changes'] = True
            changes['data_structure_changes'] = structure_changes
        
        if changes['checksum_changed']:
            changes['has_changes'] = True
        
        return changes
    
    def _detect_menu_changes(
        self,
        current: List[MenuItem],
        previous: List[MenuItem]
    ) -> List[Dict]:
        """ë©”ë‰´ ë³€ê²½ ê°ì§€"""
        
        changes = []
        
        current_urls = {m.url for m in current}
        previous_urls = {m.url for m in previous}
        
        # ì¶”ê°€ëœ ë©”ë‰´
        added = current_urls - previous_urls
        if added:
            changes.append({
                'type': 'added',
                'count': len(added),
                'urls': list(added)[:5]  # ìµœëŒ€ 5ê°œ
            })
        
        # ì‚­ì œëœ ë©”ë‰´
        removed = previous_urls - current_urls
        if removed:
            changes.append({
                'type': 'removed',
                'count': len(removed),
                'urls': list(removed)[:5]
            })
        
        return changes
    
    def _detect_pattern_changes(
        self,
        current: List[LinkPattern],
        previous: List[LinkPattern]
    ) -> List[Dict]:
        """ë§í¬ íŒ¨í„´ ë³€ê²½ ê°ì§€"""
        
        changes = []
        
        current_types = {p.url_type for p in current}
        previous_types = {p.url_type for p in previous}
        
        # ìƒˆë¡œìš´ íŒ¨í„´ íƒ€ì…
        added_types = current_types - previous_types
        if added_types:
            changes.append({
                'type': 'new_pattern_type',
                'types': list(added_types)
            })
        
        # ì‚¬ë¼ì§„ íŒ¨í„´ íƒ€ì…
        removed_types = previous_types - current_types
        if removed_types:
            changes.append({
                'type': 'removed_pattern_type',
                'types': list(removed_types)
            })
        
        return changes
    
    def _detect_structure_changes(
        self,
        current: Dict[str, DataStructure],
        previous: Dict[str, DataStructure]
    ) -> List[Dict]:
        """ë°ì´í„° êµ¬ì¡° ë³€ê²½ ê°ì§€"""
        
        changes = []
        
        # ìƒˆë¡œìš´ í˜ì´ì§€ íƒ€ì…
        added_types = set(current.keys()) - set(previous.keys())
        if added_types:
            changes.append({
                'type': 'new_page_type',
                'types': list(added_types)
            })
        
        # ì‚¬ë¼ì§„ í˜ì´ì§€ íƒ€ì…
        removed_types = set(previous.keys()) - set(current.keys())
        if removed_types:
            changes.append({
                'type': 'removed_page_type',
                'types': list(removed_types)
            })
        
        # ì„ íƒì ë³€ê²½
        for page_type in set(current.keys()) & set(previous.keys()):
            curr = current[page_type]
            prev = previous[page_type]
            
            if curr.title_selector != prev.title_selector:
                changes.append({
                    'type': 'selector_changed',
                    'page_type': page_type,
                    'field': 'title_selector',
                    'old': prev.title_selector,
                    'new': curr.title_selector
                })
        
        return changes
    
    def save_structure(self, structure: SiteStructure, filename: str = None):
        """êµ¬ì¡° ì €ì¥"""
        
        if filename is None:
            filename = f"structure_{structure.domain}_{structure.timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(structure.to_dict(), f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"êµ¬ì¡° ì €ì¥: {filename}")
        self.structure_history.append(structure)
    
    def load_structure(self, filename: str) -> Optional[SiteStructure]:
        """êµ¬ì¡° ë¡œë“œ"""
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # SiteStructure ì¬êµ¬ì„±
            menus = [MenuItem(**m) for m in data['menus']]
            link_patterns = [LinkPattern(**p) for p in data['link_patterns']]
            data_structures = {
                k: DataStructure(**v)
                for k, v in data['data_structures'].items()
            }
            
            structure = SiteStructure(
                domain=data['domain'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                base_url=data['base_url'],
                menus=menus,
                link_patterns=link_patterns,
                data_structures=data_structures,
                checksum=data['checksum']
            )
            
            return structure
            
        except Exception as e:
            self.logger.error(f"êµ¬ì¡° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("\n" + "="*60)
    print("ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°")
    print("="*60)
    print()
    
    # ë¶„ì„ê¸° ìƒì„±
    analyzer = SiteStructureAnalyzer("http://www.38.co.kr")
    
    # êµ¬ì¡° ë¶„ì„
    structure = analyzer.analyze()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ë¶„ì„ ê²°ê³¼")
    print("="*60)
    print(f"ë„ë©”ì¸: {structure.domain}")
    print(f"ë©”ë‰´ ìˆ˜: {len(structure.menus)}ê°œ")
    print(f"ë§í¬ íŒ¨í„´: {len(structure.link_patterns)}ê°œ")
    print(f"ë°ì´í„° êµ¬ì¡°: {len(structure.data_structures)}ê°œ")
    print(f"ì²´í¬ì„¬: {structure.checksum[:16]}...")
    
    # ë©”ë‰´ ì¶œë ¥
    if structure.menus:
        print("\në©”ë‰´ ëª©ë¡:")
        for menu in structure.menus[:10]:  # ìµœëŒ€ 10ê°œ
            indent = "  " * (menu.level - 1)
            print(f"{indent}- {menu.text} ({menu.url[:60]}...)")
    
    # ë§í¬ íŒ¨í„´ ì¶œë ¥
    if structure.link_patterns:
        print("\në§í¬ íŒ¨í„´:")
        for pattern in structure.link_patterns:
            print(f"  [{pattern.url_type}] {pattern.pattern}")
            print(f"    ì˜ˆì‹œ: {pattern.examples[0] if pattern.examples else 'N/A'}")
    
    # êµ¬ì¡° ì €ì¥
    detector = StructureChangeDetector()
    detector.save_structure(structure)
    
    print(f"\nâœ… êµ¬ì¡° ì €ì¥ ì™„ë£Œ: structure_*.json")


