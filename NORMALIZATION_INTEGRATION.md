# ì •ê·œí™” ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”

ì—…ë¡œë“œëœ íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, **korea_normalize.py**ì™€ **analyst_snapshot_store.py**ë¥¼ í”„ë¡œì íŠ¸ì— í†µí•©í–ˆìŠµë‹ˆë‹¤.

## âœ… ì™„ë£Œëœ ì‘ì—…

### 1. ì •ê·œí™” ì‹œìŠ¤í…œ í†µí•© (`korea_normalize.py`)

**ìœ„ì¹˜**: `korea_normalize.py`

**ì£¼ìš” ê¸°ëŠ¥**:
- í•œêµ­ ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë¥¼ **KoreaAnalystSnapshot v1** í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
- ì§€ì› ì†ŒìŠ¤: `38com`, `hankyung`, `naver`
- ì˜ê²¬ ì •ê·œí™”: ë§¤ìˆ˜(ê°•ë ¥) â†’ Strong Buy, ë§¤ìˆ˜ â†’ Buy, ì¤‘ë¦½ â†’ Hold ë“±
- ëª©í‘œì£¼ê°€, ì‹ ë¢°ë„, ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ ìë™ ì¶”ì¶œ

**ì£¼ìš” í•¨ìˆ˜**:
```python
normalize_opinion(opinion_text) -> str
normalize_from_38com(raw_data) -> Dict
normalize_from_hankyung(raw_data) -> Dict
normalize_from_naver(raw_data) -> Dict
normalize_report_metadata(report, source='auto') -> Dict
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
from korea_normalize import normalize_report_metadata

# ReportMetadata ê°ì²´ ë˜ëŠ” dictë¥¼ ì •ê·œí™”
snapshot = normalize_report_metadata(report.to_dict(), source='naver')
```

### 2. PostgreSQL ì €ì¥ì†Œ í†µí•© (`analyst_snapshot_store.py`)

**ìœ„ì¹˜**: `analyst_snapshot_store.py`

**ì£¼ìš” ê¸°ëŠ¥**:
- ì •ê·œí™”ëœ ìŠ¤ëƒ…ìƒ·ì„ PostgreSQLì— ì €ì¥/ì¡°íšŒ
- ì»¨ì„¼ì„œìŠ¤ ê³„ì‚° (ìµœê·¼ Nì¼ ë¦¬í¬íŠ¸ ì§‘ê³„)
- ìµœì‹  ë¦¬í¬íŠ¸ ì¡°íšŒ

**ì£¼ìš” ë©”ì„œë“œ**:
```python
store.upsert_snapshot(snapshot) -> str  # ì €ì¥/ì—…ë°ì´íŠ¸
store.fetch_latest(stock_code, source, limit) -> List[Dict]
store.fetch_consensus(stock_code, days=30) -> Optional[Dict]
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
from analyst_snapshot_store import AnalystSnapshotStore

db_params = {
    'host': 'localhost',
    'database': 'crawler_db',
    'user': 'postgres',
    'password': os.getenv('DB_PASSWORD')
}

with AnalystSnapshotStore(db_params) as store:
    report_id = store.upsert_snapshot(snapshot)
    consensus = store.fetch_consensus('005930', days=30)
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„± (`analyst_reports_schema.sql`)

**ìœ„ì¹˜**: `analyst_reports_schema.sql`

**í…Œì´ë¸” êµ¬ì¡°**:
- `analyst_reports`: ë¦¬í¬íŠ¸ ë©”ì¸ í…Œì´ë¸”
  - `report_id` (UUID, PK)
  - `source`, `source_url` (unique)
  - `stock_code`, `stock_name`
  - `published_at`, `opinion`, `target_price`
  - `analyst_name`, `analyst_firm`
  - `trust_score`
  - `structured_data` (JSONB, ì „ì²´ ìŠ¤ëƒ…ìƒ·)

**ì¸ë±ìŠ¤**:
- ì¢…ëª© ì½”ë“œ, ë°œí–‰ì¼, ì†ŒìŠ¤, ì˜ê²¬ë³„ ì¸ë±ìŠ¤
- JSONB í•„ë“œ ê²€ìƒ‰ìš© GIN ì¸ë±ìŠ¤

**ë·°**:
- `v_analyst_consensus`: ìµœê·¼ 30ì¼ ì»¨ì„¼ì„œìŠ¤ ì§‘ê³„

**ì ìš© ë°©ë²•**:
```sql
-- PostgreSQLì—ì„œ ì‹¤í–‰
\i analyst_reports_schema.sql
```

### 4. íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ìƒì„± (`analyst_report_pipeline.py`)

**ìœ„ì¹˜**: `analyst_report_pipeline.py`

**ì£¼ìš” ê¸°ëŠ¥**:
- í¬ë¡¤ëŸ¬ â†’ ì •ê·œí™” â†’ ì €ì¥ ìë™í™”
- ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹…
- DB ì €ì¥ í™œì„±í™”/ë¹„í™œì„±í™” ì˜µì…˜

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
from analyst_report_pipeline import AnalystReportPipeline

db_params = {...}
pipeline = AnalystReportPipeline(db_params)

# í¬ë¡¤ëŸ¬ì—ì„œ ìˆ˜ì§‘í•œ ë¦¬í¬íŠ¸ ì²˜ë¦¬
reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", "005930")
saved_count = pipeline.process_reports(reports, source='naver')

# ì»¨ì„¼ì„œìŠ¤ ì¡°íšŒ
consensus = pipeline.get_consensus('005930', days=30)
```

## ğŸ”— í¬ë¡¤ëŸ¬ í†µí•© ë°©ë²•

### ë°©ë²• 1: íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (ê¶Œì¥)

```python
from analyst_report_pipeline import AnalystReportPipeline

# í¬ë¡¤ëŸ¬ ì‹¤í–‰
reports = crawler.search_by_stock("ì‚¼ì„±ì „ì", "005930", days=7)

# íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬
pipeline = AnalystReportPipeline(db_params)
saved_count = pipeline.process_reports(reports, source='naver')
```

### ë°©ë²• 2: ì§ì ‘ í†µí•©

```python
from korea_normalize import normalize_report_metadata
from analyst_snapshot_store import AnalystSnapshotStore

# ì •ê·œí™”
snapshot = normalize_report_metadata(report.to_dict(), source='naver')

# ì €ì¥
with AnalystSnapshotStore(db_params) as store:
    report_id = store.upsert_snapshot(snapshot)
```

## ğŸ“Š ë°ì´í„° íë¦„

```
í¬ë¡¤ëŸ¬ (ReportMetadata)
    â†“
ì •ê·œí™” (korea_normalize.py)
    â†“
KoreaAnalystSnapshot v1
    â†“
PostgreSQL ì €ì¥ (analyst_snapshot_store.py)
    â†“
ì»¨ì„¼ì„œìŠ¤ ê³„ì‚° / ì¡°íšŒ
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
1. âœ… **ì •ê·œí™” ì‹œìŠ¤í…œ**: ì™„ë£Œ
2. âœ… **PostgreSQL ì €ì¥ì†Œ**: ì™„ë£Œ
3. â³ **í¬ë¡¤ëŸ¬ í†µí•©**: `site_crawling_manager.py`ì— íŒŒì´í”„ë¼ì¸ ì¶”ê°€ í•„ìš”

### ì„ íƒì  ì‘ì—…
1. **global_normalize.py í†µí•©**: í•´ì™¸ ë°ì´í„° ì •ê·œí™” (Finnhub, FMP)
2. **ì•™ìƒë¸” ë³‘í•©**: ì—¬ëŸ¬ ì†ŒìŠ¤ ë°ì´í„° í†µí•© ì „ëµ
3. **ìë™ ìŠ¤ì¼€ì¤„ë§**: Windows Task Scheduler ì—°ë™

## ğŸ“ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜
DB_HOST=localhost
DB_NAME=crawler_db
DB_USER=postgres
DB_PASSWORD=your_password
```

## ğŸ” í…ŒìŠ¤íŠ¸

### ì •ê·œí™” í…ŒìŠ¤íŠ¸
```bash
python korea_normalize.py
```

### ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
```bash
python analyst_snapshot_store.py
```

### íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
```bash
python analyst_report_pipeline.py
```

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- `korea_analyst_snapshot_v1.schema.json`: ìŠ¤í‚¤ë§ˆ ì •ì˜
- `UPLOADED_FILES_ANALYSIS.md`: ì—…ë¡œë“œ íŒŒì¼ ë¶„ì„ ê²°ê³¼
- `DEVELOPMENT_GOVERNANCE_GUIDE.md`: ê°œë°œ ê±°ë²„ë„ŒìŠ¤ ê°€ì´ë“œ

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ**: `analyst_reports_schema.sql`ì„ ë¨¼ì € ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
2. **í™˜ê²½ ë³€ìˆ˜**: DB ì—°ê²° ì •ë³´ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.
3. **ì—ëŸ¬ ì²˜ë¦¬**: `skip_errors=True`ë¡œ ì„¤ì •í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.
4. **ì¤‘ë³µ ë°©ì§€**: `source_url`ì„ unique keyë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì €ì¥ì„ ë°©ì§€í•©ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê°œì„ ì‚¬í•­

1. **í‘œì¤€í™”**: ëª¨ë“  í¬ë¡¤ëŸ¬ ë°ì´í„°ë¥¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
2. **ìë™í™”**: í¬ë¡¤ë§ â†’ ì •ê·œí™” â†’ ì €ì¥ íŒŒì´í”„ë¼ì¸ ìë™í™”
3. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì†ŒìŠ¤ ì¶”ê°€ ì‹œ `normalize_from_*` í•¨ìˆ˜ë§Œ ì¶”ê°€í•˜ë©´ ë¨
4. **ì‹ ë¢°ë„**: ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ì ìˆ˜ ìë™ ê³„ì‚°
5. **ì»¨ì„¼ì„œìŠ¤**: ì—¬ëŸ¬ ë¦¬í¬íŠ¸ë¥¼ ì§‘ê³„í•˜ì—¬ ì»¨ì„¼ì„œìŠ¤ ê³„ì‚°

