"""
í†µí•© ê²€ìƒ‰ ì—”ì§„
ì‚¬ìš©ì(a) + ì‹œìŠ¤í…œ(b) + ì •ë³´í’ˆì§ˆ(c) ì™„ì „ í†µí•©
"""

import logging
import time
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import RealDictCursor

from enhanced_search_result import (
    EnhancedSearchResult,
    SearchResultItem,
    AIInsight,
    SystemMetrics,
    ErrorInfo,
    ActionButton,
    CredibilityScore,
    TimeInfo,
    VerificationStatus,
    SourceTier,
    RelatedStock
)

logger = logging.getLogger(__name__)

logger = logging.getLogger(__name__)


class IntegratedSearchEngine:
    """í†µí•© ê²€ìƒ‰ ì—”ì§„ (a + b + c)"""
    
    def __init__(self, db_params: Dict, enable_ai: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_params: PostgreSQL ì—°ê²° íŒŒë¼ë¯¸í„°
            enable_ai: AI ì¸ì‚¬ì´íŠ¸ í™œì„±í™”
        """
        self.db_params = db_params
        self.enable_ai = enable_ai
        self.conn = None
    
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**self.db_params)
            logger.info("Database connected")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def disconnect(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
    
    def search(
        self, 
        query: str,
        limit: int = 50,
        include_ai_insight: bool = True
    ) -> EnhancedSearchResult:
        """
        í†µí•© ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ì–´
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            include_ai_insight: AI ì¸ì‚¬ì´íŠ¸ í¬í•¨ ì—¬ë¶€
        
        Returns:
            EnhancedSearchResult
        """
        start_time = time.time()
        
        try:
            self.connect()
            
            # 1. ê¸°ë³¸ ê²€ìƒ‰ (ì •ë³´ í’ˆì§ˆ c)
            items = self._search_database(query, limit)
            
            if not items:
                return self._create_empty_result(query)
            
            # 2. ì‹ ë¢°ë„ ê°•í™” (ì •ë³´ í’ˆì§ˆ c)
            items = self._enhance_credibility(items)
            
            # 3. ê´€ë ¨ ì¢…ëª© ì •ë³´ ì¶”ê°€ (ì‚¬ìš©ì ê´€ì  a)
            items = self._add_related_stocks(items)
            
            # 4. AI ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì‚¬ìš©ì ê´€ì  a)
            ai_insight = None
            if include_ai_insight and self.enable_ai:
                ai_insight = self._generate_ai_insight(query, items)
            
            # 5. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì‹œìŠ¤í…œ ê´€ì  b)
            metrics = self._collect_metrics(start_time)
            
            # 6. ì•¡ì…˜ ë²„íŠ¼ ìƒì„± (ì‚¬ìš©ì ê´€ì  a)
            actions = self._create_action_buttons(query, items)
            
            return EnhancedSearchResult(
                query=query,
                items=items,
                ai_insight=ai_insight,
                metrics=metrics,
                action_buttons=actions
            )
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            error_msg = str(e)
            # ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€
            if "relation" in error_msg.lower() or "does not exist" in error_msg.lower():
                error_msg = "ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. news_ingestion_schema.sqlì„ ì‹¤í–‰í•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„±í•˜ì„¸ìš”."
            elif "connection" in error_msg.lower() or "could not connect" in error_msg.lower():
                error_msg = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. DB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            return self._create_error_result(query, error_msg)
        
        finally:
            try:
                self.disconnect()
            except:
                pass
    
    def _search_database(self, query: str, limit: int) -> List[SearchResultItem]:
        """ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰"""
        try:
            cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'news_articles'
                );
            """)
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                logger.warning("news_articles í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return []
            
            # ì¢…ëª©ëª…/ì¢…ëª©ì½”ë“œ ë§¤ì¹­
            stock_codes = self._find_stock_codes(query)
            
            # ë³µí•© ê²€ìƒ‰ ì¿¼ë¦¬ (ì•ˆì „í•œ ì¿¼ë¦¬)
            sql = """
                SELECT 
                    na.article_id,
                    na.title,
                    COALESCE(na.content, '') as content,
                    COALESCE(na.summary, na.title) as summary,
                    COALESCE(na.url, '') as url,
                    COALESCE(na.source, 'Unknown') as source,
                    COALESCE(na.source_tier, 2) as source_tier,
                    na.published_at,
                    COALESCE(na.stock_codes, ARRAY[]::text[]) as stock_codes,
                    COALESCE(na.keywords, ARRAY[]::text[]) as keywords,
                    COALESCE(na.urgency_level, 1) as urgency_level,
                    COALESCE(na.sentiment, 'neutral') as sentiment,
                    COALESCE(na.credibility_score, 0.5) as credibility_score,
                    fc.verification_status,
                    fc.confidence_score,
                    COALESCE(fc.supporting_sources, ARRAY[]::text[]) as supporting_sources,
                    COALESCE(fc.contradicting_sources, ARRAY[]::text[]) as contradicting_sources,
                    'news' as item_type
                FROM news_articles na
                LEFT JOIN fact_checks fc ON na.article_id = fc.article_id
                WHERE 
                    (
                        na.title ILIKE %s 
                        OR COALESCE(na.content, '') ILIKE %s
                        OR (%s != '' AND %s = ANY(COALESCE(na.stock_codes, ARRAY[]::text[])))
                    )
                    AND na.published_at > NOW() - INTERVAL '30 days'
                ORDER BY 
                    na.urgency_level DESC NULLS LAST,
                    na.published_at DESC NULLS LAST
                LIMIT %s;
            """
            
            search_pattern = f"%{query}%"
            stock_code = stock_codes[0] if stock_codes else ""
            
            cursor.execute(sql, (search_pattern, search_pattern, stock_code, stock_code, limit))
            rows = cursor.fetchall()
            
            # SearchResultItem ë³€í™˜
            items = []
            for row in rows:
                try:
                    item = self._row_to_item(row)
                    items.append(item)
                except Exception as e:
                    logger.error(f"í–‰ ë³€í™˜ ì‹¤íŒ¨: {e}")
                    continue
            
            return items
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def _row_to_item(self, row: Dict) -> SearchResultItem:
        """DB í–‰ â†’ SearchResultItem ë³€í™˜"""
        
        # ì•ˆì „í•œ ê°’ ì¶”ì¶œ
        def safe_get(key, default):
            value = row.get(key, default)
            return default if value is None else value
        
        # VerificationStatus ì•ˆì „ ë³€í™˜
        verification_str = safe_get('verification_status', 'unverified')
        try:
            verification_status = VerificationStatus(verification_str)
        except (ValueError, AttributeError):
            verification_status = VerificationStatus.UNVERIFIED
        
        # ì‹ ë¢°ë„
        credibility = CredibilityScore(
            overall=float(safe_get('credibility_score', 0.5)),
            source_tier_score=0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
            cross_verify_score=0.0,
            past_accuracy=0.0,
            llm_confidence=float(safe_get('confidence_score', 0.5)),
            verification_status=verification_status,
            supporting_sources=list(safe_get('supporting_sources', [])),
            contradicting_sources=list(safe_get('contradicting_sources', []))
        )
        
        # ì‹œê°„ ì •ë³´ (ì•ˆì „í•˜ê²Œ)
        published_at = safe_get('published_at', datetime.now())
        if isinstance(published_at, str):
            try:
                published_at = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            except:
                published_at = datetime.now()
        
        time_info = TimeInfo(
            published_at=published_at,
            collected_at=datetime.now()
        )
        
        # ì†ŒìŠ¤ Tier ì•ˆì „ ë³€í™˜
        tier_value = safe_get('source_tier', 2)
        try:
            if isinstance(tier_value, str):
                tier_value = int(tier_value)
            source_tier = SourceTier(tier_value)
        except (ValueError, AttributeError):
            source_tier = SourceTier.TIER_2
        
        return SearchResultItem(
            title=str(safe_get('title', 'ì œëª© ì—†ìŒ')),
            content=str(safe_get('content', '')),
            summary=str(safe_get('summary', safe_get('title', 'ìš”ì•½ ì—†ìŒ'))),
            url=str(safe_get('url', '')),
            item_type=str(safe_get('item_type', 'news')),
            source=str(safe_get('source', 'Unknown')),
            source_tier=source_tier,
            time_info=time_info,
            credibility=credibility,
            stock_codes=list(safe_get('stock_codes', [])),
            keywords=list(safe_get('keywords', [])),
            relevance_score=1.0,
            urgency_level=int(safe_get('urgency_level', 1)),
            sentiment=str(safe_get('sentiment', 'neutral'))
        )
    
    def _enhance_credibility(self, items: List[SearchResultItem]) -> List[SearchResultItem]:
        """ì‹ ë¢°ë„ ì¬ê³„ì‚° (ì •ë³´ í’ˆì§ˆ c)"""
        for item in items:
            # Tierë³„ ì ìˆ˜
            tier_scores = {
                SourceTier.TIER_1: 0.98,
                SourceTier.TIER_2: 0.85,
                SourceTier.TIER_3: 0.65
            }
            item.credibility.source_tier_score = tier_scores.get(item.source_tier, 0.75)
            
            # êµì°¨ ê²€ì¦ ì ìˆ˜
            total = len(item.credibility.supporting_sources) + len(item.credibility.contradicting_sources)
            if total > 0:
                item.credibility.cross_verify_score = len(item.credibility.supporting_sources) / total
            else:
                item.credibility.cross_verify_score = 0.5
            
            # ê³¼ê±° ì •í™•ë„ (ì†ŒìŠ¤ë³„)
            source_accuracy = {
                'ì—°í•©ë‰´ìŠ¤': 0.92,
                'ë„¤ì´ë²„ê¸ˆìœµ': 0.88,
                'í•œêµ­ê²½ì œ': 0.90,
                'ëŒ€ì‹ ì¦ê¶Œ': 0.95,
            }
            item.credibility.past_accuracy = source_accuracy.get(item.source, 0.80)
            
            # ì¢…í•© ì ìˆ˜ ì¬ê³„ì‚°
            weights = {'tier': 0.2, 'cross': 0.3, 'past': 0.2, 'llm': 0.3}
            item.credibility.overall = (
                item.credibility.source_tier_score * weights['tier'] +
                item.credibility.cross_verify_score * weights['cross'] +
                item.credibility.past_accuracy * weights['past'] +
                item.credibility.llm_confidence * weights['llm']
            )
        
        return items
    
    def _add_related_stocks(self, items: List[SearchResultItem]) -> List[SearchResultItem]:
        """ê´€ë ¨ ì¢…ëª© ì •ë³´ ì¶”ê°€ (ì‚¬ìš©ì ê´€ì  a)"""
        # ì‹¤ì œë¡œëŠ” ì‹œì„¸ API í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        
        for item in items:
            if item.stock_codes:
                for code in item.stock_codes[:3]:  # ìµœëŒ€ 3ê°œ
                    stock = RelatedStock(
                        code=code,
                        name=self._get_stock_name(code),
                        current_price=76000.0,  # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ
                        change_rate=2.3,
                        volume_ratio=1.8
                    )
                    item.related_stocks.append(stock)
        
        return items
    
    def _generate_ai_insight(self, query: str, items: List[SearchResultItem]) -> AIInsight:
        """AI ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì‚¬ìš©ì ê´€ì  a)"""
        
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ)
        positive_count = sum(1 for item in items if item.sentiment == 'positive')
        negative_count = sum(1 for item in items if item.sentiment == 'negative')
        
        total = len(items)
        positive_ratio = positive_count / total if total > 0 else 0
        
        # ì¶”ì²œ ê²°ì •
        if positive_ratio >= 0.7:
            recommendation = "ê°•ë ¥ ë§¤ìˆ˜"
            confidence = 0.85
        elif positive_ratio >= 0.5:
            recommendation = "ë§¤ìˆ˜"
            confidence = 0.70
        elif positive_ratio >= 0.3:
            recommendation = "ë³´ìœ "
            confidence = 0.60
        else:
            recommendation = "ë§¤ë„"
            confidence = 0.65
        
        # ê·¼ê±° ì¶”ì¶œ
        reasoning = []
        risks = []
        
        # ê¸´ê¸‰ ë‰´ìŠ¤ í™•ì¸
        urgent_items = [item for item in items if item.urgency_level >= 4]
        if urgent_items:
            reasoning.append(f"ê¸´ê¸‰ ë‰´ìŠ¤ {len(urgent_items)}ê±´ ë°œìƒ")
        
        # ì‹ ë¢°ë„ ë†’ì€ ì†ŒìŠ¤
        verified_items = [
            item for item in items 
            if item.credibility.verification_status == VerificationStatus.VERIFIED
        ]
        if verified_items:
            reasoning.append(f"{len(verified_items)}ê°œ ê²€ì¦ëœ ì†ŒìŠ¤")
        
        # í‚¤ì›Œë“œ ë¶„ì„
        all_keywords = []
        for item in items:
            all_keywords.extend(item.keywords)
        
        from collections import Counter
        top_keywords = Counter(all_keywords).most_common(3)
        if top_keywords:
            keywords_str = ", ".join([k for k, _ in top_keywords])
            reasoning.append(f"ì£¼ìš” í‚¤ì›Œë“œ: {keywords_str}")
        
        # ë¦¬ìŠ¤í¬
        disputed_items = [
            item for item in items
            if item.credibility.verification_status == VerificationStatus.DISPUTED
        ]
        if disputed_items:
            risks.append(f"ë…¼ìŸ ì¤‘ì¸ ì •ë³´ {len(disputed_items)}ê±´")
        
        # í•µì‹¬ í¬ì¸íŠ¸ (ìƒìœ„ 3ê°œ ìš”ì•½)
        key_points = []
        for item in items[:3]:
            if item.summary:
                point = item.summary[:100]
                key_points.append(point)
        
        return AIInsight(
            recommendation=recommendation,
            confidence=confidence,
            reasoning=reasoning,
            risks=risks,
            key_points=key_points
        )
    
    def _collect_metrics(self, start_time: float) -> SystemMetrics:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì‹œìŠ¤í…œ ê´€ì  b)"""
        
        # ê²€ìƒ‰ ì‹œê°„
        search_time_ms = int((time.time() - start_time) * 1000)
        
        # í¬ë¡¤ ìƒíƒœ í™•ì¸ (ìµœê·¼ ì‘ì—… ë¡œê·¸)
        crawl_status = {}
        try:
            cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            
            sql = """
                SELECT DISTINCT ON (source_name)
                    source_name,
                    status,
                    completed_at
                FROM crawl_jobs
                ORDER BY source_name, completed_at DESC;
            """
            
            cursor.execute(sql)
            rows = cursor.fetchall()
            
            for row in rows:
                # 5ë¶„ ì´ë‚´ ì •ìƒ, ê·¸ ì™¸ ì§€ì—°
                if row['completed_at'] and (datetime.now() - row['completed_at']).total_seconds() < 300:
                    status = "ì •ìƒ"
                else:
                    status = "ì§€ì—° 5ë¶„"
                
                crawl_status[row['source_name']] = status
        except Exception as e:
            logger.warning(f"Failed to collect crawl status: {e}")
            crawl_status = {}
        
        return SystemMetrics(
            search_time_ms=search_time_ms,
            total_sources_checked=len(crawl_status),
            cache_hit=False,  # ì‹¤ì œë¡œëŠ” ìºì‹œ ì²´í¬
            data_freshness_minutes=2,
            crawl_status=crawl_status
        )
    
    def _create_action_buttons(
        self, 
        query: str, 
        items: List[SearchResultItem]
    ) -> List[ActionButton]:
        """ì•¡ì…˜ ë²„íŠ¼ ìƒì„± (ì‚¬ìš©ì ê´€ì  a)"""
        
        buttons = []
        
        # ê¸°ë³¸ ì•¡ì…˜
        buttons.append(ActionButton("ğŸ“ˆ ì°¨íŠ¸ ë³´ê¸°", f"open_chart:{query}", "ğŸ“ˆ", "primary"))
        buttons.append(ActionButton("ğŸ“° ë‰´ìŠ¤ ì „ì²´", f"view_all:{query}", "ğŸ“°", "secondary"))
        
        # ì¢…ëª©ì½”ë“œê°€ ìˆìœ¼ë©´
        stock_codes = set()
        for item in items:
            stock_codes.update(item.stock_codes)
        
        if stock_codes:
            buttons.append(ActionButton("â­ ê´€ì‹¬ì¢…ëª©", f"add_watchlist:{','.join(stock_codes)}", "â­", "secondary"))
            buttons.append(ActionButton("ğŸ”” ì•Œë¦¼ ì„¤ì •", f"setup_alert:{','.join(stock_codes)}", "ğŸ””", "secondary"))
        
        return buttons
    
    def _find_stock_codes(self, query: str) -> List[str]:
        """ê²€ìƒ‰ì–´ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ ë§¤í•‘ (ì‹¤ì œë¡œëŠ” DB ì¡°íšŒ)
        mapping = {
            'ì‚¼ì„±ì „ì': '005930',
            'SKí•˜ì´ë‹‰ìŠ¤': '000660',
            'ë„¤ì´ë²„': '035420',
            'NAVER': '035420',
        }
        
        return [mapping[query]] if query in mapping else []
    
    def _get_stock_name(self, code: str) -> str:
        """ì¢…ëª©ì½”ë“œ â†’ ì¢…ëª©ëª…"""
        mapping = {
            '005930': 'ì‚¼ì„±ì „ì',
            '000660': 'SKí•˜ì´ë‹‰ìŠ¤',
            '035420': 'ë„¤ì´ë²„',
        }
        return mapping.get(code, code)
    
    def _create_empty_result(self, query: str) -> EnhancedSearchResult:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return EnhancedSearchResult(
            query=query,
            items=[],
            error=ErrorInfo(
                has_error=True,
                error_type="NO_DATA",
                error_message="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
        )
    
    def _create_error_result(self, query: str, error: str) -> EnhancedSearchResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return EnhancedSearchResult(
            query=query,
            items=[],
            error=ErrorInfo(
                has_error=True,
                error_type="UNKNOWN",
                error_message=error
            )
        )


# ================================================================
# í…ŒìŠ¤íŠ¸
# ================================================================

if __name__ == '__main__':
    import json
    
    DB_PARAMS = {
        'host': 'localhost',
        'port': 5432,
        'database': 'abiseu',
        'user': 'postgres',
        'password': 'your_password'
    }
    
    engine = IntegratedSearchEngine(DB_PARAMS, enable_ai=True)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    result = engine.search("ì‚¼ì„±ì „ì", limit=20, include_ai_insight=True)
    
    # JSON ì¶œë ¥
    print("=" * 60)
    print("í†µí•© ê²€ìƒ‰ ê²°ê³¼")
    print("=" * 60)
    print(json.dumps(result.to_json(), ensure_ascii=False, indent=2))

