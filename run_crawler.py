# run_crawler.py
"""
38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì„¤ì • íŒŒì¼ì„ ì½ì–´ì„œ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import json
import sys
from pathlib import Path
from crawler_38com import ThirtyEightComCrawler

def load_config(config_path: str = "config.json") -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    
    config_file = Path(config_path)
    
    if not config_file.exists():
        print(f"âš ï¸  ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {}
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {}

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ëŸ¬ ì‹œì‘\n")
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    
    crawler_config = config.get('crawler', {})
    crawl_settings = config.get('crawl_settings', {})
    output_config = config.get('output', {})
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ThirtyEightComCrawler(
        delay=crawler_config.get('delay', 3.0),
        max_retries=crawler_config.get('max_retries', 3),
        retry_delay=crawler_config.get('retry_delay', 5.0)
    )
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    days = crawl_settings.get('days', 1)
    max_reports = crawl_settings.get('max_reports', 100)
    
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"   - ìµœê·¼ {days}ì¼")
    print(f"   - ìµœëŒ€ {max_reports}ê°œ")
    print(f"   - ìš”ì²­ ê°„ê²©: {crawler_config.get('delay', 3.0)}ì´ˆ")
    print()
    
    reports = crawler.crawl_recent_reports(
        days=days,
        max_reports=max_reports
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(reports)}ê°œ\n")
    
    if reports:
        for i, report in enumerate(reports, 1):
            print(f"{i}. {report.stock_name} ({report.stock_code})")
            print(f"   ì œëª©: {report.title[:60]}...")
            print(f"   ì• ë„ë¦¬ìŠ¤íŠ¸: {report.analyst_name} ({report.firm})")
            print(f"   ë‚ ì§œ: {report.published_date.strftime('%Y-%m-%d')}")
            
            if report.investment_opinion:
                print(f"   ì˜ê²¬: {report.investment_opinion}")
            
            if report.target_price:
                print(f"   ëª©í‘œê°€: {report.target_price}")
            
            print()
        
        # ì €ì¥
        json_file = output_config.get('json_filename', '38com_reports.json')
        csv_file = output_config.get('csv_filename', '38com_reports.csv')
        
        crawler.save_to_json(reports, json_file)
        crawler.save_to_csv(reports, csv_file)
        
        print(f"âœ… ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   - {json_file}")
        print(f"   - {csv_file}")
    else:
        print("âš ï¸  ìˆ˜ì§‘ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\nê°€ëŠ¥í•œ ì›ì¸:")
        print("1. ìµœê·¼ ë³´ê³ ì„œê°€ ì—†ìŒ")
        print("2. ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ (analyze_38com.pyë¡œ í™•ì¸)")
        print("3. ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


